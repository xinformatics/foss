{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-foundational-open-science-skills-foss-spring-2024","title":"Welcome to Foundational Open Science Skills (FOSS) Spring 2024!","text":""},{"location":"#workshop-structure","title":"Workshop Structure","text":"<p>Synchronously, we will meet each week virtually for two hours to discuss and have hands-on activities with our instructors. </p> <p>Each week we will discuss a new topic, ranging from data management to documentation to software management (see schedule). </p> <p>Asynchronously, you will read through the online materials (this website and linked websites) and develop your own project-based idea for strenghtening your Open Science skills.</p> <p>Much of the content interconnects from week to week and many of the skills and approaches we discuss relate to each other. </p> <p>Our ultimate goal in this workshop is for you to \"level up\" one or more of those philosophies/approaches/skills. </p>"},{"location":"#capstone-project","title":"Capstone Project","text":"<p>Throughout the duration of the course, students will be working on a project of their choosing that uses the tools and concepts presented in FOSS. The idea is that doing something tangible related to your own science or work will help you 'level up' your open science skills. This could be solo work or a group project depending on your interests. The culmination of the project will be a short presentation to the class on the last day of the course. You can read more about it at the capstone project page.</p>"},{"location":"#expected-outcomes","title":"Expected Outcomes","text":"<ul> <li>Proficiently organize your lab, external and internal communications, and teach and conduct research with open source software</li> <li>Ability to scale out computations from laptop to the cloud and High Performance Computing/High Throughput Computing systems</li> <li>Skillfully manage your research data through the data lifecycle </li> <li>Join a larger community of Open Science practitioners</li> <li>Be an Advocate for Open Science in your professional circles and communities</li> </ul> <p>By working through an example project relevant to your interests, you will practice open science skills using CyVerse, GitHub, R or Python, and other resources. At the end of the course, you and your team will present a plan for how to integrate open science into your research, lab, or other areas of your choosing.</p> <p>Funding and Citations:</p> <p>CyVerse is funded by the Arizona Board of Regents and the US National Science Foundation  under Award Numbers: </p> <p> </p> <p>The CyVerse Zenodo Community has published, citable versions of CyVerse materials: </p> <p></p> <p>Please cite CyVerse appropriately when you make use of our resources; see CyVerse citation policy.</p> <p> </p>"},{"location":"00_basics/","title":"The Unix Shell, Git, Github and LLMs: an Introduction","text":""},{"location":"00_basics/#requirements","title":"Requirements","text":"<p>Command Line Interfaces (CLI) are found throughout all Operating Systems, however we recommend the use of the Unix CLI. If you have a Unix based machine such as Linux/Ubuntu (or other Linux distributions), macOS, you are ready for the next step. If you use a Windows machine, please install the Windows Subsystem for Linux (WSL) as seen in the Before FOSS Starts section.</p>"},{"location":"00_basics/#the-unix-shell","title":"The Unix Shell","text":"<p>The computer is a tool. It evolved over the years from being an intricated calculator into an interactive machine with thousands of moving parts that keep us all connected through the Internet. It is now the norm to use a mouse, keyboard, and seeing flashing images on our screens through the Graphical User Interface (GUI). GUIs are central to the way we interact with computers, however, to best take advantage of the computer's many systems, one needs to learn of the Command Line Interface (CLI). The CLI sees the computer stripped down to only a Terminal from where one can run powerful commands executed through the Shell.</p> <p>Whilst the GUI allows for better accessbility to a computer, the CLI allows for advanced usage of one's computer. </p>"},{"location":"00_basics/#cli-vs-terminal-vs-shell","title":"CLI vs Terminal vs Shell","text":"<ul> <li>CLI (Command Line Interface): an interface that receives commands (and gives output) from a user in the form of lines of text.</li> <li>Terminal: the text based interface window.</li> <li>Shell: a computer program and scripting language that presents a CLI which allows you to control your computer using commands.</li> </ul> <p>The Shell sends commands to the computer through the CLI accessible through a Terminal window</p>"},{"location":"00_basics/#things-to-know-about-commands","title":"Things-to-Know About Commands","text":"<ul> <li>Shell commands are used to navigate, visualize, modify (files/folders) and automate (processes), and can only be executed through the shell's terminal window.</li> <li>For every command, typing <code>man</code> (manual) before the command, will open the manual for said command. <pre><code>$ man ls\n</code></pre><ul> <li>Doing the above command will result in opening the manual for the <code>ls</code> command. You can exist the man page by pressing <code>q</code>.</li> </ul> </li> <li>Each command has flags, or options, which are summoned with a <code>-</code>, such as <code>&lt;command&gt; -&lt;flag&gt;</code>. <pre><code>$ ls -a -l -h\n</code></pre><ul> <li>Doing the above command calls for the <code>-a</code> (all), <code>-l</code> (long), <code>-h</code> (human readable) flags. This causes <code>ls</code> to output a list of all files (inculding hidden files/folders) with human readable file size (e.g., it will list 3MB instead of 3000000), permissions, creator, and date of creation.</li> <li>If you do not know what flags are available, you can refer to the <code>man</code> command (or for many tools, use the <code>-h</code> (help) flag).</li> </ul> </li> <li><code>.</code> refers to current directory; <code>..</code> refers to above directory; <code>/</code> is the directory separator; <code>~</code> indicates the home directory. <pre><code>$ ls .            # lists files and folders in the current directory\n$ ls ..           # lists files and folders in the above directory\n$ ls ~            # lists files and folders in the home directory\n$ ls ~/Documents  # lists files and folders in Documents (a folder present in the home directory)\n</code></pre></li> </ul>"},{"location":"00_basics/#introductory-shell-commands","title":"Introductory Shell Commands","text":"<p>The following are introductory commands necessary when interacting with a computer through the Shell. These will help you orient, create and delete files. Most of this material is explained in more details in the Carpentries' Shell Module. Visit the Carpentries' website for a more in-depth tutorial.</p> <p>A short tutorial introducing the Shell</p> <p>Here below are quick explanations of a few elementary commands that will help you orient and navigate your files and folders through the Shell. If you would like to follow along the explanations for each command, feel free to download and unzip the shell-lesson-data.zip file from the Shell's Carpentry module.</p> Don't have access to a GUI? <p>Following along on a machine with no access to a GUI? Execute the following commands: <pre><code>$ sudo apt install unzip\n$ wget https://swcarpentry.github.io/shell-novice/data/shell-lesson-data.zip\n$ unzip shell-lesson-data.zip\n</code></pre></p>"},{"location":"00_basics/#navigation","title":"Navigation","text":"Command Explanation <code>pwd</code> print working directory <code>ls</code> list content of folder <code>cd</code> change directory <p>By typing <code>pwd</code>, the current working directory is printed.</p> <pre><code>$ pwd\n/mnt/d/\n</code></pre> <p>We can then use <code>ls</code> to see the contents of the current directory. By using the <code>-F</code> flag (<code>ls -F</code>) we can also see the type of file. Note: an asterisk (<code>*</code>) at the end of the object will denote a file, whilst a slash (<code>/</code>) will denote a folder.</p> <pre><code>$ ls -F \nshell-lesson-data/   shell-lesson-data.zip*\n</code></pre> <p>We can then move inside the folder of our choice doing <code>cd</code>. Doing <code>ls</code> following the opening of the folder of choice, will show the contents of the folder you just moved in. Feel free to explore the contents of the folders by using <code>cd</code> and <code>ls</code>.</p> <pre><code>$ cd shell-lesson-data\n$ ls -F\n\nexercise-data/  north-pacific-gyre/\n\n$ ls -F exercise-data/\n\nanimal-counts/  creatures/  numbers.txt*  proteins/  writing/\n</code></pre> <p>Use the Tab key to autocomplete</p> <p>You do not need to type the entire name of a folder or file. By using the tab key, the Shell will autocomplete the name of the files or folders. For example, typing the following</p> <pre><code>$ ls -F exer\n</code></pre> <p>and pressing the tab key, will result in autocompletion.</p> <pre><code>$ ls -F exercise-data/\n</code></pre> <p>You can then press tab twice, to print a list of the contents of the folder.</p> <pre><code>$ ls -F exercise-data/\nanimal-counts/ creatures/     numbers.txt    proteins/      writing/ \n</code></pre>"},{"location":"00_basics/#working-with-files-and-directories","title":"Working with Files and Directories","text":"Command Explanation <code>mkdir</code> make a directory <code>touch</code> creat empty file <code>nano</code> or <code>vim</code> text editors <code>mv</code> move command <code>cp</code> copy command <code>rm</code> remove command <p>Return to <code>shell-lesson-data</code>, and crate a directory with <code>mkdir &lt;name of folder&gt;</code>.</p> <pre><code>$ mkdir my_folder\n$ ls -F\nexercise-data/  my_folder/  north-pacific-gyre/\n</code></pre> <p>Notice the new <code>my_folder</code> directory.</p> <p>Naming your files</p> <p>It is strongly suggested that you avoid using spaces when naming your files. When using the Shell to communicate with your machine, a space can cause errors when loading or transferring files. Instead, use dashes (<code>-</code>), underscores (<code>_</code>), periods (<code>.</code>) and CamelCase when naming your files.</p> <p>Acceptable naming: <pre><code>$ mkdir my_personal_folder\n$ mkdir my_personal-folder\n$ mkdir MyPersonal.Folder\n</code></pre></p> Question <p>What do you think will happen if you attempt creating a folder by typing spaces?</p> Solution <p>You will obtain as many folders as typed words! <pre><code>$ mkdir my folder\n$ ls -F\nexercise-data/  folder/  my/  north-pacific-gyre/\n</code></pre> Notice the two folders <code>my</code> and <code>folder</code>.</p> <p>Create an empty file with <code>touch &lt;name of file&gt;</code>.</p> <pre><code>$ touch new_file\n</code></pre> <p><code>touch</code> will create an empty file, it is up to you to populate using whichever text editor you prefer. Refer to the carpentries material to know more about nano and its functionalities (link).</p> <p>Tip</p> <p>You can also use your text editor to look at the contents of your files!</p> <p>Use <code>mv &lt;name of file or folder you want to move&gt; &lt;name of destination folder&gt;</code> to move your newly created file to the directory you created previously (you can then use <code>ls</code> to check if you successully moved the file).</p> <p><pre><code>$ ls -F\nexercise-data/  new_file*  my_folder/  north-pacific-gyre/\n\n$ mv new_file my_folder/\n$ ls -F\nexercise-data/  my_folder/  north-pacific-gyre/\n\n$ ls -F my_folder/\nnew_file*\n</code></pre> <code>mv</code> can also be used to rename a file or folder with  <code>mv &lt;name of file or folder you want to change&gt; &lt;new name&gt;</code>.</p> <pre><code>$ cd my_folder/\n$ mv new_file my_file\n$ ls -F\nmy_file*\n</code></pre> <p><code>cp</code> is the command to copy a file with the syntax <code>cp &lt;name of file you want to copy&gt; &lt;name of copy file&gt;</code></p> <pre><code>$ cp my_file copy_my_file\n$ ls -F \ncopy_my_file*  my_file*\n</code></pre> <p>Copying folders</p> <p>To copy folders and the content of these folders, you will have to use the <code>-r</code> flag (recursive) for <code>cp</code> in the following manner <code>cp -r &lt;name of folder you want to copy&gt; &lt;name of copy folder&gt;</code> (following example is from the <code>shell-lesson-data/</code> directory). <pre><code>$ cp -r my_folder/ copy_my_folder\n$ ls -F\ncopy_my_folder/  exercise-data/  my_folder/  north-pacific-gyre/\n\n$ ls -F my_folder/\ncopy_my_file*  my_file*\n\n$ ls -F copy_my_folder/\ncopy_my_file*  my_file*\n</code></pre></p> <p>To remove an unwanted file, use <code>rm &lt;name of file to remove&gt;</code>.</p> <pre><code>$ rm copy_my_file\n$ ls -F \nmy_file\n</code></pre> <p>Removing folders</p> <p>Save as the \"Copying Folders\" note, you have to use the <code>-r</code> flag to remove a folder <code>rm -r &lt;name of folder you want to remove&gt;</code> (following example is from the <code>shell-lesson-data/</code> directory). <pre><code>$ rm -r copy_my_folder/\n$ ls -F\nexercise-data/  my_folder/  north-pacific-gyre/\n</code></pre></p>"},{"location":"00_basics/#introductory-remarks","title":"Introductory Remarks","text":"<p>The commands listed here above are to help you better understand directories and files. There is a lot more that one can accomplish when communicating with you computer through the Shell. In case you want to know more, here are some useful links you can visit:</p> <ul> <li>Pipes and Filters</li> <li>Loops</li> <li>Scripts</li> <li>Finding Things</li> </ul>"},{"location":"00_basics/#github-and-git","title":"Github and Git","text":"<p>FOSS stands for Foundational Open Science Skills: how many times have you worked on your code just to hit a bottleneck and found a solution on Stack Overflow? How many times have you found links that bring you to a GitHub repository with the exact snippet of code you needed?</p> <p>The beauty of Open is that it makes Science and its code available for all through the internet, sharing ideas and solutions for all. Tools like Git and Github allow for the sharing of code, and the ability to collaborate on projects.</p> <p>The following section will cover the very basics of Github so you can get up and running and using these tools. The concept of version control and the mechanics of using Git will be touched on in more depth later on in FOSS</p> <ul> <li> <p>Git: </p> <ul> <li>First developed in 2005, git is a version control software that allows users to make changes and add versions to their code.</li> <li>Changes and versions are saved locally.</li> <li>Accessible through the Shell.</li> </ul> </li> <li> <p>GitHub:</p> <ul> <li>First launched in 2008, its main focus is hosting and sharing code.</li> <li>Uses Git version control software. </li> <li>Changes and versions are saved online (requires an account).</li> <li>Mainly administered through the web (it also has a desktop app).</li> <li>Acquired by Microsoft in 2018.</li> </ul> </li> </ul> <p>  Git vs GitHub, simplified </p>"},{"location":"00_basics/#introducing-github","title":"Introducing GitHub","text":"<p>Since we are talking about making science accessible, we invite you to use GitHub to save and share your code. Please start by creating a GitHub account at https://github.com/.  </p>"},{"location":"00_basics/#user-profile","title":"User Profile","text":"<p>Just like in any other social media platform, you can create a profile for yourself. This is where you can add a picture, a description of yourself, and a link to your website. You can also add your location, your organization, and your pronouns. You can have a list of your most important repositories and show off your daily contributions. You are able to customize your profile to your liking. Check out this profile for fancy example. </p> <p></p> <p></p>"},{"location":"00_basics/#search","title":"Search","text":"<p>At the top of most pages, is a search bar. Use this to find repositories, users, and organizations. You can also use it to search for specific code within a repository. </p> <p></p>"},{"location":"00_basics/#starring-repositories","title":"Starring Repositories","text":"<p>You can star repositories that you like. This is a way to bookmark repositories that you want to come back to later. You can also use this to show your appreciation for a repository. You can see all of your starred repositories by clicking on your profile picture and then clicking on Your stars.</p> <p></p> <p></p>"},{"location":"00_basics/#create-your-own-repository","title":"Create Your Own Repository","text":"<p>Repositories are where your code is stored. A suggestion is to have one repository for one project.</p> <p>You can create repositories by clicking on the Repositories tab, and then clicking New.</p> <p></p> <p>Here, you can choose the name of your own repository, choose to make it private or public, adding a README and a licence. It is strongly reccomended that you choose to add an empty README file.  </p> <p></p> <p></p> <p>So, why a README?</p> <p>There are two main reasons why you would like a README file:</p> <ol> <li>It adds structure to your repository automatically - otherwise you would need to create said structure by yourself (not recommended for beginners).</li> <li>It is the \"default\" file that GitHub reads upon opening the repository. It can be treated as the go-to file that explains what the repository is for, what each file does, how to cite your reasearch, amongst other things.</li> </ol> <p>Adding a Licence</p> <p>The addition of a licence can heavily contribute to the shareability of your code. Make sure that whichever licence you choose is in line with your principals as well as your project's. GitHub comes with a list of licences which you can review. It is also common to choose a licence later on! We will cover licences in more depth later in the course.</p> <p></p> <p>Ultimately, your new repository should look like the following screenshot. Notice the LICENCE document and the README.md</p> <p></p> <p>Editing the README.md</p> <p>The Github repository file has a .md extension which stands for Markdown. Markdown is a lightweight markup language for creating formatted text using a plain-text editor common throughout text files on the web. It uses symbols (*~-#`) for syntaxing text, and it is what GitHub (and this website!) use to format text. Markdown is easier to use than HTML. You can read more on Markdown on the Markdown Guide.</p> <p> </p>"},{"location":"00_basics/#adding-and-modifying-code-in-github","title":"Adding and Modifying Code in Github","text":"<p>GitHub allows you to add and modify code in two ways: 1. through the online portal (the webpage you're seeing), and 2. On your local computer. Throughout the following section, we will show you how to do it through the online portal. We will save the local computer for later for Lesson 5 later in the course. </p> <p>Adding code to your repository through the web page is suggested if what you want to add is simple (Like a README file!).</p> <ul> <li>Click the Add File button, which will allow you to either create a new file, or upload files from your computer. Select Create New File.</li> <li>The editing page will open: choose a name and an extension on the top of the page.</li> <li>On the editing page you can modify code as you see necessary (writing, pasting)</li> </ul> <p></p> <ul> <li>You can also see your changes (if formatted) with the preview function (with the Preview button).</li> <li>To \"Save\" your changes, you will need to commit your changes:<ul> <li>navigate at the bottom of the page, specify your commit with a name and add a description if necessary.  </li> </ul> </li> <li>You will be able to see your newly created file on your repository home after committing your changes.</li> </ul> <p>Committing changes</p> <p>Committing is the term used for saving changes you've made to your code. Each commit can be accessed within the GitHub web interface, which will show you the code prior and after the changes you've made. To see a list of all commits you made, click on the  icon under the Code button.</p> <ul> <li> <p>You can see from the picture below the lines that have been removed (in red), and the lines that have been added (in green). </p> </li> <li> <p>Additionally, you can also see the full list of commits made to the file or repository. </p> </li> </ul> <p></p>"},{"location":"00_basics/#hosting-web-pages-in-github","title":"Hosting Web Pages in Github","text":"<p>GitHub allows you to host web pages through the use of GitHub Pages. This is a free service that allows you to host a website directly from your GitHub repository. You can use this to host your personal website, or to host a website for your project.</p> <p>For example, the FOSS website is hosted through GitHub Pages. The repository for the website can be found here</p>"},{"location":"00_basics/#introduction-to-prompt-engineering","title":"Introduction to Prompt Engineering","text":"<p>This section is taken from the \"GPT101\" CyVerse Workshop</p> <p>A great set of tools that can help you with your own research, if used in the correct way, are the new Large Language Models (LLMs) available publicly. These include  ChatGPT,  Bard and  Bing Chat (integrated with  Microsoft Edge).</p>"},{"location":"00_basics/#llms-in-150-words-or-less","title":"LLMs in 150 words (or less)","text":"<p>How they're made: LLMs work by training on vast amounts of text from the internet. They learn patterns, grammar, and context from this data. When you give them a prompt, they generate text based on what they've learned. Imagine a super-smart autocomplete for text, but it can also create entire paragraphs or articles.</p> <p>How they work: LLMs don't understand like humans do. They predict what comes next in a sentence using math and probabilities. They don't have thoughts or feelings. They mimic human language but can make mistakes or write nonsense if not guided well.</p> <p>How you can use them: They're incredibly versatile. You can use them for answering questions, writing essays, coding help, and more. But you must be cautious because they can generate biased or false information if not used responsibly. </p> <p>In a nutshell, LLMs are like super-powered text generators trained on the internet's vast knowledge.</p>"},{"location":"00_basics/#prompt-writing","title":"Prompt Writing","text":"<p>GPT Chat asks for a message to begin its conversation. These messages are called \"Prompts\". </p> <p>Begin a conversation with a specific type of prompt. This will help narrow the potential range of responses and improve results to subsequent prompts. </p>"},{"location":"00_basics/#priming","title":"Priming","text":"<p>GPTs do better when provided with \"prompt primers\".</p> <p>Zero-shot unconditioned prompts are likely to return the least specific responses. </p> <p>Responses are more likely to be useful when multiple specific output types are defined.</p> Types of Priming Example Zero (Shot) \"Write five examples of assessments for watershed health.\" Single \"Write five examples of assessments for watershed health. Here is one example: Geomorphology\" Multiple \"Write five examples of assessments for watershed health related to geomorphology, water quality, and species diversity.\""},{"location":"00_basics/#prompt-structure","title":"Prompt Structure","text":"Role Task Format Act as [ROLE] Create a [TASK] ... show as [FORMAT] <p>Your prompt should specify the role in which ChatGPT responds, what its task is, and the format of how its outputs should be returned. </p> <p>A second step to the initial prompt is to link or chain your subsequent prompts. </p> <p>This lesson only covers ChatGPT, but the same prompt techniques can be used in other LLMs.</p>"},{"location":"00_basics/#role","title":"Role","text":"<p>Set the role for ChatGPT to play during your session. </p> <p>\"I want you to act as ...\" will establish what type of conversation you are planning to have. </p> Types of Roles Project Manager Copywriter / Editor Paper Reviewer Teacher / Mentor / Advisor Student / Learner / Participant Software Engineer DevOps Engineer Linux Terminal Python Interpreter Web Browser <p>Examples of roles you might ask for are: a domain science expert, an IT or DevOps engineer, software programmer, journal editor, paper reviewer, mentor, teacher, or student. You can even instruct ChatGPT to respond as though it were a Linux terminal, a web browser, a search engine, or language interpreter.</p> Data Scientist <p>Let's try an example prompt with role-playing to help write code in the R programming language.</p> <pre><code>I want you to act as a data scientist with complete knowledge of the R language, \nthe TidyVerse, and RStudio. \n\nWrite the code required to create a new R project environment,\nDownload and load the Palmer Penguins dataset, and plot regressions of body mass, \nbill length, and width for the species of Penguins in the dataset. \n\nYour response output should be in R and RMarkDown format \nwith text and code delineated with ``` blocks.\n\nAt the beginning of new file make sure to install any \nRStudio system dependencies and R libraries that Palmer Penguins requires.\n</code></pre> <p>Example can use <code>GPT-3.5-Turbo</code> or <code>GPT-4</code></p> Talk to Dead Scientists <p>Try to ask a question with and without Internet access enabled:</p> <p><pre><code>I want you to respond as though you are the mathematician Benoit Mandelbrot\n\nExplain the relationship of lacunarity and fractal dimension for a self-affine series\n\nShow your results using mathematical equations in LaTeX or MathJax style format\n</code></pre> Again, there is no guarantee that the results ChatGPT provides are factual, but it does greatly improve the odds that they are relevant to the prompt. Most importantly, these extensions provide citations for their results, allowing you to research the results yourself. </p>"},{"location":"00_basics/#tasks","title":"Tasks","text":"<p>Prompts which return informative responses to questions like \"What is ...\" or \"How does ...\"</p> <p>Because of ChatGPT's proclivity at making up information, using it without a way of validating the authenticity of its responses makes it less trustworthy than regular search engines. </p> Types of Task Scientific Article Essay Blog Post Outline Email Cover Letter Recipe Tutorial Lesson Plan Jupyter Notebook Configuration Code Software Script <p>Bing and Bard fill an important space in these types of prompts - they return websites which match the query criterion and allow you to research your own answers.</p> <p>There are extension tools for ChatGPT which allows you to prompt with references.</p>"},{"location":"00_basics/#format","title":"Format","text":"<p>By default ChatGPT outputs MarkDown syntax text. It can also output software code, and soon images, video, music and sounds.</p> Formats to output MarkDown Text (\\&amp; emojis) List Table HTML CSS Regular Expression CSV / TXT JSON Rich Text Gantt Chart Word Cloud Graphs Spreadsheets <p>You can also ask ChatGPT to explain complex topics or to act as a cook-book step-by-step guide. </p> <p>ChatGPT can provide instructional details about how to do specific tasks. </p> Documentation Writer <pre><code>I want you to act as a DIY expert. You will help me develop the skills necessary \nto complete simple lab documentation, create tutorials and guides for beginners and experts, \nand explain complex concepts in layman's terms using visual techniques, and develop helpful resources.\n\nI want you to create a tutorial for building and deploying a github.io website using the MkDocs Material Theme\n</code></pre>"},{"location":"00_basics/#further-documentation-questions","title":"Further Documentation &amp; Questions","text":"<p>For a more in depth quick start, go to the GPT 101 workshop. </p> <p>Documentation of interest:</p> <ul> <li>Read the  ChatGPT Documentation</li> <li>Read the  ChatGPT Technical Report</li> <li>Read the  Bard Documentation</li> </ul> How long can or should a prompt be? <p>The length of a prompt is measured in \"tokens\". A token can represent an individual character, a word, or a subword depending on the specific tokenization approach. A rough estimate for the average number of words in English language per token is <code>0.75</code>. </p> <p>Currently, ChatGPT version <code>GPT-3.5turbo</code> uses up to 2,048 tokens per prompt, GPT-4 and Bing Chat can take up to 32,768 tokens. BARD currently has a limit of 20,000 tokens in a prompt. </p> <p>This means that a 2,048 token prompt would be equivalent to about 1,536 words (3-6 pages), and a 32,768 token prompt would be 24,576 words (50-100 pages). </p> <p>However, this is only an approximation and may vary depending on the specific text and model. </p> <p>What this also means is that current GPT are not capable of reading many PDFs at one time, for example, to do a literature review, or to write a sequel to a novel or book series. </p> ChatGPT  Awesome Lists <p>There is an ever changing meta-list of  Awesome lists curated around ChatGPT plugins and extensions.</p> <p> search: <code>chatgpt+awesome</code></p> <p>Check out lists around:</p> <p> ChatGPT Prompts</p> <p> ChatGPT Data Science Prompts</p> <p> API plugins, extensions, &amp; applications</p> Access the Internet <p>By default, ChatGPT does not have access to the Internet, and is limited to the time period before September 2021 (as of mid-2023) for its training data time frame. </p> <p>There are third-party extensions, like WebChatGPT which you can install in your browser (Firefox or Chrome), that will extend OpenAI ChatGPT's reach to the internet.</p> <p>We presently recommend using  Bing Chat with Edge Browser instead of ChatGPT 3.5 for prompting which works with the internet.</p> <p> Bard also has access to the web and limited integration with Google Workspace.</p>"},{"location":"01_intro_open_sci/","title":"Introduction to Open Science","text":"<p>Learning Objectives</p> <p>After this lesson, you should be able to:</p> <ul> <li>Explain what Open Science is</li> <li>Explain the components of Open Science</li> <li>Describe the behaviors of Open Science</li> <li>Explain why Open Science matters in education, research, and society</li> <li>Understand the advantages and the challenges to Open Science</li> <li>Identify who the practitioners of Open Science are</li> <li>Understand the underlying Ethos of Open Science</li> </ul>"},{"location":"01_intro_open_sci/#2023-the-year-of-open-science","title":"2023: the Year of Open Science","text":"<p>The White House, joined by 10 federal agencies, and a coalition of more than 85 universities, declared 2023 the Year of Open Science. As the year came to a close, the governement aimed to create a spotlight through The White House Office of Science &amp; Technology Policy Open Science Recognition Challenge, recognizing open science stories that addressed current societial challenges (winners to be annouced).</p> <p>  The year of Open Science</p>"},{"location":"01_intro_open_sci/#what-is-open-science","title":"What is Open Science?","text":"<p>If you ask a dozen researchers this question, you will probably get just as many answers.</p> <p>This means that Open Science isn't necessarily a set of checkboxes you need to tick, but rather a holistic approach to doing science. In that spirit, it can also be useful to think about Open Science as a spectrum, from less to more open. </p> <p>Definitions</p> <p>\"Open Science is defined as an inclusive construct that combines various movements and practices aiming to make multilingual scientific knowledge openly  available,  accessible  and  reusable  for  everyone,  to  increase  scientific  collaborations  and  sharing of information for the benefits of science and society, and to open the processes of scientific knowledge creation, evaluation and communication to societal actors beyond the traditional scientific community.\" - UNESCO Definition</p> <ul> <li>UNESCO's Recommendation on Open Science</li> </ul> <p>\"Open Science is the movement to make scientific research (including publications, data, physical samples, and software) and its dissemination accessible to all levels of society, amateur or professional...\"   Wikipedia definition</p> <p>Open and Collaborative Science Network's Open Science Manifesto</p> Six Pillars  of Open Science <p> Open Access Publications</p> <p> Open Data</p> <p> Open Educational Resources</p> <p> Open Methodology</p> <p> Open Peer Review</p> <p> Open Source Software</p> Wait, how many pillars  of Open Science Really Are There? <p>The number can be from 4  to 8 </p> Foster Open Science Diagram <p> </p> <pre><code>Graphic by [Foster Open Science](https://www.fosteropenscience.eu/){target=_blank}\n</code></pre> <pre><code>flowchart LR\n\nid1([open science]) --&gt; id3([publishing]) &amp; id4([data]) &amp; id5([open source software])\n\nid3([publishing]) --&gt;  id41([access]) &amp; id42([reviews]) &amp; id43([methods]) &amp; id44([educational resources]) \n\nid5([open source software]) --&gt; id13([container registries]) &amp; id10([services]) &amp; id101([workflows]) &amp; id12([version control systems])\n\nid12([version control systems]) --&gt; id101([workflows])\n\nid13([container registries]) --&gt; id101([workflows])\n\nid14([public data registry]) --&gt; id101([workflows])\n\nid10([services]) --&gt; id101([workflows]) \n\nid44([educational resources]) --&gt; id21([university libraries])\n\nid21([university libraries]) --&gt; id101([workflows])\n\nid22([federal data archives]) --&gt; id101([workflows]) \n\nid4([data]) --&gt; id21([university libraries]) &amp; id22([federal data archives]) &amp; id14([public data registries]) \n\nid101([workflows]) --&gt; id15([on-premises]) &amp; id16([commercial cloud]) &amp; id17([public cloud])\n</code></pre> <p>Mermaid Diagram: Conceptual relationships of Open Science and cyberinfrastructure</p>  Awesome Lists of Open Science <p>Awesome lists were started on GitHub by Sindre Sorhus and typically have a badge associated with them  </p> <p>(There is even a Searchable Index of Awesome Lists)</p> <p>We have created our own Awesome Open Science List here which may be valuable to you.</p> <p></p>"},{"location":"01_intro_open_sci/#open-access-publications","title":"Open Access Publications","text":"<p>Definitions</p> <p>\"Open access is a publishing model for scholarly communication that makes research information available to readers at no cost, as opposed to the traditional subscription model in which readers have access to scholarly information by paying a subscription (usually via libraries).\" -- OpenAccess.nl</p> New Open Access Mandates in US <p>The White House Office of Science and Technology (OSTP) has recently released a policy stating that tax-payer funded research must by open access by 2026.</p> Open Access Publishing <p>Major publishers have provided access points for publishing your work </p> <p>AAAS</p> <p>Nature</p> <p>American Geophysical Union</p> <p>Commonwealth Scientific and Industrial Research Organisation (CSIRO)</p> <p>Open Research Europe</p> <p>PLOS</p> <p>MDPI</p> <p>Ecosphere</p> Financial Support for Open Access Publishing Fees <p>There are mechanisms for helping to pay for the additional costs of publishing research as open access:</p> <p>SciDevNet</p> <p>Health InterNetwork Access to Research Initiative (HINARI)</p> <p>Some institutions offer support for managing publishing costs (check to see if your institution has such support):</p> <p>University of Arizona Open Access Investment Fund</p> <p>Colorado University at Boulder Open Access Fund</p> <p>Max Planck Digital Library - German authors can have OA fees in Springer Nature research journals paid for.</p> <p>Bibsam Consortium - Swedish authors can have OA fees in Springer Nature research journals paid for.</p> Pre-print Services <p>ASAPbio Pre-Print Server List - ASAPbio is a scientist-driven non-profit promoting transparency and innovation comprehensive list of pre-print servers inthe field of life science communication.</p> <p>ESSOar - Earth and Space Science Open Archive hosted by the American Geophysical Union.</p> <p>Peer Community In (PCI) a free recommendation process of scientific preprints based on peer reviews</p> <p>OSF.io Preprints are partnered with numerous projects under the \"-rXivs\"</p> The rXivs <p>AfricArXiv</p> <p>AgrirXiv</p> <p>Arabixiv</p> <p>arXiv - is a free distribution service and an open-access archive for 2,086,431 scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.</p> <p>BioHackrXiv</p> <p>BioRxiv -  is an open access preprint repository for the biological sciences.</p> <p>BodorXiv</p> <p>EarthArXiv - is an open access preprint repository for the Earth sciences.</p> <p>EcsArXiv - a free preprint service for electrochemistry and solid state science and technology</p> <p>EdArXiv - for the education research community</p> <p>EngrXiv for the engineering community</p> <p>EvoEcoRxiv - is an open acccess preprint repository for Evolutionary and Ecological sciences.</p> <p>MediArXiv for Media, Film, &amp; Communication Studies</p> <p>MedRxiv - is an open access preprint repository for Medical sciences.</p> <p>PaleorXiv - is an open access preprint repository for Paleo Sciences</p> <p>PsyrXiv - is an open access preprint repository for Psychological sciences.</p> <p>SocArXiv - is an open access preprint repository for Social sciences.</p> <p>SportrXiv - is an open access preprint for Sports sciences.</p> <p>ThesisCommons - open Theses</p> <p> </p>"},{"location":"01_intro_open_sci/#open-data","title":"Open Data","text":"<p>Open Data are a critical aspect of open science. There are three key attributes of Open Data:</p> <ul> <li>Availability and accessibility</li> <li>Reusability</li> <li>Inclusivity</li> </ul> <p>Definitions</p> <p>\u201cOpen data and content can be freely used, modified, and shared by anyone for any purpose\u201d - The Open Definition</p> <p>\"Open data is data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and sharealike.\" - Open Data Handbook</p> <p> Wikipedia definition</p> DIKW Pyramid <p>Data are the basis of our understanding the natural world. The Data-Information-Knowledge-Wisdom (DIKW) pyramid describes for us how data are refined into information and knowledge.</p> <p></p> FAIR &amp; CARE Principles <p>FAIR Principles</p> <p>In 2016, the FAIR Guiding Principles for scientific data management and stewardship were published in Scientific Data. Read it.</p> <p>Findable</p> <ul> <li>F1. (meta)data are assigned a globally unique and persistent identifier</li> <li>F2. data are described with rich metadata (defined by R1 below)</li> <li>F3. metadata clearly and explicitly include the identifier of the data it describes</li> <li>F4. (meta)data are registered or indexed in a searchable resource</li> </ul> <p>Accessible</p> <ul> <li>A1. (meta)data are retrievable by their identifier using a     standardized communications protocol</li> <li>A1.1 the protocol is open, free, and universally implementable</li> <li>A1.2 the protocol allows for an authentication and authorization     procedure, where necessary</li> <li>A2. metadata are accessible, even when the data are no longer     available</li> </ul> <p>Interoperable</p> <ul> <li>I1. (meta)data use a formal, accessible, shared, and broadly     applicable language for knowledge representation.</li> <li>I2. (meta)data use vocabularies that follow FAIR principles</li> <li>I3. (meta)data include qualified references to other (meta)data</li> </ul> <p>Reusable</p> <ul> <li>R1. meta(data) are richly described with a plurality of accurate     and relevant attributes</li> <li>R1.1. (meta)data are released with a clear and accessible data     usage license</li> <li>R1.2. (meta)data are associated with detailed provenance</li> <li>R1.3. (meta)data meet domain-relevant community standard</li> </ul> <p>Open vs. Public vs. FAIR</p> <p>FAIR does not demand that data be open: See one definition of open: http://opendefinition.org/</p> <p>Why Principles?</p> <p>FAIR is a collection of principles. Ultimately, different communities within different scientific disciplines must work to interpret and implement these principles. Because technologies change quickly, focusing on the desired end result allows FAIR to be applied to a variety of situations now and in the foreseeable future.</p> <p>CARE Principles</p> <p>The CARE Principles for Indigenous Data Governance were drafted at the International Data Week and Research Data Alliance Plenary co-hosted event \"Indigenous Data Sovereignty Principles for the Governance of Indigenous Data Workshop,\" 8 November 2018, Gaborone, Botswana.</p> <p>Collective Benefit</p> <ul> <li>C1. For inclusive development and innovation</li> <li>C2. For improved governance and citizen engagement</li> <li>C3. For equitable outcomes</li> </ul> <p>Authority to Control</p> <ul> <li>A1. Recognizing rights and interests</li> <li>A2. Data for governance</li> <li>A3. Governance of data</li> </ul> <p>Responsibility</p> <ul> <li>R1. For positive relationships</li> <li>R2. For expanding capability and capacity</li> <li>R3. For Indigenous languages and worldviews</li> </ul> <p>Ethics</p> <ul> <li>E1. For minimizing harm and maximizing benefit</li> <li>E2. For justice</li> <li>E3. For future use</li> </ul> <p>FAIR - TLC</p> <p>Traceable, Licensed, and Connected</p> <ul> <li>The need for metrics: https://zenodo.org/record/203295#.XkrzTxNKjzI</li> </ul> <p>How to get to FAIR?</p> <p>This is a question that only you can answer, that is because it depends on (among other things)</p> <ol> <li>Your scientific discipline: Your datatypes and existing standards     for what constitutes acceptable data management will vary.</li> <li>The extent to which your scientific community has implemented     FAIR: Some disciplines have significant guidelines on FAIR, while     others have not addressed the subject in any concerted way.</li> <li>Your level of technical skills: Some approaches to implementing     FAIR may require technical skills you may not yet feel comfortable     with.</li> </ol> <p>While a lot is up to you, the first step is to evaluate how FAIR you think your data are:</p> Self FAIR assessment <p>Thinking about a dataset you work with, complete the ARDC FAIR assessment.</p> Resources <ul> <li>The FAIR Guiding Principles for scientific data management and stewardship</li> <li>Wilkinson et al. (2016) established the guidelines to improve the Findability, Accessibility, Interoperability, and Reuse (FAIR) of digital assets for research. </li> <li>Go-FAIR website</li> <li>Carroll et al. (2020) established the CARE Principles for Indigenous Data Governance. full document </li> <li>Indigenous Data Sovereignty Networks</li> </ul> <p>Connecting FOSS and CARE: Lydia Jennings</p> <p>Lydia was a Data Science Fellow at the University of Arizona, attending FOSS in the Fall of 2022. Since then, Lydia graduated from the University of Arizona's Department of Evironemtal Sciences, and published a paper on the application of the CARE principles to ecology and biodiversity research. Go Lydia!</p> <p>Check it out! Appying the 'CARE Principles for Indigenous Data Governance' to ecology and biodiversity, Nature Ecology &amp; Evolution, 2023.  </p> Linked Open Data Cloud <p>The Linked Open Data Cloud shows how data are linked to one another forming the basis of the semantic web .</p> <p></p> <p></p>"},{"location":"01_intro_open_sci/#open-educational-resources","title":"Open Educational Resources","text":"<p>Definitions</p> <p>\"Open Educational Resources (OER) are learning, teaching and research materials in any format and medium that reside in the public domain or are under copyright that have been released under an open license, that permit no-cost access, re-use, re-purpose, adaptation and redistribution by others.\" - UNESCO</p> <p> Wikipedia definition</p> Digital Literacy Organizations <p>The Carpentries - teaches foundational coding and data science skills to researchers worldwide  </p> <p>EdX - Massively Online Online Courses (not all open) hosted through University of California Berkeley</p> <p>EveryoneOn - mission is to unlock opportunity by connecting families in underserved communities to affordable internet service and computers, and delivering digital skills trainings </p> <p>ConnectHomeUSA - is a movement to bridge the digital divide for HUD-assisted housing residents in the United States under the leadership of national nonprofit EveryoneOn</p> <p>Global Digital Literacy Council -  has dedicated more than 15 years of hard work to the creation and maintenance of worldwide standards in digital literacy</p> <p>IndigiData - training and engaging tribal undergraduate and graduate students in informatics</p> <p>National Digital Equity Center a 501c3 non-profit, is a nationally recognized organization with a mission to close the digital divide across the United States</p> <p>National Digital Inclusion Allaince - advances digital equity by supporting community programs and equipping policymakers to act</p> <p>Net Literacy</p> <p>Open Educational Resources Commons</p> <p>Project Pythia is the education working group for Pangeo and is an educational resource for the entire geoscience community</p> <p>Research Bazaar - is a worldwide festival promoting the digital literacy emerging at the centre of modern research</p> <p>TechBoomers - is an education and discovery website that provides free tutorials of popular websites and Internet-based services in a manner that is accessible to older adults and other digital technology newcomers</p> Educational Materials <p>Teach Together by Greg Wilson</p> <p>DigitalLearn</p> <p></p>"},{"location":"01_intro_open_sci/#open-methodology","title":"Open Methodology","text":"<p>The use of version control systems like GitHub and GitLab present one of the foremost platforms for sharing open methods for digital research.</p> <p>Definitions</p> <p>\"An open methodology is simply one which has been described in sufficient detail to allow other researchers to repeat the work and apply it elsewhere.\" - Watson (2015)</p> <p>\"Open Methodology refers to opening up methods that are used by researchers to achieve scientific results and making them publicly available.\" - Open Science Network Austria</p> Protocols and Bench Techniques <p>BioProtocol</p> <p>Current Protocols</p> <p>Gold Biotechnology Protocol list</p> <p>JoVE - Journal of Visualized Experiments</p> <p>Nature Protocols</p> <p>OpenWetWare</p> <p>Protocol Exchange</p> <p>Protocols Online</p> <p> Protocols</p> <p>SciGene</p> <p>Springer Nature Experiments</p> Concept of Preregistration <p>In response to the Reproducibility Crisis, many researchers, particularly in fields like psychology, have begun to advocate for preregistration of studies. </p> <p>This involves writing out and publishing your entire research plan, from data collection to analysis and publication, for the sake of avoiding practices like p-hacking or HARKing. </p> <p>What preregistration also does is make the process of your work more open, including many of the small decisions and tweaks you make to a project that probably wouldn't make it into a manuscript. </p> <p>To learn more about preregistration, you can check out the Open Science Foundation, a project that provides a preregistration platform and other Open Science tools. You can also read this publication</p> <p></p>"},{"location":"01_intro_open_sci/#open-peer-review","title":"Open Peer Review","text":"<p>Pros and Cons of Open Peer Review</p> <p>Definitions</p> <p>Ross-Hellauer et al. (2017) ask What is Open Peer Review? and state that there is no single agreed upon definition</p> <p> Wikipedia's definition</p> <p>A manuscript review process that includes some combination of Open Identities, Open Reports, Open Participation, and even Open Interaction</p> Open Peer Review Resources <p>F1000Research the first open research publishing platform. Offering open peer review rapid publication</p> <p>PREreview provides a space for open peer reviews, targeted toward early career researchers.</p> <p>ASAPbio Accelerating Science and Publication in Biology, an open peer review source for biologists and life scientists.</p> <p>PubPeer platform for post-publication of peer reviews.</p> <p>Sciety platform for evaluating preprints.</p> <p></p>"},{"location":"01_intro_open_sci/#open-source-software","title":"Open Source Software","text":"<p>Definitions</p> <p>\"Open source software is code that is designed to be publicly accessible\u2014anyone can see, modify, and distribute the code as they see fit. Open source software is developed in a decentralized and collaborative way, relying on peer review and community production.\" -  Red Hat</p> <p> Open Source Initiative definition</p> <p> Wikipedia definition</p> <p>Awesome list</p>"},{"location":"01_intro_open_sci/#breakout-discussion-1","title":"Breakout Discussion 1","text":"<p>As you already know, being a scientist requires you to wear many hats, and trying to do Open Science is no different.</p> <p> Bernery et al. (2022) Figure 2: The positive aspects of doing a PhD. </p> <p>As we mentioned, Open Science is not a set of boxes you need to check off to be \"Certified Open\", but an intersecting set of philosophies and approaches, all of which occur on some type of spectrum. </p> <p>To get a feel for how Open Science can be multifaceted and different for each researcher, we will do a short breakout group session to discuss what Open Science means to you.</p> What does Open Science mean to you? Which of the  pillars of Open Science is nearest to your own heart? <p> Open Access Publications</p> <p> Open Data</p> <p> Open Educational Resources</p> <p> Open Methodology</p> <p> Open Peer Review</p> <p> Open Source Software</p> Are any of the  pillars more important than the others? Are there any  pillars not identified that you think should be considered? What characteristics might a paper, project, lab group require to qualify as doing Open Science What are some limitations to you, your lab group, or your domain?"},{"location":"01_intro_open_sci/#why-do-open-science","title":"WHY do Open Science?","text":"<p>There are many reasons to do Open Science, and presumably one or more of them brought you to this workshop. </p> <p>Whether you feel an ethical obligation, want to improve the quality of your work, or want to look better to funding agencies, many of the same approaches to Open Science apply.</p> <p>A paper from Bartling &amp; Friesike (2014) posits that there are 5 main schools of thought in Open Science, which represent 5 underlying motivations:</p> <ol> <li> <p>Democratic school: primarily concerned with making scholarly work freely available to everyone</p> </li> <li> <p>Pragmatic school: primarily concerned with improving the quality of scholarly work by fostering collaboration and improving critiques</p> </li> <li> <p>Infrastructure school: primarily focused on the platforms, tools, and services necessary to conduct efficient research, collaboration, and communication</p> </li> <li> <p>Public school: primarily concerned with societal impact of scholarly work, focusing on engagement with broader public via citizen science, understandable scientific communication, and less formal communication</p> </li> <li> <p>Measurement school: primarily concerned with the existing focus on journal publications as a means of measuring scholarly output, and focused on developing alternative measurements of scientific impact</p> </li> </ol> <p>  In Bartling &amp; Friesike (2014) Open Science: One Term, Five Schools of Thought </p> <p>While many researchers may be motivated by one or more of these aspects, we will not necessarily focus on any of them in particular. If anything, FOSS may be slightly more in the Infrastructure school, because we aim to give you the tools to do Open Science based on your own underlying motivations.</p>"},{"location":"01_intro_open_sci/#breakout-discussion-2","title":"Breakout Discussion 2","text":"<p>Let's break out into groups again to discuss some of our motivations for doing Open Science.</p> What motivates you to do Open Science? Do you feel that you fall into a particular \"school\"? If so, which one, and why? Are there any motivating factors for doing Open Science that don't fit into this framework?"},{"location":"01_intro_open_sci/#ethos-of-open-science","title":"Ethos of Open Science","text":"<p>Doing Open Science requires us to understand the ethics of why working with data which do not belong to us is privileged.</p> <p>We must also anticipate how these could be re-used in ways contrary to the interests of humanity. </p> <p>Ensure the use of Institutional Review Boards (IRB) or your local ethical committee. </p> <p>Areas to consider: </p> <p> </p> <p>Source: UK Statistics Authority </p> <ul> <li>Geolocation (survey, land ownership, parcel data), see UK Statistics Authority Ethical Considerations </li> <li>Personal identification information  US Personal Identifiable Information (PII), General Data Protection Regulation (GDPR)</li> <li>Health information US HIPAA , EU GDPR</li> <li>Protected and Endangered Species (US Endangered Species Act)</li> <li>Indigenous data sovereignty: CARE Principles for Indigenous Data Governance , Global Indigenous Data Alliance (GIDA), First Nations OCAP\u00ae (Ownership Control Access and Possession), Circumpolar Inuit Protocols for Equitable and Ethical Engagement </li> <li> <p>Artificial intelligence/machine learning Assessment List Trustworthy AI (ALTAI) from the European AI Alliance</p> </li> <li> <p> \"Nothing about us, without us\"</p> </li> <li> <p>Funnel et al. (2019)</p> </li> </ul> <p>For more information (training):</p> <p>January in Tucson - intensive education session brings together distinguished faculty in the field of Indigenous governance and Indigenous rights, and gives them the opportunity to teach and hold discussions with Indigenous leaders, practitioners, and community members, and anyone interested in Indigenous affairs.</p> <p>Ethics and Data Access (General Information with BioMedical and Life Sciences Data) includes a legal and ethical checklist lesson for researchers around \"FAIR Plus\".</p>"},{"location":"01_intro_open_sci/#recommended-open-science-communities","title":"Recommended Open Science Communities","text":"<p> Open Scholarship Grassroots Community Networks</p>  International Open Science Networks <p>Center for Scientific Collaboration and Community Engagement (CSCCE)</p> <p>Center for Open Science (COS)</p> <p>Eclipse Science Working Group</p> <p>eLife</p> <p>NumFocus</p> <p>Open Access Working Group</p> <p>Open Research Funders Group</p> <p>Open Science Foundation</p> <p>Open Science Network</p> <p>pyOpenSci</p> <p>R OpenSci</p> <p>Research Data Alliance (RDA)</p> <p>The Turing Way</p> <p>UNESCO Global Open Science Partnership</p> <p>World Wide Web Consortium (W3C)</p>  US-based Open Science Networks <p>CI Compass - provides expertise and active support to cyberinfrastructure practitioners at USA NSF Major Facilities in order to accelerate the data lifecycle and ensure the integrity and effectiveness of the cyberinfrastructure upon which research and discovery depend.</p> <p>Earth Science Information Partners (ESIP) Federation -  is a 501\u00a9(3) nonprofit supported by NASA, NOAA, USGS and 130+ member organizations.</p> <p>Internet2 - is a community providing cloud solutions, research support, and services tailored for Research and Education. </p> <p>Minority Serving Cyberinfrastructure Consortium (MS-CC) envisions a transformational partnership to promote advanced cyberinfrastructure (CI) capabilities on the campuses of Historically Black Colleges and Universities (HBCUs), Hispanic-Serving Institutions (HSIs), Tribal Colleges and Universities (TCUs), and other Minority Serving Institutions (MSIs). </p> <p>NASA Transform to Open Science (TOPS) - coordinates efforts designed to rapidly transform agencies, organizations, and communities for Earth Science</p> <p>OpenScapes - is an approach for doing better science for future us</p> <p>The Quilt - non-profit regional research and education networks collaborate to develop, deploy and operate advanced cyberinfrastructure that enables innovation in research and education.</p>  Oceania Open Science Networks <p>New Zealand Open Research Network - New Zealand Open Research Network (NZORN) is a collection of researchers and research-associated workers in New Zealand.</p> <p>Australia &amp; New Zealand Open Research Network - ANZORN is a network of local networks distributed without Australia and New Zealand.</p>"},{"location":"01_intro_open_sci/#self-assessment","title":"Self Assessment","text":"True or False: All research papers published in the top journals, like Science and Nature, are always Open Access? Answer <p>False</p> <p>Major Research journals like Science and Nature have an \"Open Access\" option when a manuscript is accepted, but they charge an extra fee to the authors to make those papers Open Access.</p> <p>These high page costs are exclusionary to the majority of global scientists who cannot afford to front these costs out of pocket.</p> <p>This will soon change, at least in the United States. The Executive Branch of the federal government recently mandated that future federally funded research be made Open Access after 2026.</p> True or False: an article states all of the research data used in the experiments \"are available upon request from the corresponding author(s),\" meaning the data are \"Open\" Answer <p>False</p> <p>In order for research to be open, the data need to be freely available from a digital repository, like Data Dryad, Zenodo.org, or CyVerse.</p> <p>Data that are 'available upon request' do not meet the FAIR data principles. </p> True or False: Online Universities and Data Science Boot Camps like UArizona Online, Coursera, Udemy, etc. promote digital literacy and are Open Educational Resources? Answer <p>False</p> <p>These examples are for-profit programs which teach data science and computer programming online. Some may be official through public or private universities and offer credits toward a degree or a certificate. Some of these programs are known to be predatory.</p> <p>The organizations we have listed above are Open Educational Resources - they are free and available to anyone who wants to work with them asynchronously, virtually, or in person.</p> Using a version control system to host the analysis code and computational notebooks, and including these in your Methods section or Supplementary Materials, is an example of an Open Methodology? Answer <p>Yes!</p> <p>Using a VCS like GitHub or GitLab is a great step towards making your research more reproducible. </p> <p>Ways to improve your open methology can include documentation of your physical bench work, and even video recordings and step-by-step guides for every part of your project.</p> You are asked to review a paper for an important journal in your field. The editor asks if you're willing to release your identity to the authors, thereby \"signing\" your review. Is this an example of \"Open Peer Review\"? Answer <p>No</p> <p>Just because you've given your name to the author(s) of the manuscript, this does not make your review open.</p> <p>If the journal later publishes your review alongside the final manuscript, than you will have participated in an Open Review. </p> You read a paper where the author(s) wrote their own code and licensed as \"Open Source\" software for a specific set of scientific tasks which you want to replicate. When you visit their personal website, you find the GitHub repository does not exist (because its now private). You contact the authors asking for access, but they refuse to share it 'due to competing researchers who are seeking to steal their intellectual property\". Is the software open source? Answer <p>No</p> <p>Just because an author states they have given their software a permissive software license, does not make the software open source. </p> <p>Always make certain there is a LICENSE associated with any software you find on the internet. </p> <p>In order for the software to be open, it must follow the Open Source Initiative definition</p>"},{"location":"02_project_management/","title":"Introduction to Project Management","text":"<p>Learning Objectives</p> <p>After this lesson, you should be able to:</p> <ul> <li>Discuss different levels of project management</li> <li>Describe tools and approaches to managing collaborative projects</li> <li>Describe best practices for computational project organization</li> <li>Understand benefits of establishing project management practices from the start of a project until after it ends</li> </ul> <p>\"Project Management\" by itself may sound a bit vague and broad. </p> Definition <p>\"Project management is the use of specific knowledge, skills, tools and techniques to deliver something of value to people. The development of software for an improved business process, the construction of a building, the relief effort after a natural disaster, the expansion of sales into a new geographic market\u2014these are all examples of projects.\" - Project Management Institute </p> <p> Wikipedia definition</p> <p>Here we use the term in two different contexts.</p> <ol> <li> <p>First, we'll go over the project management of scientific labs, groups, and projects, talking about things like governance, how to develop operations manuals, laying out roles and responsibilities, planning steps and the workflows which connect them. </p> </li> <li> <p>Next, we'll go over project management as \"research objects\": making sure your data, code, and documents are well-organized. These are crucial for future topics like version control and reproducibility.</p> </li> </ol>"},{"location":"02_project_management/#1-classic-project-management","title":"1. Classic Project Management","text":"<p>This type of overall project management may be required for some grants, and while it may be tempting to put in the minimal effort on one of the many pieces of paperwork you're required to complete, this type of overall project planning can be very useful. </p> Traditional Organizations <p>Major research (R1) universities are organized around hierarchical frameworks, often described using an Organizational Chart.</p> <p> </p> <p>Scientific Research Projects are generally organized around a \"Principal Investigator\" with \"Co-Principal Investigators\" and \"Senior Personnel\" in supporting roles. Postdoctoral Researchers and gradaute students are often employed by research projects as part of their continued education and \"professional preparation\". </p> <p></p> <p>Given the nebulous breakdown of authority within lab groups and small research projects, the organization and governance of teams can be difficult to determine from the outside perspective. Indeed, internally team members on projects often do not know who is in charge or who reports to whom.</p> <p>The Turing Way offer a lesson on Project Design related to effective project planning and management.</p>"},{"location":"02_project_management/#project-governance","title":"Project Governance","text":"Definitions <p>Project Governance is the set of rules, procedures and policies that determine how projects are managed and overseen.</p> <p>\"The set of policies, regulations, functions, processes, and procedures and responsibilities that define the establishment, management and control of projects, programmes or portfolios.\" - APM (2012), open.edu</p> <p> Wikipedia Definition</p> <p>No matter how small, i.e., even single person-run projects, a good Project Governance structure can help keep work on track and headed toward a timely finish.</p> <p>Establishing a project governance document at the onset of a project is a good way of setting boundaries, roles and responsibilities, pre-registration about what deliverables are expected, and what the consequences will be for breaking trust.</p> Example Governance Documents <p>Munoz-Torres et al. 2020</p>"},{"location":"02_project_management/#research-collaborations","title":"Research Collaborations","text":"<p>Sahneh &amp; Balk et al. (2020) Ten simple rules to cultivate transdisciplinary collaboration in data science, discuss the interactions amongst teams of diverse researchers.</p> <p> Sahneh &amp; Balk et al. (2020) Fig 1. How the rules work together and intersect. There are multiple components in collaborations: person\u2013person interactions, person\u2013technology interactions, person\u2013data interactions, and data\u2013technology interactions. Synergy between these components results in a successful collaboration. </p>"},{"location":"02_project_management/#team-roles-and-responsibilities","title":"Team Roles and Responsibilities","text":"<p>It can be easy for certain tasks to slip through the cracks. Established roles and responsibilities of teams can help ensure nobody gets saddled with too much work, and reduces chances of disputes among collaborators.</p> Project Management Professional (PMP)\u00ae <p>A Project Management Professional (PMP)\u00ae certification has been embraced globally as adding value to your professional resume. </p> <p>Academia has also embraced PMP certification as part of continuing education for academic staff and faculty.</p> <p>University of Arizona PMP prep</p> Team roles and titles <p>Again, The Turing Way provide an excellent set of examples of infrastructure job titles and roles on software driven projects:</p> <p>Community Manager - \"responsibilities include establishing engagement, organising community spaces and events, supporting people through inclusive practices, developing and maintaining resources, growing and evaluating use cases and collaborating with people involved in research and scientific communities.\" (1, 2)</p> <p> This image was created by Scriberia for The Turing Way community and is used under a CC-BY 4.0 licence. </p> <p>Data Science Educator - \"... data science in education refers to the application of data science methods, while other times it refers to data science as a context for teaching and learning\" Rosenberg et al. (2020), Estrellado et al.</p> <p>Data Scientist - a professional who uses analytical, statistical, and programming skills to collect, analyze, and describe data.</p> <p>Data Steward - \"... responsible for ensuring the quality and fitness for purpose of the organization's data assets, including the metadata for those data assets.\" - Wikipedia</p> <p>Developer Advocate - sometimes called platform evangelism, advocates represent the voice of the user (in the case of open science, the scientists) internally to the project team or company, and the voice of the project or company externally to the public.</p> <p>DevOps Engineer - a combinination of software development \"Dev\" and IT operations \"Ops\", responsibilities focus on \"continuous delivery\" and agile software development</p> <p>Research Application Manager (RAM) - in some ways a combination of Community Manager and Developer Advocate,</p> <p> Fig. 94 Research Application Managers work with the research team to embed outputs into user organisations. The Turing Way Community, &amp; Scriberia. (2020, November). Illustrations from the Turing Way book dashes. Zenodo. http://doi.org/10.5281/zenodo.4323154 </p> <p>Research Software Engineer - those who regularly use expertise in programming to advance research - US Research Software Engineer (US-RSE) Association</p>"},{"location":"02_project_management/#open-source-research-software-maintainer","title":"Open Source Research Software Maintainer","text":"<p>Becoming an open source software maintainer is not to be taken lightly.</p> <p>  Image Credit: XKCD Dependency </p> <p>When you create a new software, library, or package, you are becoming its parent and guardian. </p>"},{"location":"02_project_management/#development-methodology","title":"Development Methodology","text":"the \"leaps of faith\" required in Agile vs Waterfall. Image Credit: Wikimedia Commons CC BY 4.0  <p>In software development, there are two common methologies which have similar applications to a research project:</p> <ul> <li> Agile<ul> <li>Scrum</li> <li>Kanban</li> </ul> </li> <li> Waterfall</li> </ul> <p> </p>  the effort distribtion of Agile vs Waterfall. Image Credit: Wikimedia Commons CC BY 4.0  <p>Comparisons between methodologies</p> <ul> <li> <p>LucidChart Blog: Agile vs Waterfall vs Kanban vs Scrum  </p> </li> <li> <p>Ontology of Value: Agile vs Waterfall vs Kanban vs Scrum</p> </li> </ul>"},{"location":"02_project_management/#breakout-discussion","title":"Breakout Discussion","text":"<p>Now we will do a breakout discussion section to talk about overall project management.</p> <p>What is an example of a poorly managed project you were involved in? What contributed to this feeling?</p> <p>Why do you think effective project management is important to Open Science?</p> <p>What are some limitations to you, your lab/group, or your domain?</p>"},{"location":"02_project_management/#2-research-objects","title":"2. Research Objects","text":"Definition <p>\"A workflow-centric research object bundles a workflow, the provenance of the results obtained by its enactment, other digital objects that are relevant for the experiment (papers, datasets, etc.), and annotations that semantically describe all these objects.\" - Corcho et al. 2012</p> <p>\"... semantically rich aggregations of resources, that can possess some scientific intent or support some research objective.\" - Bechhofer et al. 2010</p> <p> Wikipedia definition</p> <p>When we talk about project management in this section, we mean the way you organize data, code, images, documents, and documentation within a project. One way to think about this is in the context of \"research objects\" which condense into a single end point (think: a URL like a digital object identifier (DOI)) where others can come to reproduce your research. </p> Examples of Research Objects <p>Boettiger 2018</p> <p>Gillan et al. 2021</p> <p>  Research Objects from ResearchObject.org</p> Research Object Services <p>ResearchObject</p> <p>ROHub - Garcia-Silva et al. 2019</p>"},{"location":"02_project_management/#research-object-organization","title":"Research Object Organization","text":"<p>If you've ever had to navigate someone else's computer or a GitHub repository, you probably know that a poorly organized project can greatly reduce its accessibility. On the other hand, a well-organized project can:</p> <ul> <li>make your work more accessible to others</li> <li>help collaborators effectively contribute to your project</li> <li>ease the growing pains of a rapidly scaling project</li> <li>make life much easier for your future self</li> </ul> <p>It can be easy to overlook sound project management, opting for a \"just get it done ASAP\" approach to your work, but this almost always costs you more time in the end. The best time to introduce good project management is at the start of a project, and the second best time is right now. </p> <p>  An hour spent reorganizing a project today may save you days of headaches later on.</p>"},{"location":"02_project_management/#organization-examples","title":"Organization Examples","text":"<ul> <li>Example data project organization from UArizona Libraries</li> <li>CookieCutter Templates</li> </ul> <p>Example project structure:</p> <pre><code>.\n\u251c\u2500\u2500 AUTHORS.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 bin                &lt;- Your compiled model code can be stored here (not tracked by git)\n\u251c\u2500\u2500 config             &lt;- Configuration files, e.g., for doxygen or for your model if needed\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 external       &lt;- Data from third party sources.\n\u2502   \u251c\u2500\u2500 interim        &lt;- Intermediate data that has been transformed.\n\u2502   \u251c\u2500\u2500 processed      &lt;- The final, canonical data sets for modeling.\n\u2502   \u2514\u2500\u2500 raw            &lt;- The original, immutable data dump.\n\u251c\u2500\u2500 docs               &lt;- Documentation, e.g., doxygen or scientific papers (not tracked by git)\n\u251c\u2500\u2500 notebooks          &lt;- Ipython or R notebooks\n\u251c\u2500\u2500 reports            &lt;- For a manuscript source, e.g., LaTeX, Markdown, etc., or any project reports\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 figures        &lt;- Figures for the manuscript or reports\n\u2514\u2500\u2500 src                &lt;- Source code for this project\n    \u251c\u2500\u2500 data           &lt;- scripts and programs to process data\n    \u251c\u2500\u2500 external       &lt;- Any external source code, e.g., pull other git projects, or external libraries\n    \u251c\u2500\u2500 models         &lt;- Source code for your own model\n    \u251c\u2500\u2500 tools          &lt;- Any helper scripts go here\n    \u2514\u2500\u2500 visualization  &lt;- Scripts for visualisation of your results, e.g., matplotlib, ggplot2 related.\n</code></pre> Best Practices <ol> <li> <p>Projects should be self-contained</p> <ul> <li>this is probably the most important concept</li> <li>strictly necessary for version control</li> <li>use relative paths</li> </ul> </li> <li> <p>Use structure to organize files</p> </li> <li> <p>Don't underestimate complexity</p> </li> <li> <p>Keep raw data raw</p> </li> <li> <p>Treat generated output as disposable</p> </li> <li> <p>Avoid manual (point-and-click) steps as much as possible</p> <ul> <li>if necessary, record in detail</li> <li>should also be recorded in prior and subsequent steps</li> </ul> </li> <li> <p>Avoid spaces in file and folder names</p> <ul> <li>consider <code>snake_case</code> <code>camelCase</code> <code>PascalCase</code> <code>kebab-case</code> instead</li> </ul> </li> <li> <p>Describe structure in README</p> </li> <li> <p>The best time to organize is at the start, the 2<sup>nd</sup> best is right now</p> </li> <li> <p>Reorganize if necessary, but don't overdo it</p> </li> <li> <p>Using same basic structure can help you navigate new/old projects</p> </li> </ol> Automate the creation a working directory <p>You might find a nice basic structure that works as a good starting place for many of your projects, or smaller components of big projects.</p> <p>Instead of having to repeat the process of making that directory structure, which could be tedious and introduce mistakes, you could write some code to do it for you. </p> <p>The following is a <code>bash</code> script that takes one argument, the name of the new project (with no spaces), and creates that project with a premade directory structure for you to put files into.</p> <pre><code>#!/usr/bin/env bash\n\n# Run this script with the name of the new project as \n# an argument, like so: `bash make_project.sh my_project`\n# It will generate a project with the following structure:\n\n#.\n#|-- README.md\n#|-- data\n#|   |-- cleaned\n#|   `-- raw\n#|-- images\n#|-- reports\n#`-- scripts\n\nmkdir \"$1\"\n\ncd \"$1\" || exit\n\necho \"# $1\" &gt;&gt; README.md\n\nmkdir data\n\nmkdir data/raw\n\nmkdir data/cleaned\n\nmkdir scripts\n\nmkdir images\n\nmkdir reports\n</code></pre> <p>This approach to automating repetitive tasks is something we'll dig into even deeper in later lessons.</p> Productivity Software <p> CryptPad - online rich text pad. </p> <p> Draw.io - drawings and diagrams in browser.</p> <p> Excel - love it or hate it, many people still work in it or with <code>.xlsx</code> format files. </p> <p> Google Docs - is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google.</p> <p> HackMD - online markdown editor.</p> <p> JupyterBook - create documentation using Jupyter Notebooks and Markdown</p> <p> MkDocs - is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation.</p> <p> LaTeX -  is a high-quality typesetting system</p> <p> Overleaf - LaTeX online document sharing platform.</p> <p> ReadTheDocs - documentation using a variety of Markup langages</p> <p>Software Heritage - preserves software source code for present and future generations.</p>  Project Management Software <p>OSF.io</p> <ul> <li>Examples</li> </ul> <p> Atlassian</p> <ul> <li> <p> Confluence</p> </li> <li> <p> Jira</p> </li> <li> <p> Trello</p> </li> </ul> <p> GitHub Issues</p> <p>Open Project</p> <p> ZenHub</p> <p>Basecamp</p>"},{"location":"02_project_management/#breakout-discussion_1","title":"Breakout Discussion","text":"<p>Now we will do a breakout discussion section to talk about research objects</p> <p>Who here has created a research object or attempted to?</p> <p>Do you think someone could reproduce your research by accessing your research object?</p> <p>Where might a research object not work for your research?</p> <p>What would a research object look like for your research?</p>"},{"location":"02_project_management/#other-resources","title":"Other Resources","text":"<p>There are many other resources on more specific elements of project management. We'll link to some of them here.</p> <ul> <li>Using R Projects with RStudio: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects</li> <li>Using the R package <code>here</code>: https://github.com/jennybc/here_here and https://here.r-lib.org/</li> <li>An even more compartmentalized approach to project management: https://hrdag.org/2016/06/14/the-task-is-a-quantum-of-workflow/</li> </ul>"},{"location":"02_project_management/#self-assessment","title":"Self Assessment","text":"Why is Project Management used in research? <ol> <li> <p>Reduces [wasted] effort </p> </li> <li> <p>Tracks progress or identifies more quickly when there is a lack of progress</p> </li> <li> <p>Establishes a formal structure for teams</p> </li> </ol> What are established roles and responsibilities of collaborative teams? <p>Example 1: Traditional University Research Teams</p> <p>i. Principal Investigator, Co-Principal Investigators</p> <p>ii. Senior Personnel, Postdoctoral Researchers, Bench Scientists </p> <p>iii. (Graduate) Students</p> <p>Example 2: Research Infrastructure Teams</p> <p>Research infrastructure job titles and roles (Turing Way)</p> <p>i. Community Managers</p> <p>ii. Data Science Educators</p> <p>iii. Data Scientists</p> <p>iv. Developer Advocates</p> <p>iv. Research Software Engineers</p> What are some uses of a Project Governance Document? Answers <ul> <li> <p>Sets expectations for behavior and operations</p> </li> <li> <p>Establishes roles and responsibilities of PI, staff, and senior personnel</p> </li> <li> <p>Uses Pre-registration techniques about what deliverables are expects, and by when </p> </li> <li> <p>Establishes what consequences will be for breaking trust</p> </li> </ul> Research Objects must include all components of research: governance document, manuals, documentation, research papers, analysis code, data, software containers Answers <p>While a Research Object (RO) may include the entire kitchen sink from a research project, it does NOT always contain all of these things.</p> <p>Fundamentally, a RO should contain enough information and detail to reproduce a scientific study from its linked or self-contained parts. </p> <p>Components like large datasets may not be a part of the RO, but the code or analysis scripts should have the ability to connect to or stream those data.</p>"},{"location":"03_managing_data/","title":"Managing Data","text":"<p>Learning Objectives</p> <p>After this lesson, you should be able to:</p> <ul> <li>Recognize data as the foundation of open science and be able to describe the \"life cycle of data\"</li> <li>Use self-assessments to evaluate your current data management practices</li> <li>Cite tools and resources to improve your data management practices</li> <li>Know the biggest challenge to effective data management</li> </ul>"},{"location":"03_managing_data/#why-should-you-care-about-data-management","title":"Why should you care about data management?","text":"<p>Ensuring that data are effectively organized, shared, and preserved is critical to making your science impactful, efficient, and open.</p> Warning <p>The biggest challenge to data management is making it an afterthought.</p> <p>Unfortunately, poor data management doesn't have a high upfront cost. You can do substantial work before realizing you are in trouble. Like a swimmer in rip current, by the time you realize you are in trouble, you may already be close to drowning.</p> <p>The solution? Make data management the first thing you consider when starting a research project. It also needs to be a policy you institute right away for your research group.</p> How would you answer? <ul> <li>If you give your data to a colleague who has not been involved with your project, would they be able to make sense of it? Would they be able to use it properly?</li> <li>If you come back to your own data in five years, will you be able to make sense of it? Will you be able to use it properly?</li> <li>When you are ready to publish a paper, is it easy to find all the correct versions of all the data you used and present them in a comprehensible manner?</li> </ul> <p>Well-managed Data Sets:</p> <ul> <li>Make life much easier for you and your collaborators</li> <li>Benefit the scientific research community by allowing others to reuse your data</li> <li>Are required by most funders and many journals</li> <li>Recent Dear Colleague letter from NSF</li> <li>NSF proposal preparation guidelines</li> </ul> <p> </p>"},{"location":"03_managing_data/#data-types","title":"Data Types","text":"<p>Different types of data require different management practices. What are some data types and sources you might use in your work? (Adapted from DMP Tool Data management general guidance)</p> <p>Data Types</p> <ul> <li>Text: field or laboratory notes, survey responses</li> <li>Numeric: tables, counts, measurements</li> <li>Audiovisual: images, sound recordings, video</li> <li>Models, computer code</li> <li>Discipline-specific: FASTA in biology, FITS in astronomy, CIF in chemistry</li> <li>Instrument-specific: equipment outputs</li> </ul> <p>Data Sources</p> <p>Observational</p> <ul> <li>Captured in real-time, typically outside the lab</li> <li>Usually irreplaceable and therefore the most important to safeguard</li> <li>Examples: Sensor readings, telemetry, survey results, images</li> </ul> <p>Experimental</p> <ul> <li>Typically generated in the lab or under controlled conditions</li> <li>Often reproducible, but can be expensive or time-consuming</li> <li>Examples: gene sequences, chromatograms, magnetic field readings</li> </ul> <p>Simulation</p> <ul> <li>Machine generated from test models</li> <li>Likely to be reproducible if the model and inputs are preserved</li> <li>Examples: climate models, economic models</li> </ul> <p>Derived / Compiled</p> <ul> <li>Generated from existing datasets</li> <li>Reproducible, but can be very expensive and time-consuming</li> <li>Examples: text and data mining, compiled database, 3D models</li> </ul> <p> </p>"},{"location":"03_managing_data/#data-self-assessment","title":"Data Self-assessment","text":"<p>Activity</p> <p>In small groups, discuss the following questions. You will be provided with a space for documenting our shared answers.</p> <p>1. What are the two or three data types that you most frequently work with?         -   Think about the sources (observational, experimental, simulated, compiled/derived)         -   Also consider the formats (tabular, sequence, database, image, etc.)</p> <p>2.  What is the scale of your data?</p> Tip <p>We often talk about the scale of data using the \"Three V's\":</p> <ul> <li>Volume: Size of the data (MBs, GBs, TBs); can also include how many files (e.g dozens of big files, or millions of small ones)</li> <li>Velocity: How quickly are these data produced and analyzed? A lot coming in a single batch infrequently, or, a constant small amount of data that must be rapidly analyzed?</li> <li>Variety: How many different data types (raw files? databases?) A fourth V (Veracity) captures the need to make decisions about data processing (i.e., separating low- and high-quality data)</li> </ul> <p>3.  What is your strategy for storing and backing up your data?</p> <p>4.  What is your strategy for verifying the integrity of your data? (i.e. verifying that your data has not be altered)</p> <p>5.  What is your strategy for searching your data?</p> <p>6.  What is your strategy for sharing (and getting credit for) your data? (i.e. How will do you share with your community/clients? How is that sharing documented? How do you evaluate the impact of data shared? )</p> <p> </p>"},{"location":"03_managing_data/#the-data-life-cycle","title":"The Data Life Cycle","text":"<p>Tip</p> <p>The Data Life Cycle</p> <p>Data management is the set of practices that allow researchers to effectively and efficiently handle data throughout the data life cycle. Although typically shown as a circle (below) the actually life cycle of any data item may follow a different path, with branches and internal loops. Being aware of your data's future helps you plan how to best manage them.</p> <p></p> <p>Image from Strasser et al.</p> <p>The summary below is adapted from the excellent DataONE best practices primer.</p>"},{"location":"03_managing_data/#plan","title":"Plan","text":"<ul> <li>Describe the data that will be compiled, and how the data will be managed and made accessible throughout its lifetime</li> <li>A good plan considers each of the stages below</li> </ul> Warning <p>The biggest challenge to data management making it an afterthought.</p> <p>Unfortunately, poor data management doesn't have a high upfront cost. You can do substantial work before realizing you are in trouble. Like a swimmer in rip current, by the time you realize you are in trouble, you may already be close to drowning.</p> <p>The solution? Make data management the first thing you consider when starting a research project. It also needs to be a policy you institute right away for your research group.</p>"},{"location":"03_managing_data/#collect","title":"Collect","text":"<ul> <li>Have a plan for data organization in place before collecting data</li> <li>Collect and store observation metadata at the same time you collect the metadata</li> <li>Take advantage of machine generated metadata</li> </ul>"},{"location":"03_managing_data/#assure","title":"Assure","text":"<ul> <li>Record any conditions during collection that might affect the quality of the data</li> <li>Distinguish estimated values from measured values</li> <li>Double check any data entered by hand</li> <li>Perform statistical and graphical summaries (e.g., max/min, average, range) to check for questionable or impossible values.</li> <li>Mark data quality, outliers, missing values, etc.</li> </ul>"},{"location":"03_managing_data/#describe","title":"Describe","text":"<ul> <li> <p>Comprehensive data documentation (i.e. metadata) is the key to     future understanding of data. Without a thorough description of     the context of the data, the context in which they were collected,     the measurements that were made, and the quality of the data, it     is unlikely that the data can be easily discovered, understood, or     effectively used.</p> </li> <li> <p>Thoroughly describe the dataset (e.g., name of dataset, list of     files, date(s) created or modified, related datasets) including     the people and organizations involved in data collection (e.g.,     authors, affiliations, sponsor). Also include:</p> <ul> <li>An ORCID (obtain one if you don't have one).</li> <li>The scientific context (reason for collecting the data, how they were collected, equipment and software used to generate the data, conditions during data collection, spatial and temporal resolution)</li> <li>The data themselves</li> <li>How each measurement was produced</li> <li>Units</li> <li>Format</li> <li>Quality assurance activities</li> <li>Precision, accuracy, and uncertainty</li> </ul> </li> </ul> <p>Some metadata standards you may want to consider:</p> <ul> <li>DataCite for publishing data</li> <li>Dublin Core for sharing data on the web</li> <li>MIxS Minimum Information for any (x) sequence</li> <li>OGC standards for geospatial data</li> </ul> <p>Ontologies provide standardization for metadata values:</p> <ul> <li>Example: Environment Ontology terms for the MIxS standards</li> <li>Example: Plant Ontology for plant tissue types or development stages</li> <li>FAIRSharing.org lists standards and ontologies for life sciences.</li> </ul>"},{"location":"03_managing_data/#preserve","title":"Preserve","text":"<p>In general, data must be preserved in an appropriate long-term archive (i.e. data center). Here are some examples:</p> <ul> <li>Sequence data should go to a national repository, frequently NCBI</li> <li>Identify data with value - it may not be necessary to preserve all data from a project</li> <li>The CyVerse Data Commons provides a place to publish and preserve data that was generated on or can be used in CyVerse, where no other repository exists.</li> <li>See lists of repositories at FAIRSharing.org</li> <li>See lists of repositories at Data Dryad</li> <li>Github repos can get DOIs through Zenodo</li> <li>Be aware of licensing and other intellectual property issues<ul> <li>Repositories will require some kind of license, often the     least restrictive (see for example Creative Commons)</li> <li>Repositories are unlikely to enforce reuse restrictions, even     if you apply them.</li> </ul> </li> </ul>"},{"location":"03_managing_data/#discover","title":"Discover","text":"<ul> <li>Good metadata allows you to discover your own data!</li> <li>Databases, repositories, and search indices provide ways to     discover relevant data for reuse <ul> <li>Google dataset search</li> <li>DataOne</li> <li>FAIRSharing.org</li> </ul> </li> </ul>"},{"location":"03_managing_data/#integrate","title":"Integrate","text":"<ul> <li>Data integration is a lot of work</li> <li>Standards and ontologies are key to future data integration</li> <li>Know the data before you integrate them</li> <li>Don't trust that two columns with the same header are the same data</li> <li>Properly cite the data you reuse!</li> <li>Use DOIs (Digital Object Identifiers) wherever possible</li> </ul>"},{"location":"03_managing_data/#analyze","title":"Analyze","text":"<ul> <li>Follow open science principles for reproducible analyses (CyVerse,     RStudio, notebooks, IDEs)</li> <li>State your hypotheses and analysis workflow before collecting     data. Tools like Open Science Framework (OSF) allow you to make this public.</li> <li>Record all software, parameters, inputs, etc.</li> </ul>"},{"location":"03_managing_data/#references-and-resources","title":"References and Resources","text":"<p>DataOne best practices</p> <p>Center for Open Science</p> <p> </p>"},{"location":"03_managing_data/#fair-data","title":"FAIR Data","text":"<p>Learning Objectives</p> <ul> <li>Recall the meaning of FAIR</li> <li>Understand why FAIR is a collection of principles (rather than rules)</li> <li>Use self-assessments to evaluate the FAIRness of your data</li> </ul>"},{"location":"03_managing_data/#fair-principles","title":"FAIR Principles","text":"<p>In 2016, the FAIR Guiding Principles for scientific data management and stewardship were published in Scientific Data. Read it.</p> <p>Findable</p> <ul> <li>F1. (meta)data are assigned a globally unique and persistent identifier</li> <li>F2. data are described with rich metadata (defined by R1 below)</li> <li>F3. metadata clearly and explicitly include the identifier of the data it describes</li> <li>F4. (meta)data are registered or indexed in a searchable resource</li> </ul> <p>Accessible</p> <ul> <li>A1. (meta)data are retrievable by their identifier using a     standardized communications protocol</li> <li>A1.1 the protocol is open, free, and universally implementable</li> <li>A1.2 the protocol allows for an authentication and authorization     procedure, where necessary</li> <li>A2. metadata are accessible, even when the data are no longer     available</li> </ul> <p>Interoperable</p> <ul> <li>I1. (meta)data use a formal, accessible, shared, and broadly     applicable language for knowledge representation.</li> <li>I2. (meta)data use vocabularies that follow FAIR principles</li> <li>I3. (meta)data include qualified references to other (meta)data</li> </ul> <p>Reusable</p> <ul> <li>R1. meta(data) are richly described with a plurality of accurate     and relevant attributes</li> <li>R1.1. (meta)data are released with a clear and accessible data     usage license</li> <li>R1.2. (meta)data are associated with detailed provenance</li> <li>R1.3. (meta)data meet domain-relevant community standard</li> </ul> <p>Tip</p> <p>Open vs. Public vs. FAIR:</p> <p>FAIR does not demand that data be open: See one definition of open: http://opendefinition.org/</p> <p>Tip</p> <p>Why Principles?</p> <p>FAIR is a collection of principles. Ultimately, different communities within different scientific disciplines must work to interpret and implement these principles. Because technologies change quickly, focusing on the desired end result allows FAIR to be applied to a variety of situations now and in the foreseeable future.</p>"},{"location":"03_managing_data/#care-principles","title":"CARE Principles","text":"<p>The CARE Principles for Indigenous Data Governance were drafted at the International Data Week and Research Data Alliance Plenary co-hosted event \"Indigenous Data Sovereignty Principles for the Governance of Indigenous Data Workshop,\" 8 November 2018, Gaborone, Botswana.</p> <p>Collective Benefit</p> <ul> <li>C1. For inclusive development and innovation</li> <li>C2. For improved governance and citizen engagement</li> <li>C3. For equitable outcomes</li> </ul> <p>Authority to Control</p> <ul> <li>A1. Recognizing rights and interests</li> <li>A2. Data for governance</li> <li>A3. Governance of data</li> </ul> <p>Responsibility</p> <ul> <li>R1. For positive relationships</li> <li>R2. For expanding capability and capacity</li> <li>R3. For Indigenous languages and worldviews</li> </ul> <p>Ethics</p> <ul> <li>E1. For minimizing harm and maximizing benefit</li> <li>E2. For justice</li> <li>E3. For future use</li> </ul>"},{"location":"03_managing_data/#how-to-get-to-fair","title":"How to get to FAIR?","text":"<p>This is a question that only you can answer, that is because it depends on (among other things)</p> <ol> <li>Your scientific discipline: Your datatypes and existing standards     for what constitutes acceptable data management will vary.</li> <li>The extent to which your scientific community has implemented     FAIR: Some disciplines have significant guidelines on FAIR, while     others have not addressed the subject in any concerted way.</li> <li>Your level of technical skills: Some approaches to implementing     FAIR may require technical skills you may not yet feel comfortable     with.</li> </ol> <p>While a lot is up to you, the first step is to evaluate how FAIR you think your data are:</p> <p>Exercise</p> <p>Thinking about a dataset you work with, complete the ARDC FAIR assessment.</p>"},{"location":"03_managing_data/#references-and-resources_1","title":"References and Resources","text":"<p>https://www.nature.com/articles/sdata201618</p> <p> </p>"},{"location":"03_managing_data/#data-management-plans","title":"Data Management Plans","text":"<p>Learning Objectives</p> <ul> <li>Describe the purpose of a data management plan</li> <li>Describe the important elements of a data management plan</li> <li>Use a self-assessment to design a data management plan</li> </ul>"},{"location":"03_managing_data/#what-is-a-dmp","title":"What is a DMP?","text":"<p>\"A data management plan or DMP is a formal document that outlines how data are to be handled both during a research project, and after the project is completed. [1] The goal of a data management plan is to consider the many aspects of data management, metadata generation, data preservation, and analysis before the project begins; this may lead to data being well-managed in the present, and prepared for preservation in the future.\"(Source: https://en.wikipedia.org/wiki/Data_management_plan)</p> <p>Example DMPs</p>"},{"location":"03_managing_data/#todays-guest-speaker","title":"Today's Guest Speaker","text":"Dr. Wade Bishop, Professor in the School of Information Sciences at University of Tennessee, Knoxville  <p>Dr. Bishop's article on DMPs</p> <p>Why bother with a DMP?</p> <p>How would you answer?</p> <p>Do you have a data management plan? If so, how do you use it?</p> <p>\"Those who fail to plan, plan to fail\"</p> <p>Returning to the assertion that data (and its value) is at the foundation of your science, working without a data management plan should be considered scientific misconduct.</p> <p>Those are strong words. And while we might have an intuition of the boundaries of research ethics - data mismanagement seems more like an annoyance than misconduct. However, if your mismanagement leads to error in your research data, or the inability to make publicly-funded research open to the public, these are serious consequences. Increasingly, funders realize this.</p> <p>Stick:</p> <ul> <li>You have to make one</li> <li>Reviewers definitely look at them, but they may not be enforced.</li> </ul> <p>Carrot:</p> <ul> <li>Make your life easier</li> <li>Planning for you project makes it run more smoothly</li> <li>Avoid surprise costs</li> </ul>"},{"location":"03_managing_data/#elements-of-a-good-dmp","title":"Elements of a good DMP","text":"<ul> <li> <p>Information about data &amp; data format(s)</p> <ul> <li>data types</li> <li>data sources</li> <li>analysis methods</li> <li>formats</li> <li>QA/QC</li> <li>version control</li> <li>data life cycle</li> </ul> </li> <li> <p>Metadata content and format(s)</p> <ul> <li>format</li> <li>standards</li> </ul> </li> <li> <p>Policies for access, sharing, and re-use</p> <ul> <li>funder obligations</li> <li>ethical and privacy issues (data justice)</li> <li>intellectual property, copyright, citation</li> <li>timeline for releases</li> </ul> </li> <li> <p>Long-term storage, data management, and preservation</p> <ul> <li>which data to preserve</li> <li>which archive/repository</li> </ul> </li> <li> <p>Budget(PAPPG)</p> <ul> <li>each of the above elements cost time/money</li> <li>Personnel time for data preparation, management,     documentation, and preservation (including time)</li> <li>Hardware and/or software for data management, back up,     security, documentation, and preservation (including time)</li> <li>Publication/archiving costs (including time)</li> </ul> </li> </ul> <p>DMP Tools</p> <p>Make your life a little easier by creating DMPs with online tools </p> <p>Data Stewardship Wizard</p> <p>DMPTool</p> <p> </p>"},{"location":"03_managing_data/#licenses","title":"Licenses","text":"<p>By default, when you make creative work, that work is under exclusive copyright. This means that you have the right to decide how your work is used, and that others must ask your permission to use your work.</p> <p>If you want your work to be Open and used by others, you need to specify how others can use your work. This is done by licensing your work.</p> <p>MIT License</p> <p>GNU General Public License v3.0</p> <p>FOSS material has been licensed using the Creative Commons Attribution 4.0 International License.  </p> <p>Licensing your Github Repository</p> <ul> <li>Apache License 2.0</li> <li>GNU General Public License v3.0</li> <li>MIT License</li> <li>BSD 2-Clause \"Simplified\" License</li> <li>BSD 3-Clause \"New\" or \"Revised\" License</li> <li>Boost Software License 1.0</li> <li>Creative Commons Zero v1.0 Universal</li> <li>Eclipse Public License 2.0</li> <li>GNU Affero General Public License v3.0</li> <li>GNU General Public License v2.0</li> <li>GNU Lesser General Public License v2.1</li> <li>Mozilla Public License 2.0</li> <li>The Unlicense</li> </ul> <p> </p> <p>Open Source Licensing Resources</p> <p>https://choosealicense.com/</p> <p>https://opensource.guide/legal/</p>"},{"location":"03_managing_data/#references-and-resources_2","title":"References and Resources","text":"<ul> <li>NSF Guidelines on DMPs</li> <li>https://dmptool.org/general_guidance</li> <li>https://dmptool.org/public_templates</li> <li>Professional and scholarly societies, e.g., theEcological Society of America http://www.esa.org/esa/science/data-sharing/resources-and-tools/</li> <li>DataOne - https://dataoneorg.github.io/Education/bestpractices/</li> <li>Data Carpentry - http://datacarpentry.org/</li> <li>The US Geological Survey https://www.usgs.gov/data-management</li> <li>Repository registry (and search) service: http://www.re3data.org/</li> <li>Your university library</li> </ul>"},{"location":"03_managing_data/#self-assessment","title":"Self Assessment","text":"What is a Data Management Plan? <p>Important: A data management plan (DMP) is now required aspect of publicly funded research.</p> <p>DMPs are short, formal, documents outlining what types of data will be used, and what will be done with the data both during and after a research project concludes.</p> True or False: When science project funding ends, the data should end with it <p>False</p> <p>Data live on after a project ends.</p> <p>Ensuring that data have a full lifecycle where they can be (re)hosted and made available after a project ends is critical to open science and reproducible research</p> <p>Maybe</p> <p>Sometimes destroying data is part of the life cycle of data - this may be required if data are sensitive and could be used unethically in the future, beyond the control of the original investigator team. </p> True or False: FAIR and CARE data principles are the same <p>False</p> <p>The CARE principles were created in order to help guide and answer when and how applying FAIR data principles to soverign indigenous-controlled data should be done and when it should not. </p>"},{"location":"04_documentation_communication/","title":"Documentation &amp;  Communication","text":"<p>Learning Objectives</p> <p>After this lesson, you should be able to:</p> <ul> <li>Identify and explain different types of project documentation (both internal and external)</li> <li>Describe tools and approaches to creating your own documentation</li> <li>Describe best practices for maintaining documentation</li> <li>Create your own GitHub Pages website (!)</li> </ul>"},{"location":"04_documentation_communication/#project-documentation","title":"Project Documentation","text":"<p>Documentation is the practice of recording, preserving, and organizing information, data, or details in a structured and systematic manner. Documentation is also essential to communicate with your future self, your collaborators, or the world on specific ideas and information. Effective documentation must take into consideration the following points:</p> <ul> <li> Clarity: Documentation should be easy to understand with clear language and no ambiguity.</li> <li> Completeness: It must cover all essential details, leaving nothing crucial undocumented.</li> <li> Accuracy: Information should be up-to-date and correct to prevent errors and misunderstandings.</li> <li> Organization: A logical structure and clear organization make it easy to navigate and find information.</li> <li> Relevance: Documentation should focus on what's pertinent to its intended audience or purpose, avoiding unnecessary information.</li> </ul> <p>Not all documentation is the same. The documentation system, by Divio, categorizes the different types of documentation into 4 quadrants:</p> <p>  Read more in depth on the documentation system here: https://documentation.divio.com </p> Explanining the quadrants <ul> <li>Tutorials: Lessons! Tutorials are lessons that take the reader by the hand to understand how the basics of a tool work. They are what your project needs in order to show a beginner that they can achieve something with it. The techical teaching we do in FOSS are mostly tutorials. For example, we do simple tutorials to teach the mechanics of version control. </li> <li>How-to-guides: Recipes! How-to-guides take the reader through the steps required to acheive a specific outcome or answer a specific question. An example how-to-guide could be a guide on how to install a specific software on a specific operating system.</li> <li>References: References offer technical descriptions of the machinery and how to operate it. References have one job only: to describe. They are code-determined, because ultimately that\u2019s what they describe: key classes, functions, APIs, and so they should list things like functions, fields, attributes and methods, and set out how to use them.</li> <li>Explanation: Discussions! The aims of explanations are to clarify and illuminate a particular topic by broadening the documentation\u2019s coverage of a topic.</li> </ul>"},{"location":"04_documentation_communication/#public-repositories-for-documentation","title":"Public Repositories for Documentation","text":"<p> GitHub</p> <ul> <li>On Github, good documentation starts with a robust ReadMe file. The ReadMe file is the first thing that people see when they visit your repository. It is a good place to explain what your project does, how to use it, and how to contribute to it. Here is an example.</li> <li>Also on Github, you can use the Wiki feature to create a separate space for documentation. The Wiki is a place to document your project in a way that is separate from the code. Here is an example</li> </ul> <p> GitHub Pages</p> <ul> <li>You can pull templates from other GitHub users for your website,     e.g.  Jekyll themes</li> <li>GitHub pages are free, fast, and easy to build, but limited in use     of subdomain or URLs.</li> <li>The FOSS website is rendered using  GitHub Pages using  MkDocs and the Material theme for MkDocs.</li> <li>Other popular website generator for GitHub Pages is  Bootstrap.js.</li> </ul> <p> Material MkDocs</p> <ul> <li>Material Design theme for MkDocs, a static site generator geared towards (technical) project documentation.</li> <li>publish via GitHub Actions</li> <li>Uses open source Material or ReadTheDocs Themes</li> </ul> <p> ReadTheDocs</p> <ul> <li>publishing websites via     ReadTheDocs.com costs money.</li> <li>You can work in an offline state, where you develop the materials     and publish them to your localhost using     Sphinx</li> <li>You can work on a website template in a GitHub repository, and     pushes are updated in near real time using ReadTheDocs.com.</li> <li>Here is example documentation of Pytorch using ReadTheDocs: PyTorch.</li> </ul> <p> Bookdown</p> <ul> <li> Bookdown is an open-source R package that facilitates writing books and long-form articles/reports with R Markdown.</li> <li>Bookdown websites can be hosted by RStudio     Connect</li> <li>You can publish a Bookdown website using Github     Pages</li> </ul> <p> Quarto</p> <ul> <li> Quarto is an open-source scientific and technical publishing system built on Pandoc</li> <li>Build a website using Quarto's template builder</li> <li>Build with Github Pages</li> </ul> <p> JupyterBook</p> <ul> <li>Based on Project Jupyter <code>ipynb</code> and MarkDown</li> <li>Uses <code>conda</code> package management</li> </ul> <p> GitBook</p> <ul> <li>GitBook websites use MarkDown syntax</li> <li>Free for open source projects, paid plans are available</li> </ul> <p> Confluence Wikis</p> <ul> <li> Confluence Wikis are another tool for documenting your work. You can see an example from Cyverse.</li> </ul> <p>Things to remember about Documentation</p> <ul> <li> <p>Documentation should be written in such a way that people who did not write the documentation can read and then use or read and then teach others in the applications of the material.</p> </li> <li> <p>Documentation is best treated as a living document, but version control is necessary to maintain it</p> </li> <li> <p>Technology changes over time, expect to refresh documentation every 3-5 years as your projects age and progress.</p> </li> </ul>"},{"location":"04_documentation_communication/#websites-to-host-methods-protocols","title":"Websites to Host Methods &amp; Protocols","text":"<p>Open Science Framework for free. OSF can be directly linked to your ORCID.</p> <ul> <li>Integrated project management tools</li> <li>Uses templates to create a project website</li> <li>Can publish preprints from within project management tools</li> </ul> <p>Protocols.io - collaborative platform and preprint server for: science methods, computational workflows, clinical trials, operational procedures, safety checklists, and instructions / manuals.</p> <p>QUBES - community of math and biology educators who share resources and methods for preparing students to tackle real, complex, biological problems.</p> What are the benefits of using a GitHub.io website? <p>Github Pages are hosted directly from your GitHub repository. </p> <p>Just edit, push, and your changes are live.</p> <p>You do not need to run your own web server!!</p>"},{"location":"04_documentation_communication/#communication","title":"Communication","text":""},{"location":"04_documentation_communication/#internal-project","title":"Internal Project","text":"<p>Choosing which software to use for your internal lab communication can be complicated by the cost of setting up, the cost of maintaining, and simply by the sheer number of platforms that are out there.</p> <p>For this workshop, we use  SLACK (Searchable Log of All Conversation &amp; Knowledge). Microsoft's competitor to SLACK is  Microsoft Teams.</p> <p>Remember, the intention of these platforms are to improve productivity &amp; not become a distraction.</p> <p> SLACK</p> <ul> <li>Slack has plenty of apps for coordinating     multiple services, i.e. Calendars, Github, GoogleDrive, Box, etc.</li> <li>Free Slack is limiting (e.g., 90 day history; limited connections across workspaces).</li> <li>Paid Slack is $7.25 per user per month. (10 users for 1 year = $870)</li> </ul> <p> Microsoft Teams</p> <ul> <li>Teams is used by many R1 research universities as part of their     campus wide license agreement for Office 365 Business and Education</li> <li>For example, anyone with a <code>arizona.edu</code> email address can use Teams for free</li> <li>Limitations:<ul> <li>Not sure you can create your own Teams</li> <li>Limited to messaging with people in your university Team</li> </ul> </li> </ul> <p>Other popular alternatives</p> <ul> <li> BaseCamp</li> <li> Discord</li> <li> Mastodon</li> <li> Mattermost</li> </ul> <p>Useful links for creating a SLACK workspace</p> <ol> <li>Create a new Workspace</li> <li>Create channels, add apps &amp; tools</li> </ol>"},{"location":"04_documentation_communication/#external-public","title":"External (Public)","text":"<p>Communicating with the public and other members of your science community (in addition to traditional peer-review publications and conferences) is one of the most important parts of your science!</p> <p>There are many ways scientists use social media and the web to share their data science ideas:</p> <ol> <li> \"Science Twitter\" (now X) -     is really just regular Twitter, but with a focus on following other scientists and organizations, and tweeting about research you're interested in. By building up a significant following, more people will know you, know about your work, and you'll have a higher likelihood of meeting other new collaborators.</li> <li>Blogging Platforms such as Medium are a great place to self publish your writing on just about any topic. It's free to sign up and start blogging, but does have a fee for accessing premium content. Some of my favorite blogs include Toward Data Science and Chris Holmes.</li> <li>Community groups - There are lists (and lists of lists) of nationals research organizations, in which a researcher can become involved. These older organziations     still rely on official websites, science journal blogs, and email lists to communicate with their members. In the earth sciences there are open groups which focus on communication like the Earth Science Information Partners (ESIP) with progressive ideas about how data and science can be done. Other groups, like The Carpentries and Research Bazaar are focused on data science training and digital literacy.</li> <li>Podcasts - Creating and distributing audio content to masses is easier than ever before. There are many podcast hosting platforms including Spotify, Podbean, Acast, and Libsyn. From there is it simple to make your podcast availble in the Google Podcast app or Apple Podcast app. </li> <li>Webinars - With platforms such as Zoom, Microsoft Teams, and Google Meet, it is so easy nowadays to host a webinar touting and explaining your science. </li> <li>Youtube - The king of video sharing platforms is a great place to post content promoting your science (and yourself!). For example, Cyverse posts lots of content on cyberinfrastructure and data processing pipelines. Some of my favorite podcasts hosted on Youtube include StarTalk and Lex Fridman.</li> </ol> <p>Important</p> <p>Remember: Personal and Professional Accounts are Not Isolated</p> <p>You decide what you post on the internet. Your scientist identity may be a part of your personal identity on social media, it might be separate. A future employer or current employer can see your old posts. What you post in your personal accounts can be considered a reflection of the organization you work for and may be used in decisions about hiring or dismissal.</p>"},{"location":"04_documentation_communication/#hands-on-building-a-github-pages-website-using-mkdocs","title":"Hands-on: Building a GitHub Pages Website using MkDocs","text":"<p>This section is built in order to educate on and simplify the steps necessary that newcomers need to take in order to build a successful GitHub Pages hosted website. </p> <p>This tutorial is inspired by academicpages, a Jekyll themed template created in order to help scientists and academics build their own websites.</p> <p>The easy way would be to fork/import the foss-reference-hub website (repository) and modify it to reflect your requirements; this tutorial will cover the necessary files and repository structure you require in order to build a successful personal website.</p> <p>Repository Explanation</p> <p>A GitHub hosted website running the MkDocs-material theme requires the following files in order to function:</p> <ul> <li>A <code>docs</code> folder:<ul> <li>A folder that contains all the documents necessary to populate the website's pages.</li> <li>All of the documents that the user needs to change are in here.</li> </ul> </li> <li>A <code>mkdocs.yml</code> file:<ul> <li>A <code>yml</code> file which contains critical information on the website structure, including themes, fonts, and extensions.</li> </ul> </li> <li>A <code>requirements.txt</code> file:<ul> <li>A file with a list of software necessary to build the website, primilily used by GitHub Actions.</li> </ul> </li> <li>A <code>.github/workflow</code> folder:<ul> <li>Contains the <code>ghpages.yml</code> file that controls the GitHub Action.</li> </ul> </li> </ul> <p>The structure of the basic repository is the following:</p> <pre><code>.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 mkdocs.yml              &lt;- Governing file for website building\n\u251c\u2500\u2500 requirements.txt        &lt;- Requirements file for pip installation (required by website)      \n\u251c\u2500\u2500 docs                           \n\u2502   \u251c\u2500\u2500 assets              &lt;- Folder for images and additional graphic assets\n\u2502   \u2514\u2500\u2500 index.md            &lt;- Main website home page\n\u2514\u2500\u2500 .github\n    \u2514\u2500\u2500 workflows\n        \u2514\u2500\u2500 ghpages.yml     &lt;- GitHub Actions controlling file\n</code></pre> <p>Upon pushing changes, a <code>gh-pages</code> branch will be automatically created by the GitHub Action; it is where the website is rendered from.</p>"},{"location":"04_documentation_communication/#directions-a-forking-or-importing-an-existing-repo","title":"Directions A: forking or importing an existing repo","text":"<p>Prerequisites</p> <p>You will require the following in case you want to add code locally.</p> Create a GitHub account <p>Navigate to the GitHub website and click Sign Up, and follow the on screen instructions.</p> <p>Additionally, you can choose between Generating a Personal Access Token or using SSH keys. This is useful if you want to work locally and push your changes to GitHub. We are going to cover this further in next week's lesson on Version Control.</p> Choice A: Generate a Personal Access Token <p>You can follow the official documentation on how to generate Tokens here. We  discussed how to generate tokens in Week 0. Here's are quick steps you can follow in order to setup your account on your machine using tokens:</p> <ol> <li>On your coumputer:<ol> <li>Clone your repository (<code>git clone &lt;repository&gt;</code>)</li> <li>Make changes where necessary, and add (<code>git add &lt;changed files&gt;</code>), commit (<code>git commit -m \"&lt;message on changes&gt;\"</code>) and push your changes (<code>git push origin</code>).</li> <li>You should be prompted to logging in your GitHub account. Put your email but not your password. Instead, open your web browser and follow the steps below:</li> </ol> </li> <li>On GitHub:<ol> <li>Navigate to your GitHub Settings (You can access your account Settings from the drop down menu where your account icon is, on the top right of the screen)</li> <li>Scroll to the bottom of the left hand side menu to find Developer settings and open it.</li> <li>Click Personal access tokens &gt; Tokens (classic)</li> <li>Click Generate new token &gt; Generate new token (classic). You might need to input your Authentification code if you have enabled 2FA.</li> <li>Give it a name, and all the scopes you require (tip: select all scopes and No Expiration), then click Generate Token. Copy the new generated Token</li> </ol> </li> <li>Back on your computer:<ol> <li>If you have been following the steps above, you should still be in your shell with GitHub still asking for your password.</li> <li>Paste your Token here, and you should be logging in. Your changes should then be saved to GitHub.</li> </ol> </li> </ol> Choice B: Connecting via SSH <p>The process of connecting your computer to GitHub using an SSH key is more expedited (and probably less confusing). </p> <p>As a setup step, see if your computer is already connected to GitHub by doing <code>ssh -T git@github.com</code>. If the response message is <code>git@github.com: Permission denied (publickey).</code> it signifies that your computer is not yet linked with GitHub. To link your computer to github to the following:</p> <ol> <li>Generate an SSH key with a level of encryption that you prefer: <code>ssh-keygen -t ed25519 -C &lt;your github email&gt;</code>. This command generates an SSH key with ed25519 encryption (harder to crack!) and adds your email as \"comment\" (<code>-C</code>, will help recongizing the user adding the key). A number of additional questions are going to ask you where you'd like to save the key and whether you'd like to add a password for protection; unless you want to save it elsewhere, feel free to use the default options. Upon completion you should see something like this: <pre><code>Your identification has been saved in /c/Users/&lt;user&gt;/.ssh/id_ed25519\nYour public key has been saved in /c/Users/&lt;user&gt;/.ssh/id_ed25519.pub\nThe key fingerprint is:\nSHA256:SMSPIStNyA00KPxuYu94KpZgRAYjgt9g4BA4kFy3g1o &lt;your github email&gt;\nThe key's randomart image is:\n+--[ED25519 256]--+\n|^B== o.          |\n|%*=.*.+          |\n|+=.E =.+         |\n| .=.+.o..        |\n|....  . S        |\n|.+ o             |\n|+ =              |\n|.o.o             |\n|oo+.             |\n+----[SHA256]-----+\n</code></pre></li> <li>Upon generating the ssh key, copy it. You can reveal it by doing <code>cat ~/.ssh/id_ed25519.pub</code>.</li> <li>In GitHub, go to your settings: click your account icon on top right, and from the drop down menu, select Settings and then SSH and GPG keys. Here, click on New SSH Key, where you can then paste the newly geneated key. Add a name reflecting your machine and save changes. </li> </ol> <p>Optional: if you want to check if you successfully linked your computer to GitHub, do <code>ssh -t git@github.com</code>. You should receive the following message: `Hi ! You've successfully authenticated, but GitHub does not provide shell access. <ol> <li>Fork or import the FOSS Reference Hub website tutorial repository branch<ul> <li>Forking or importing will allow you to have your own copy of a specific repository; Cloning a repository without forking/importing it first, will lead to the changes being applied to the original repository and not your own copy. You should clone your forked or imported repository, not the original!</li> </ul> </li> <li>Navigate to Settings &gt; Actions &gt; General:<ul> <li>Under Action Permissions select Allow all actions and reusalbe workflows</li> <li>Under Workflow permissions select Read and write permissions and Allow GitHub Actions to create and approve pull requests</li> </ul> </li> <li>Edit the <code>mkdocs.yml</code> and push your changes<ul> <li>The first changes you should be making are in the first few lines in the <code>mkdocs.yml</code> file in order to reflect your necessities:<ul> <li>Line 1: <code>site_name:</code> change to any title you want for your website </li> <li>Line 2: <code>site_description:</code> give a short description of the website</li> <li>Line 3: <code>site_author:</code> who you are</li> <li>Line 4: <code>site_url:</code> change it to the URL reflected in Settings, which will most likely be <code>https://&lt;github-username.github.io&gt;/</code></li> <li>Line 7: <code>repo_name:</code> give the name of your repository (e.g., <code>academicpages-mkdocs</code> in this case)</li> <li>Line 8: <code>repo_url:</code> give the git repository URL </li> <li>Line 11: <code>copyright:</code> change <code>your name</code> to the maintainer of the website (likely to be you)</li> </ul> </li> </ul> <p>Workflow expectations</p> <p>The previos changes should trigger the GitHub action workflow, which is setup to apply changes to the website every time a commit is pushed. One of the first thing that <code>mkdocs-material</code> will do, is to create the <code>gh-pages</code> branch (in case you do not have it already). The workflow will fail because the <code>ghpages.yml</code> in the <code>.github/workflows</code> directory is disabled (\"commented out\"). To enable it, remove the <code>#</code> at the beginnig on each line and commit your changes. Upon changes, the workflow should go ahead and create the <code>gh-pages</code> branch.</p> </li> <li>Navigate to Settings &gt; Pages and make sure that Source is Deploy from a branch and Branch is gh-pages, /(root)<ul> <li>You should be able to access your website at <code>https://&lt;github-username&gt;.github.io/</code>. If you cannot find your website, go to the repository's settings page and navigate to Pages: your website address will be there.</li> </ul> </li> <li>Edit documents as necessary.<ul> <li>Don't forget to add, commit and push changes!</li> <li>Changes will only be visible on the website after a successful push.</li> <li>After each push, next to the commit identifier GitHub will show either a yellow circle (, meaning building), green check (, meaning success), or red cross (, meaning failure).</li> </ul> Failure? Try again! <p>If you've been given the red cross , GitHub will notify you with what went wrong. By clicking on the , GitHub will open up a new page showing you the broken process.</p> </li> </ol>"},{"location":"04_documentation_communication/#directions-b-creating-your-own","title":"Directions B: Creating your own","text":"<p>Prerequisites</p> <p>You will require the following in case you want to add code locally.</p> Create a GitHub account <p>Navigate to the GitHub website and click Sign Up, and follow the on screen instructions.</p> <p>Additionally, you can choose between Generating a Personal Access Token or using SSH keys. This is useful if you want to work locally and push your changes to GitHub. We are going to cover this further in next week's lesson on Version Control.</p> Choice A: Generate a Personal Access Token <p>You can follow the official documentation on how to generate Tokens here. We  discussed how to generate tokens in Week 0. Here's are quick steps you can follow in order to setup your account on your machine using tokens:</p> <ol> <li>On your coumputer:<ol> <li>Clone your repository (<code>git clone &lt;repository&gt;</code>)</li> <li>Make changes where necessary, and add (<code>git add &lt;changed files&gt;</code>), commit (<code>git commit -m \"&lt;message on changes&gt;\"</code>) and push your changes (<code>git push origin</code>).</li> <li>You should be prompted to logging in your GitHub account. Put your email but not your password. Instead, open your web browser and follow the steps below:</li> </ol> </li> <li>On GitHub:<ol> <li>Navigate to your GitHub Settings (You can access your account Settings from the drop down menu where your account icon is, on the top right of the screen)</li> <li>Scroll to the bottom of the left hand side menu to find Developer settings and open it.</li> <li>Click Personal access tokens &gt; Tokens (classic)</li> <li>Click Generate new token &gt; Generate new token (classic). You might need to input your Authentification code if you have enabled 2FA.</li> <li>Give it a name, and all the scopes you require (tip: select all scopes and No Expiration), then click Generate Token. Copy the new generated Token</li> </ol> </li> <li>Back on your computer:<ol> <li>If you have been following the steps above, you should still be in your shell with GitHub still asking for your password.</li> <li>Paste your Token here, and you should be logging in. Your changes should then be saved to GitHub.</li> </ol> </li> </ol> Choice B: Connecting via SSH <p>The process of connecting your computer to GitHub using an SSH key is more expedited (and probably less confusing). </p> <p>As a setup step, see if your computer is already connected to GitHub by doing <code>ssh -T git@github.com</code>. If the response message is <code>git@github.com: Permission denied (publickey).</code> it signifies that your computer is not yet linked with GitHub. To link your computer to github to the following:</p> <ol> <li>Generate an SSH key with a level of encryption that you prefer: <code>ssh-keygen -t ed25519 -C &lt;your github email&gt;</code>. This command generates an SSH key with ed25519 encryption (harder to crack!) and adds your email as \"comment\" (<code>-C</code>, will help recongizing the user adding the key). A number of additional questions are going to ask you where you'd like to save the key and whether you'd like to add a password for protection; unless you want to save it elsewhere, feel free to use the default options. Upon completion you should see something like this: <pre><code>Your identification has been saved in /c/Users/&lt;user&gt;/.ssh/id_ed25519\nYour public key has been saved in /c/Users/&lt;user&gt;/.ssh/id_ed25519.pub\nThe key fingerprint is:\nSHA256:SMSPIStNyA00KPxuYu94KpZgRAYjgt9g4BA4kFy3g1o &lt;your github email&gt;\nThe key's randomart image is:\n+--[ED25519 256]--+\n|^B== o.          |\n|%*=.*.+          |\n|+=.E =.+         |\n| .=.+.o..        |\n|....  . S        |\n|.+ o             |\n|+ =              |\n|.o.o             |\n|oo+.             |\n+----[SHA256]-----+\n</code></pre></li> <li>Upon generating the ssh key, copy it. You can reveal it by doing <code>cat ~/.ssh/id_ed25519.pub</code>.</li> <li>In GitHub, go to your settings: click your account icon on top right, and from the drop down menu, select Settings and then SSH and GPG keys. Here, click on New SSH Key, where you can then paste the newly geneated key. Add a name reflecting your machine and save changes. </li> </ol> <p>Optional: if you want to check if you successfully linked your computer to GitHub, do <code>ssh -t git@github.com</code>. You should receive the following message: `Hi ! You've successfully authenticated, but GitHub does not provide shell access. <ol> <li>Create your own repository<ul> <li>Add a README and a license and keep the repository public</li> </ul> </li> <li>Create a <code>docs</code> folder<ul> <li>Within the folder, create an <code>index.md</code> file</li> </ul> </li> <li>Navigate to Settings &gt; Actions &gt; General:<ul> <li>Under Action Permissions select Allow all actions and reusalbe workflows</li> <li>Under Workflow permissions select Read and write permissions and Allow GitHub Actions to create and approve pull requests</li> </ul> </li> <li> <p>Create an <code>requirements.txt</code> file and populate it with the following requirement list:</p> Expand for code! <pre><code>bump2version\ncoverage\nflake8\ngrip\nipykernel\nlivereload\nnbconvert&gt;=7\npip\nsphinx\ntox\ntwine\nwatchdog\nwheel\nmkdocs-git-revision-date-plugin \nmkdocs-jupyter \nmkdocs-material \nmkdocs-pdf-export-plugin\nmkdocstrings \nmkdocstrings-crystal\nmkdocstrings-python-legacy\n#pygments&gt;=2.10,&lt;2.12\n#pymdown-extensions&lt;9.4\n\n# Requirements for core\njinja2&gt;=3.0.2\nmarkdown&gt;=3.2\nmkdocs&gt;=1.4.0\nmkdocs-material-extensions&gt;=1.0.3\npygments&gt;=2.12\npymdown-extensions&gt;=9.4\n\n# Requirements for plugins\nrequests&gt;=2.26\n</code></pre> </li> <li> <p>Create an <code>mkdocs.yml</code> file and  populate it with the following:</p> Expand for code! <pre><code>site_name: Name of your website\nsite_description: Tell people what this website is about\nsite_author: Who you are\nsite_url: The website URL\n\n# Repository\nrepo_name: The repository name\nrepo_url: The repository URL\nedit_uri: edit/main/docs/\n# Copyright\ncopyright: 'Copyright &amp;copy; 2023 - 2024'\n\n\n# Configuration\ntheme:\n    name: material\nhighlightjs: true\nfont:\n    text: Roboto\n    code: Regular\npalette:\n    scheme: default\n\n# Features  \nfeatures:\n- navigation.instant\n- navigation.tracking\n- navigation.tabs\n- navigation.tabs.sticky\n- navigation.indexes\n- navigation.top\n- toc.follow\n\n# 404 page\nstatic_templates:\n    - 404.html\n\n# Search feature\ninclude_search_page: false\nsearch_index_only: true\n\n# Palette and theme (uses personalized colours)\nlanguage: en\npalette:\n    primary: custom\n    accent: custom\nicon:\n    logo: material/cogs\n    favicon: material/cogs\n\n# Page tree\nnav:\n- Home: index.md\n\n# Extra Plugins\nplugins:\n    - search\n    - mkdocstrings\n    - git-revision-date\n    - mkdocs-jupyter:\n        include_source: True\n        ignore_h1_titles: True\n\n# Extensions (leave as is)\nmarkdown_extensions:\n- admonition\n- abbr\n- attr_list\n- def_list\n- footnotes\n- meta\n- md_in_html\n- toc:\n    permalink: true\n    title: On this page\n- pymdownx.arithmatex:\n    generic: true\n- pymdownx.betterem:\n    smart_enable: all\n- pymdownx.caret\n- pymdownx.critic\n- pymdownx.details\n- pymdownx.emoji:\n    emoji_index: !!python/name:materialx.emoji.twemoji\n    emoji_generator: !!python/name:materialx.emoji.to_svg\n- pymdownx.highlight\n- pymdownx.inlinehilite\n- pymdownx.keys\n- pymdownx.magiclink:\n    repo_url_shorthand: true\n    user: squidfunk\n    repo: mkdocs-material\n- pymdownx.mark\n- pymdownx.smartsymbols\n- pymdownx.superfences:\n    custom_fences:\n        - name: mermaid\n        class: mermaid\n        format: !!python/name:pymdownx.superfences.fence_code_format\n- pymdownx.tabbed\n- pymdownx.tasklist:\n    custom_checkbox: true\n- pymdownx.tilde\n</code></pre> </li> <li> <p>Create a <code>.github/workflows</code> folder and add a <code>ghpages.yml</code> with the following:</p> Expand for code! <pre><code>name: Publish docs via GitHub\non:\npush:\n    branches:\n    - main\n\njobs:\nbuild:\n    name: Deploy docs\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - uses: actions/setup-python@v4\n        with:\n            python-version: 3.9\n    - name: run requirements file\n        run:  pip install -r requirements.txt \n    - name: Deploy docs\n        run: mkdocs gh-deploy --force\n        env:\n            GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n</code></pre> </li> <li> <p>Navigate to Settings &gt; Pages and make sure that Source is Deploy from a branch and Branch is gh-pages, /(root)</p> <ul> <li>You should be able to access your website at <code>https://&lt;github-username&gt;.github.io/</code>. If you cannot find your website, go to the repository's settings page and navigate to Pages: your website address will be there.</li> </ul> </li> <li>Edit documents as necessary.<ul> <li>Don't forget to add, commit and push changes!</li> <li>Changes will only be visible on the website after a successful push.</li> <li>After each push, next to the commit identifier GitHub will show either a yellow circle (, meaning building), green check (, meaning success), or red cross (, meaning failure).</li> </ul> </li> </ol>"},{"location":"04_documentation_communication/#further-documentation","title":"Further Documentation","text":"<p>Here are some guides that you may find useful:</p> <ul> <li>MarkDown cheatsheet: for correct MarkDown synthax.</li> <li>MkDocs-material: a starting guide to MkDocs Material theme (massive list of supported emojis here).</li> <li>MkDocs-material References: more sophisticated documentation for MkDocs Material. </li> <li>YouTube link to FOSS 2022: Michael explains (~1h mark) his Jekyll-based website and gives a tutorial on how to use academicpages.</li> </ul>"},{"location":"04_documentation_communication/#self-paced-material","title":"Self-Paced Material","text":"<ul> <li>15 Data Science Communities to Join</li> <li>Python &amp; Slack</li> <li>Slack CLI notifications</li> <li>Meetups</li> </ul>"},{"location":"04_documentation_communication/#github-pages-website-quickstarts","title":"GitHub Pages Website Quickstarts","text":"<ul> <li> <p> GitHub Pages</p> <ol> <li>Create a GitHub account</li> <li>Clone the repo <code>https://github.com/username/username.github.io</code></li> <li>Create an <code>index.html</code></li> <li>Push it back to GitHub</li> </ol> </li> <li> <p> ReadTheDocs.org</p> <ol> <li>Install</li> <li>Use Github</li> <li>Create a ReadTheDocs account</li> </ol> </li> <li> <p> Material MkDocs</p> <ol> <li>Install Material <ol> <li>use a <code>reqirements.txt</code> </li> <li>or <code>pip install mkdocs-material</code></li> </ol> </li> <li>Clone a repository with an existing template or create a new repo with <code>mkdocs new .</code> </li> <li>Run <code>python -m mkdocs serve</code> to build and serve locally</li> <li>Open your browser to preview the build at https://localhost:8000`</li> </ol> </li> <li> <p> Bookdown</p> <ol> <li>Install R and RStudio</li> <li>Install Bookdown package with <code>install.packages(\"bookdown\", dependencies=TRUE)</code></li> <li>Open the Bookdown demo and get started</li> </ol> </li> <li> <p> Quarto</p> <ul> <li>Follow these instructions</li> </ul> </li> <li> <p> JupyterBook</p> <ul> <li>Create your first book</li> </ul> </li> <li> <p> GitBook</p> <ul> <li>Follow Template builder</li> </ul> </li> </ul>"},{"location":"04_documentation_communication/#self-assessment","title":"Self Assessment","text":"True or False: Tutorials and How-to-Guides are the same <p>False</p> <p>Tutorials are in general introductory and longer than How-to-Guides and are intended for teaching learners a new concept by describing applications and providing justifications. </p> <p>How-to-Guides are more like cooking recipes which include step-by-step instructions for a specific task.</p> True or False: Teams should communicate over a single messaging platform. <p>False</p> <p>While it may be advisable to push informal communication toward a platform like SLACK or Microsoft Teams, there is no one-platform-fits-all solution for managing a diverse science team.</p> What is the best communication platform for team science? <p>There is no best platform, but there are some best practices</p> <p>In general, communications amongst team members may be best suited for messaging services like SLACK, Teams, or Chat.</p> <p>For software development, GitHub Issues are one of the primary means of documenting changes and interactions on the web.</p> <p>Formal communication over email is preferred, and is necessary for legal, budgetary, and institutional interactions.</p>"},{"location":"05_version_control/","title":"Version Control","text":"<p>Learning Objectives</p> <p>After this lesson, you should be able to:</p> <ul> <li>Understand the basics of <code>git</code> as a resource for reproducible programming</li> <li>Describe tools and approaches to creating your <code>git</code> Repositories</li> <li>Describe best practices for maintaining GitHub Organizations and Repositories</li> <li>Maintain own GitHub user profile and repositories</li> </ul> <p>Version control refers to keeping track of the version of a file, set of files, or a whole project.</p> <p>Some version control tools:</p> <ul> <li> Microsoft Office's Track Changes functionality</li> <li> Apple's Time Machine</li> <li> Google Docs' Version History</li> <li> Git</li> </ul> <p>Version control is as much a philosophy as a set of tools; you don't need to master Git to utilize version control (though it is certainly a worthwhile tool for many researchers).</p> <p>  We have all been here, taken by the Software Carpentry Version Control lesson. </p>"},{"location":"05_version_control/#git-vs-github","title":"Git vs.  GitHub","text":"<p>Git is a command-line program for version control of repositories. It keeps track of changes you make to files in your repository and stores those changes in a .git folder in that repository. These changes happen whenever you make a commit. Git stores the history of these commits in a \"tree\", so you can go back to any previous commit. By keeping track of the differences between commits, Git can be much more efficient than storing an entire copy of each version in a document's history.</p> <p>You could utilize Git completely on its own, on your local computer, and get a lot of benefits. You will have a history of the changes you made to a project, allowing you to go back to any old version of your work. However, where Git really shines is in collaborative work. In order to effectively collaborate with others on a project, you need two basic features: a way to allow people to work in parallel, and a way to host repositories somewhere where everyone can access them. The first feature is branching, which is part of Git, and the hosting part can be taken care of by platforms like GitHub, GitLab, or Bitbucket. We will focus on GitHub.</p> <p>GitHub is a site that can remotely host your Git repositories. By putting your repository onto GitHub, you get a backup of the repository, a way to collaborate with others, and a lot of other features.</p> <p>  Git vs GitHub, simplified </p>"},{"location":"05_version_control/#definitions","title":"Definitions","text":"<p>Git-related Definitions</p> <p>Platforms:</p> <ul> <li>Git: tool for version control.</li> <li>GitHub: hosted server that is also interactive.</li> </ul> <p>Locations and directions:</p> <ul> <li>repo: short for repository</li> <li>local: on your personal computer.</li> <li>remote: somewhere other than your computer. GitHub can host remote repositories.</li> <li>upstream: primary or main branch of original repository.</li> <li>downstream: branch or fork of repository.</li> </ul> <p>Actions:</p> <ul> <li>clone: copy of a repository that lives locally on your computer. Pushing changes will affect the repository online.</li> <li>pull: getting latest changes to the repository on your local computer.<ul> <li>the fetch command does the same, however one needs to also merge the changes, whilst with pull, the merge action is automatic.</li> </ul> </li> <li>branch: a history of changes to a repository. You can have parallel branches with separate histories, allowing you to keep a \"main\" version and development versions.</li> <li>fork: copy of someone else's repository stored locally on your account. From forks, you can make pull requests to the main branch.</li> <li>commit: finalize a change.</li> <li>push: add changes back to the remote repository.</li> <li>merge: takes changes from a branch or fork and applies them to the main.</li> </ul> <p>These are also commands when paired with <code>git</code>!</p> <p>Using the following synthax <code>git &lt;command&gt;</code> one can trigger an action. An example is <code>git pull</code>, which will pull all of the latest changes in the remote repository.</p> <p>Funtional: </p> <ul> <li> <p>pull request: proposed changes to/within a repository.</p> </li> <li> <p>issue: suggestions or tasks needed for the repository. Allows you to track decisions, bugs with the repository, etc.</p> </li> </ul> <p>  Visualizing the commands through a workflow example  (graphic's correction: marged merged) </p>"},{"location":"05_version_control/#practical-git-techniques","title":"Practical Git Techniques","text":"<p>  The version control path sofware takes before release </p> <p>The basic Git life cycle</p> <p>When using Git for your version control, the usual life cycle is the following:</p> Action Explanation 1. <code>git clone &lt;repository&gt;</code> Clones the target repository to your machine 2. <code>git status</code> Checks whether there are changes in the remote, original repository 3. <code>git pull</code> Pulls any change to your local repository 4. <code>git add &lt;changes&gt;</code> Adds to a future commit any change 5. <code>git commit -m \"&lt;message&gt;\"</code> Creates the commit and adds a descriptive message 6. <code>git push</code> Pushes the changes commited from local to the remote repository <p>If there are no branches or external pull requests, the basic Git life cycle is summarizable like this:</p> <pre><code>graph LR\nA[1. git clone] --&gt; B[2. git status] --&gt;C([differences from origin?]):::colorclass;\nC--&gt;|yes| D[3. git pull]--&gt; E;\nC--&gt;|no| E[4. git add];\nE--&gt;F[5. git commit] --&gt;G[6. git push];\nG--&gt;B;\nclassDef colorclass fill:#f96</code></pre> <p>After learning the basics of using Git, which you can learn with the Software Carpentry Git Lesson, there are some next things that can be useful to learn. Here are a couple topics that are worth digging into more:</p> <ul> <li> <p> Using the Git log</p> <ul> <li>You can access using git log</li> <li>Will show you your commit history</li> <li>Useful for figuring out where you need to roll back to</li> </ul> </li> <li> <p> Reverting</p> <ul> <li>There are a lot of different ways to \"undo\" something in Git</li> <li>Some are safer, some are a bit riskier</li> <li>Depends on what stage of the commit process you're in</li> <li>Here are some useful resources:<ul> <li>10 Common Git Problems and How to Fix Them</li> <li>\"So you have a mess on your hands...\"</li> <li>How to undo almost anything</li> </ul> </li> </ul> </li> <li> <p> Branching</p> <ul> <li>This is important to learn if you're going to be doing any sort of collaboration</li> <li>Here is a fantastic resource for learning how git branching really works: https://learngitbranching.js.org/</li> <li>you will probably have to deal with merge conflicts at some point<ul> <li>Merge conflicts happen when two branches are being merged, but they have different changes to the same part of a file</li> <li>Perhaps you are working on a feature branch, and you change line 61 in file.R, but someone else made a change to the main branch at line 61 in file.R. When you try to merge the feature and main branches, Git won't know which changes to line 61 in file.R are correct, and you will need to manually decide.</li> <li>Here are some good resources:<ul> <li>Resolving merge conflicsresolving-a-merge-conflict-using-the-command-line</li> <li>git - ours &amp; theirs, a CLI resource to help with conflicts</li> </ul> </li> </ul> </li> </ul> </li> <li> <p> .gitignore</p> <ul> <li>You often want Git to completely ignore certain files</li> <li>Generated files (like HTML files from Markdown docs)</li> <li>IDE-specific files like in .RStudio or .vscode folders</li> <li>really big files, like data or images<ul> <li>If you accidentally commit a really big file, GitHub might not let you push that commit</li> <li>If you have a huge file in Git, your repository size can get way too big</li> <li>This is a pain to solve, so use the .gitignore file ahead of time, but if you need to fix this, here is a great resource: </li> <li>Removing Large Files From git Using BFG and a Local Repository</li> </ul> </li> </ul> </li> </ul>"},{"location":"05_version_control/#large-data-and-github","title":"Large Data and GitHub","text":"<p>GitHub allows commited files to be uploaded only if the file is of 100MB or less (with a warning being issued for files between 50MB and 100MB). Additionally, GitHub recommends to keep repositories below the 1GB threshold, as this also allows for quicker cloning and sharing of the repository. If a large file has been uploaded by mistake and you wish to remove it, you can follow these instrutctions.</p> <p>If you do have to work with large files and Git, here are some questions to ask yourself:</p> <ul> <li>Is this data shareable?</li> <li>Are there alternative file hosting platforms I can use?</li> <li>How will this data impact the sharability of this repository?</li> <li>Am I using a .gitignore?</li> </ul> <p>GitHub now offers the Git Large File Storage ( Git LFS): the system works by storing references to the file in your repository, but not the file itself -- it creates a pointer file within the repo, and stores the file elsewhere. If you were to clone the repository, the pointer file will act as a map to show you how to obtain the original file.</p> <p>Git LFS data upload limits are based on your GitHub subscription: </p> <ul> <li>2 GB for GitHub free and GitHub Pro</li> <li>4 GB for GitHub Team</li> <li>5 GB for GitHub Enterprise Cloud</li> </ul> <p>  A depiction of how the Git LFS pointer-repository relationship works. </p>"},{"location":"05_version_control/#useful-github-features","title":"Useful GitHub Features","text":"<p>At its core, GitHub is just a place to host your Git repositories. However, it offers a lot of functionality that has less to do with Git, and more to do with Project Management. We will walk through a few of these useful features.</p> <ul> <li> <p> Issues</p> <ul> <li>Issues let you plan out changes and suggestions to a repo</li> <li>Closing/reopening</li> <li>Labels</li> <li>Assigning</li> <li>Templates</li> <li>Numbering/mentioning</li> </ul> </li> <li> <p> Pull Requests</p> <ul> <li>Pull requests are a way to request merging code from one branch to another</li> <li>typical workflow is for someone to fork a repo, then make a PR from that repo to another</li> <li>Reviews</li> <li>Commenting</li> <li>Merging</li> <li>Closing issues</li> </ul> </li> <li> <p> Organizations</p> <ul> <li>You can use Organizations to organize sets of repositories</li> <li>Roles</li> <li>Teams</li> <li>GitHub documentation:</li> </ul> </li> <li> <p>Other neat things</p> <ul> <li>Permissions/collaborators</li> <li>GitHub Classroom</li> <li>Gists</li> <li>CSV and map rendering</li> <li>Code editor</li> </ul> </li> </ul>"},{"location":"05_version_control/#beyond-git-and-github","title":"Beyond Git and GitHub","text":"<p>There are other platforms that address Version Control and have similar functionalities to GitHub:</p> <ul> <li> <p> GitLab: An alternative to GitHub, GitLab offers both a cloud-hosted platform and a self-hosted option (GitLab CE/EE). It provides a comprehensive DevOps platform with built-in CI/CD, container registry, and more.</p> </li> <li> <p> Bitbucket: Atlassian's Bitbucket is a Git repository hosting service that also supports Mercurial repositories. It offers integration with Jira, Confluence, and other Atlassian products.</p> </li> <li> <p> SourceForge: A platform that provides Git and Subversion hosting, as well as tools for project management, issue tracking, and collaboration.</p> </li> <li> <p> AWS CodeCommit: Part of Amazon Web Services (AWS), CodeCommit is a managed Git service that integrates seamlessly with other AWS services.</p> </li> <li> <p> Azure DevOps Services (formerly VSTS)): Microsoft's Azure DevOps Services offers Git repository hosting along with a wide range of DevOps tools for planning, developing, testing, and deploying software.</p> </li> <li> <p> Mercurial: Like Git, Mercurial is a distributed version control system, but with a different branching and merging model. It's an alternative to Git for version control.</p> </li> </ul>"},{"location":"05_version_control/#self-assessment","title":"Self Assessment","text":"True or False: Using <code>Git</code> requires a GitHub account <p>False</p> <p><code>Git</code> is open source software.</p> <p>GitHub is a privately owned (Microsoft) company</p> <p>Other platforms like GitLab, GitBucket, and GNU Savannah all offer <code>Git</code> as a version control system service.</p> True or False: Using <code>Git</code> is easy <p>False</p> <p>Using <code>Git</code> can be frustrating to even the most experienced users</p> When you find a new repository on GitHub that you think can help your research, what are the first things you should do? <p>Look at the README.md</p> <p>Most GitHub repositories have a README.md file which explains what you're looking at.</p> <p>Look at the LICENSE</p> <p>Not all repositories are licensed the same way - be sure to check the LICENSE file to see whether the software is open source, or if it has specific requirements for reuse. </p>"},{"location":"05_version_control/#adding-code-to-github-locally","title":"Adding Code to Github Locally","text":"<p>Prerequisites</p> <p>You will require the following in case you want to add code locally.</p> Create a GitHub account <p>Navigate to the GitHub website and click Sign Up, and follow the on screen instructions.</p> <p>Installing Git</p> <p>You can follow the official guidelines here: https://github.com/git-guides/install-git. Here we recommend how to install Git on your local machine.</p> Windows <p>These instructions are for Windows users NOT using WSL2. If you do have WSL2, follow the Unix instructions.</p> <ol> <li>Navigate to the latest Git for Windows installer and download the latest version.</li> <li>Once the installer has started, follow the instructions as provided in the Git Setup wizard screen until the installation is complete.</li> <li>Search and open Git Bash. From here, you should be able to run Git commands.</li> </ol> MacOS <ol> <li>Install Homebrew (a package manager for MacOS): `/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"``</li> <li>Install Git: <code>brew install git</code></li> </ol> Unix <p>The following command will install git and all related packages on your Unix machine. <pre><code>$ sudo apt-get install -y git-all\n</code></pre></p> <p>Additionally, you can choose between Generating a Personal Access Token or using SSH keys. This is useful if you want to work locally and push your changes to GitHub. We are going to cover this further in next week's lesson on Version Control.</p> Choice A: Generate a Personal Access Token <p>You can follow the official documentation on how to generate Tokens here. We  discussed how to generate tokens in Week 0. Here's are quick steps you can follow in order to setup your account on your machine using tokens:</p> <ol> <li>On your coumputer:<ol> <li>Clone your repository (<code>git clone &lt;repository&gt;</code>)</li> <li>Make changes where necessary, and add (<code>git add &lt;changed files&gt;</code>), commit (<code>git commit -m \"&lt;message on changes&gt;\"</code>) and push your changes (<code>git push origin</code>).</li> <li>You should be prompted to logging in your GitHub account. Put your email but not your password. Instead, open your web browser and follow the steps below:</li> </ol> </li> <li>On GitHub:<ol> <li>Navigate to your GitHub Settings (You can access your account Settings from the drop down menu where your account icon is, on the top right of the screen)</li> <li>Scroll to the bottom of the left hand side menu to find Developer settings and open it.</li> <li>Click Personal access tokens &gt; Tokens (classic)</li> <li>Click Generate new token &gt; Generate new token (classic). You might need to input your Authentification code if you have enabled 2FA.</li> <li>Give it a name, and all the scopes you require (tip: select all scopes and No Expiration), then click Generate Token. Copy the new generated Token</li> </ol> </li> <li>Back on your computer:<ol> <li>If you have been following the steps above, you should still be in your shell with GitHub still asking for your password.</li> <li>Paste your Token here, and you should be logging in. Your changes should then be saved to GitHub.</li> </ol> </li> </ol> Choice B: Connecting via SSH <p>The process of connecting your computer to GitHub using an SSH key is more expedited (and probably less confusing). </p> <p>As a setup step, see if your computer is already connected to GitHub by doing <code>ssh -T git@github.com</code>. If the response message is <code>git@github.com: Permission denied (publickey).</code> it signifies that your computer is not yet linked with GitHub. To link your computer to github to the following:</p> <ol> <li>Generate an SSH key with a level of encryption that you prefer: <code>ssh-keygen -t ed25519 -C &lt;your github email&gt;</code>. This command generates an SSH key with ed25519 encryption (harder to crack!) and adds your email as \"comment\" (<code>-C</code>, will help recongizing the user adding the key). A number of additional questions are going to ask you where you'd like to save the key and whether you'd like to add a password for protection; unless you want to save it elsewhere, feel free to use the default options. Upon completion you should see something like this: <pre><code>Your identification has been saved in /c/Users/&lt;user&gt;/.ssh/id_ed25519\nYour public key has been saved in /c/Users/&lt;user&gt;/.ssh/id_ed25519.pub\nThe key fingerprint is:\nSHA256:SMSPIStNyA00KPxuYu94KpZgRAYjgt9g4BA4kFy3g1o &lt;your github email&gt;\nThe key's randomart image is:\n+--[ED25519 256]--+\n|^B== o.          |\n|%*=.*.+          |\n|+=.E =.+         |\n| .=.+.o..        |\n|....  . S        |\n|.+ o             |\n|+ =              |\n|.o.o             |\n|oo+.             |\n+----[SHA256]-----+\n</code></pre></li> <li>Upon generating the ssh key, copy it. You can reveal it by doing <code>cat ~/.ssh/id_ed25519.pub</code>.</li> <li>In GitHub, go to your settings: click your account icon on top right, and from the drop down menu, select Settings and then SSH and GPG keys. Here, click on New SSH Key, where you can then paste the newly geneated key. Add a name reflecting your machine and save changes. </li> </ol> <p>Optional: if you want to check if you successfully linked your computer to GitHub, do <code>ssh -t git@github.com</code>. You should receive the following message: `Hi ! You've successfully authenticated, but GitHub does not provide shell access. <p>Adding code locally is a more complex than adding code through the web page, but it allows for better control on what files you commit.</p> <ul> <li>To add or modify code locally, you need to clone the repository on your computer.</li> <li>You can then clone the repository by clicking on the Code button, and copying the link shown</li> <li></li> <li>On your machine, open a terminal window and type the following command: <pre><code>$ git clone &lt;repository address&gt;     # Replace &lt;repository address&gt; with the link you copied such as below\n\n$ git clone https://github.com/CosiMichele/3_git_tutorial.git\nCloning into 'foss23_git_tutorial'...\nremote: Enumerating objects: 13, done.\nremote: Counting objects: 100% (13/13), done.\nremote: Compressing objects: 100% (12/12), done.\nremote: Total 13 (delta 5), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (13/13), 14.47 KiB | 90.00 KiB/s, done.\n</code></pre></li> <li>Your code is now available to you on your machine, and you can add and modify files as needed.</li> </ul> <p>You have modified your code locally, however you still have to push it to the repository. Prior to doing so there are a couple of steps you should do:</p> <ul> <li><code>git status</code>: it checkes on the status of the repository (files that have been modified, deleted, added - from either local or in the online repository)</li> <li><code>git pull</code>: it checks and \"pulls\" changes from the online repository to your local repository. It ensures that you are always updated on the repository files and it can save a lot of time in case there are clashing commits from different users.</li> </ul> <p>To do so:</p> <ul> <li>Add all fiels you have modified and want to commit: <pre><code>$ git add .    # Recall that \".\" (period) stands for all files in a folder \n</code></pre></li> <li>Commit the changes. When committing changes, you have to add a message (in quotation marks) with the <code>-m</code> flag. This message is a concise and descriptive few words about what you did: <pre><code>$ git commit -m \"locally added and modified files\"\n[main 05f0ef6] locally added and modified files\n 2 files changed, 11 insertions(+), 1 deletion(-)\n create mode 100644 file_from_local.md\n</code></pre></li> <li>push your changes with push: <pre><code>$ git push\nEnumerating objects: 6, done.\nCounting objects: 100% (6/6), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (4/4), 585 bytes | 32.00 KiB/s, done.\nTotal 4 (delta 0), reused 0 (delta 0)\nTo https://github.com/CosiMichele/foss22_git_tutorial.git\n   b649de3..05f0ef6  main -&gt; main\n</code></pre></li> </ul> First time Pushing a commit and using Tokens? <p>GitHub is not going to blindly allow you to push changes to the repo, but it will be asking for you to log in.</p> <ul> <li>When asked for the user name:</li> <li>Add the username that you use to login into GitHub</li> <li>When it asks you for the password:</li> <li>DO NOT PUT YOUR PASSWORD, you will require a token instead</li> <li>Generate the token by: <ul> <li>On GitHub, click on your avatar (top right, and navigate to Settings)</li> <li>Scroll down to the bottom of the left hand menu, select Developer settings, and then Personal access tokens</li> <li>Now click on Generate new token (Enter password if requested)</li> <li>Choose the lenght of time for which this token is valid for, a note (for example, a reminder of what computer you're using this token on), and all the functionalities attached to it (as this is your private repository, you can select all the functionalities). Scroll to the bottom of the page and click Generate token</li> <li>Once created, the token is going to appear: copy the token and paste it in the password field in your terminal instead of your password.</li> </ul> </li> </ul> <p>You can now see the changes you made locally on the GitHub repository page.</p> <p></p>"},{"location":"05_version_control/#branching","title":"Branching","text":"<p>Branching allows you to develop your code whilst in a contained environment separate from your main environment. You can view the list and number of branches on the top of your repository.</p> <p></p> <p>Why working on branches?</p> <p>Branches allow you to add/remove/change exisiting code independently from your main branch. This code can include alphas, betas and different versions of your code. Branches can be used to develop documentation or include different functionalitiets focused on Operating Systems and/or clusters and job schedulers. If needed, you can add these codes to your main branch later using pull requests.</p> <p>To create a new branch select the  branch icon (listing the number of branches). This will open the branch page, which will list all of the branches in this repository.</p> <p></p> <p>Select New Branch on the top right. Give the new branch a name of your choice, select the source of code (in this case the only source of code can be the main branch) and select Create branch.</p> <p></p> <p>You can now see the updated list of all your branches.</p> <p></p> <p>You can now use this new branch to create changes you are not yet ready to put in your main branch.</p> <p>Want to delete a branch?</p> <p>You can delete a branch from the branch web page by clicking on the  trash can icon. Beware! All the changes you've made on that branch will be deleted!</p> <p>Working on your machine?</p> <p>Once you create a branch online, you can change to the desired branch on your machine with <code>git switch &lt;branch&gt;</code>. Don't forget to push your changes first!</p> <p>Pull and Tab</p> <ul> <li>Don't forget to perform a <code>git pull</code>!</li> <li>Don't know your branches? Tab! When typing <code>git switch</code>, press tab to see the options of all the branches you've created.</li> </ul>"},{"location":"05_version_control/#pull-requests","title":"Pull Requests","text":"<p>Pull requests (PR) are proposed changes you can make on a repository. In this specific case, pull requests can be used to merge changes from a branch to another. Pull requests can also come from forks of your repository that another user or collaborator has made. </p> <p>Assuming you have made changes in your branch (added a file, for example), a pop up will notify you that a branch has pushed some changes. In case you want to merge the branch and the main repository, you can review and merge by clicking the Compare &amp; pull request button. However, you may want to wait until more changes are made.</p> <p></p> <p>Once you are ready to merge the changes onto your main branch, click on the  branch icon, and select New pull request from the branch you have just made changes. This will open a new page which will list all the changes made showing all files that have been modified, added, or deleted. When you're done reviewing your changes, click Create pull request.</p> <p></p> <p>Pay attention to the information on the PR page!</p> <p>The PR page will not only show you what changes you've made, but also where the changes are coming from (which branch), as well as reviewers, assigneers, labels and other information necessary when working on a big project. It will also show whether the changes are Able to be merged () or not ()! </p> <p>Upon createing the pull request, a new page will open which will test whether the changes can be merged automatically. Changes that are not able to be merged usually clash with other changes other collaborators have made - this will require your revision prior to merging the PR! After revision, select Merge pull request and Confirm merge.</p> <p></p> <p>Your main repository should now have the files created in your other branch and merged through the PR!</p> <p></p>"},{"location":"06_reproducibility_i/","title":"Repeatability and Reproducibility","text":"<p>Learning Objectives</p> <p>After this lesson, you should be able to:</p> <ul> <li>Describe what reproducibility is</li> <li>Discriminate between reproducibility, replicability, and repeatability</li> <li>Explain why reproducible research is valuable </li> <li>Set up a software project with an environment</li> </ul> <p>The so-called reproducibility crisis (see 1 , 2 , 3) is something you have probably heard about (and maybe one of the reasons you have come to FOSS). Headlines in the media (such as Most scientists can't replicate studies by their peers) definitely give pause to researchers and ordinary citizens who hope that the science used to recommend a course of medical treatment or design self-driving cars is sound.</p>"},{"location":"06_reproducibility_i/#software-dependency-hell","title":"Software Dependency Hell","text":"<p>Think for a moment about all the branching possibilities for how a computer could be set up:</p> <ul> <li>hardware: CPUs, GPUs, RAM</li> <li>Operating system: many flavors of Linux, MacOS, Windows</li> <li>Software versions: R, Python, etc.</li> <li>Package versions: specific R or Python packages, etc.</li> </ul> <p>Simply trying to get the same setup as anyone else is difficult enough, but you can also run into all sorts of dependencies. Let's say you try to update a package to match the version someone else used for a project. However, after updating it, you realize you need to update 3 other packages. After that, you realize you need a newer version of R. You finally manage to get everything set up, but when you go back to a different project the next week, nothing works! All those updates made your code for your other project break. You spend a week fixing your code to work with the newer software, and you're finally done... but now your advisor gives you a dataset 10x the size and says you'll need to run it on the cloud. You throw your laptop out the window and move to the woods to live the life of a hermit.</p> <p>All jokes aside, dealing with software dependencies can be extremely frustrating, and so can setting stuff up on a remote location. It can be even more frustrating if you're trying to reproduce results but you don't actually know the entire software stack used to generate them.</p>"},{"location":"06_reproducibility_i/#defining-reproducibility","title":"Defining Reproducibility","text":"<p>Question</p> <p>How do you define reproducible science?</p> Answer <p>In Reproducibility vs. Replicability, Hans Plesser gives the following useful definitions:</p> <ul> <li>Repeatability (Same team, same experimental setup): The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operatingconditions, in the same location on multiple trials.  For computational experiments, this means that a researcher can reliably repeat her own computation.</li> <li>Replicability (Different team, same experimental setup): The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials.  For computational experiments, this means that an independent group can obtain the same result using the author's own artifacts.</li> <li>Reproducibility (Different team, different experimental setup): The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials.  For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently.</li> </ul> <p>The paper goes on to further simplify:</p> <ul> <li>Methods reproducibility: provide sufficient detail about procedures and data so that the same procedures could be exactly repeated.</li> <li>Results reproducibility: obtain the same results from an independent study with procedures as closely matched to the original study as possible.</li> <li>Inferential reproducibility: draw the same conclusions from either an independent replication of a study or a reanalysis of the original study.</li> </ul> <p>Discussion Question: How do these definitions apply to your research/teaching?</p> <p>Work with your fellow learners to develop a shortlist of ways reproducibility relates to your work. Try to identify challenges and even successes you'd like to share.</p> <p>Often, when we say \"reproducibility\" we mean all or at least several of the concepts the proceeding discussion encompasses. Really, reproducibility can be thought of as set values such as some laboratories express in a code of conduct, see for example Ross-Ibarra Lab code of conduct or Bahlai Lab Policies.</p> <p>Reproducibility comes from our obligations and desires to work ethically, honestly, and with confidence that the data and knowledge we produce is done has integrity. Reproducibility is also a \"spectrum of practices\", not a single step. (See figure below from Peng, 2011).</p> <p></p> <p>Assuming you have taken in the potentially anxiety inducing information above, the most important thing to know is that there is a lot of help to make reproducibility a foundation of all of your research.</p>"},{"location":"06_reproducibility_i/#repeatability-a-first-step","title":"Repeatability: a first step","text":"<p>A big first step on the road to reproducibility is repeatability. In the context of computation, this means that you should be able to reliably generate the same results.</p> <p>In many ways, this is the biggest hurdle to reproducibility, as it often requires the biggest leap in skills. You can think of repeatability in a few ways.</p> <p>Discussion Question</p> <p>Have you ever had any hurdles to reproducing your work?</p> <ul> <li>Have you ever run into a problem that prevented you from generating the same results, figures, analyses as before?</li> <li>Have you ever lost time trying to figure out how you (or a collaborator) got a particular result?</li> <li>What were the issues you ran into, and how might you have solved them?</li> </ul>"},{"location":"06_reproducibility_i/#strategies-for-improving-repeatability","title":"Strategies for Improving Repeatability","text":""},{"location":"06_reproducibility_i/#automation","title":"Automation","text":"<p>In the process of making your work more repeatable, you will often be trying to reduce the amount of work you're doing \"by hand\". Reducing the human input necessary at each step of a project is a key to reliably reproducing the same results, but it can also help save you a lot of time in the long run.</p> <p>Have you ever manually edited a figure for a manuscript, only to be asked to change something that negated all your manual edits? Well, in the short run, it may have been quicker to just tinker with the graph by hand, but in the long run, figuring out how to use code to generate the whole thing would have saved you time. </p> <p>Automating tasks often comes with an up-front cost, but it is important for the eventual reproducibility of the work, and will often save you time in the short run. </p> <p>Automation also tends to make tasks scale more easily (editing 10 rows of data by hand is fine, editing 10,000 is much harder), adapt to new scenarios, and extend to future projects.</p> <p>Discussion Question</p> <p>What are some tasks you have automated or want to automate?</p> <ul> <li>Have you ever successfully automated a task?</li> <li>Found a way to make something scale or take less time? </li> <li>What was the task, and how did you do it? </li> <li>Are there any things you wish you could automate?</li> <li>What are some barriers to automating them?</li> </ul> <p>While we often think about writing scripts to clean data, run analyses, and generate figures, there are even more parts of a research project that can be automated. Here are a few examples:</p> <ul> <li>data validation</li> <li>model checking/validation</li> <li>software installation</li> <li>report/manuscript generation</li> <li>citation management</li> <li>email/GitHub/Slack notifications</li> <li>workflow itself (using things like make, Snakemake, Nextflow, targets)</li> </ul> <p>Code can be thought of as a set of machine-actionable instructions, or instructions that we write for a computer to follow. What other sets of instructions do you have, either written down or in your head? How can you turn them into something machine-actionable?</p>"},{"location":"06_reproducibility_i/#get-off-your-own-machine","title":"Get off your own machine","text":"<p>More and more work is being done somewhere other than a personal computer. This could be an HPC cluster at a university or a cloud computing provider. \"Cloud\" just means somebody else is handling the computers, and you get to use them when you need to, typically for a price. Some 'cloud' options include: Binder, Colab, and Cyverse VICE and Github Codespace. </p> <p>The take home message on Cloud is that it is a great way to make your work more reproducible, as you can share a link to your work, and anyone can run it without having to install anything.</p>"},{"location":"06_reproducibility_i/#software-management","title":"Software Management","text":"<p>Have you ever tried to run a script, only to realize you had updated a package without knowing, and now the script doesn't work? </p> <p>Package managers that create and manage custom environments can be extremely helpful in keeping software versions aligned with projects. Good examples of package managers are Conda and Pip.</p> <p>In Python, it is common to use <code>pip</code> and a <code>requirements.txt</code> file, and in R, the <code>renv</code> package can be used to keep package versions stable within individual projects.</p>"},{"location":"06_reproducibility_i/#conda-a-great-environment-and-package-manager","title":"Conda: a GREAT Environment and Package Manager","text":"<p>Conda is an open-source package management system and also an environment management system. This means that it helps manage libraries and dependencies within different projects and can isolate different versions of packages and even Python itself into different environments to maintain project consistency and avoid conflicts between package versions.</p> <p>Here's a breakdown of what Conda offers:</p> <ul> <li> <p>Environment Management:</p> <p>Conda allows users to create isolated environments for their projects. Each environment can have its own set of packages, dependencies, and even its own version of Python. This ensures that different projects can have their own specific requirements without interfering with each other. It allows for consistent and reproducible results across different systems and setups</p> </li> <li> <p>Package Management:</p> <p>Beyond managing environments, Conda is also a package manager. It can install specific versions of software packages and ensure that all dependencies are met. While it's commonly associated with Python, Conda can also manage packages from other languages. You can search for packges at https://anaconda.org/.</p> </li> <li> <p>Cross-Platform:</p> <p>Conda is platform-agnostic. This means you can use it across various operating systems like Windows, macOS, and Linux.</p> </li> <li> <p>Repository Channels:</p> <p>Conda packages are retrieved from repositories known as channels. The default channel has a wide array of commonly used packages. However, users can add third-party channels, such as \"conda-forge\", to access even more packages or specific versions of packages. You can specify the channel by using the <code>-c</code> flag when installing packages.</p> </li> <li> <p>Integration with Anaconda:</p> <p>Conda is the package and environment manager for the Anaconda distribution, which is a distribution of Python and R for scientific computing and data science. However, Conda can be used independently of Anaconda.</p> </li> </ul> <p>Conda comes in two different flavours: Anaconda and Miniconda. Whilst both have the same Conda engine, Miniconda is smaller is size and contains only necessary packages (~500MB), whilst Anaconda comes with everything Miniconda has and more software modules (~2.5GB).</p> <p>  Conda, Miniconda, and Anaconda.  Taken from Getting Started with Conda, Medium. </p> Tip: slow Conda? Try Mamba. <p>Conda is known to take time processing some software installation. A solution is to use Mamba, a reimplementation of Conda in C++ for quicker queries and installations. Mamba is then invoked by using <code>mamba</code> instead of <code>conda</code> (whilst keeping options and the rest of the command synthax the same). </p> <p>The quickest way to install mamba is with <code>conda install -c conda-forge mamba</code>, or follow the official installation documentation here. </p>"},{"location":"06_reproducibility_i/#automation-workflow-management-with-nextflow-and-snakemake","title":"Automation: Workflow Management with Nextflow and Snakemake","text":"<p>Workflow management is key to automation: it helps with bridging the gap between software, creating useful pipelines that help extracting information from data. Two examples of workflow management system popular (but not restricted to!) within the field of Bioinformatics are Nextflow and Snakemake. </p> <p>Both workflow managers are powerful tools that tackle reproducibility and scalability, however the two have some key differences:</p> Feature Snakemake Nextflow Language style Python-based DSL* Groovy-based DSL* Execution Model Rule-based Dataflow-driven Parallelization and Resource Management Implicit parallelization, limited resource management Explicit parallelization, more flexible resource management Integration seamless with Python seamless with Java** <p>* = Domain Specific Langauge  ** = No, it does not mean you need to learn Java</p> <p>Although there are differences between the two workflow managers, there are a number of features which both share:</p> <ul> <li>Available on all OS</li> <li>Well integrated with Conda</li> <li>Support the use of Containers</li> <li>Deployable on HPC systems</li> <li>Integrated error reporting</li> <li>Well established communities***</li> </ul> <p>*** = It should be noted that Nextflow has a community driven repository, nf-core, where people can upload their Nextflow pipelines for others to use. Power to the people!</p> <p>Both are incredibly powerful when it comes to creating pipelines, ultimately the choice on which to use depends on you (and the goal of your project). </p> <p>Here are a couple of script examples for each workflow manager. Although different, the examples try to </p> <ol> <li>Download a genome</li> <li>Align reads using BWA (Burrows-Wheeler Aligner tool)</li> <li>Gather aignment statistics using Samtools (Sequence Alignment/Map)</li> </ol> <p>Snakemake:</p> <pre><code># Snakefile\n\n# Define rule for downloading reference genome\nrule download_reference_genome:\n    output:\n        \"reference_genome.fa\"\n    shell:\n        \"wget http://example.com/reference_genome.fa\"\n\n# Define rule for aligning reads to the reference genome using BWA\nrule align_reads:\n    input:\n        \"reference_genome.fa\",\n        fastq=\"data/{sample}.fastq\"\n    output:\n        \"aligned/{sample}.bam\"\n    shell:\n        \"bwa mem {input.reference_genome} {input.fastq} | samtools view -b - &gt; {output}\"\n\n# Define rule to aggregate alignment statistics\nrule aggregate_stats:\n    input:\n        expand(\"aligned/{sample}.bam\", sample=config['samples'])\n    output:\n        \"alignment_stats.txt\"\n    shell:\n        \"samtools flagstat {input} &gt; {output}\"\n\n# Define samples\nconfigfile: \"config.yaml\"\n</code></pre> <p>Notice how in the Snakefile above, there are a set number of rules that define input, output and commands.</p> <p>Nextflow:</p> <pre><code>// main.nf\n\nparams.samples = ['sample1', 'sample2', 'sample3']\n\n// Define process to download reference genome\nprocess download_reference_genome {\n    output:\n    file('reference_genome.fa')\n\n    script:\n    \"\"\"\n    wget http://example.com/reference_genome.fa -O reference_genome.fa\n    \"\"\"\n}\n\n// Define process to align reads using BWA\nprocess align_reads {\n    input:\n    file(reference_genome), file(fastq) from fastqs\n\n    output:\n    file(\"aligned/${sample}.bam\")\n\n    script:\n    \"\"\"\n    bwa mem ${reference_genome} ${fastq} | samtools view -b - &gt; aligned/${sample}.bam\n    \"\"\"\n}\n\n// Define process to aggregate alignment statistics\nprocess aggregate_stats {\n    input:\n    set file(aligned_bam) from aligned_bams\n\n    output:\n    file(\"alignment_stats.txt\")\n\n    script:\n    \"\"\"\n    samtools flagstat ${aligned_bam} &gt; alignment_stats.txt\n    \"\"\"\n}\n\n// Define workflow execution\nworkflow {\n    // Define input data\n    Channel.fromPath(\"data/*.fastq\").set { fastqs }\n\n    // Execute processes in parallel\n    download_reference_genome()\n    align_reads(fastqs)\n    aggregate_stats()\n}\n</code></pre> <p>The Nextflow file \"forces\" you to know what you are expecting prior to execution by first defining what you want processes you need before defining the workflow script. </p> <p>Although not shown here, Nextflow uses \"Channels\" to help with workflow execution. A channel can have one or more defined processes, but it can \"wait\": these processes wait until the input is ready before executing, thus creating the possibilty of an asynchronous processing pipeline (and parallel processing!). This is also what makes Nextflow able to manage resources better than Snakemake (the processes in \"inactive\" channels do not use resources until they are needed). </p>"},{"location":"06_reproducibility_i/#reproducibility-tutorial","title":"Reproducibility Tutorial","text":"<p>This section is going to cover a short tutorial on reproducibility using software, tools and practices discussed today and throughout FOSS.</p> <p>OS of choice</p> <p>This tutorial will be performed using the CyVerse CLI (Command Line Interface). However, if you'd like to use your own computer feel free to! If you're on Mac or Linux, open your terminal; If you're on Windows, use the Windows Subsystem for Linux (WSL)</p> How to Scroll in Cyverse(Tmux) Cloud Shell <p>If you're using the Cyverse Cloud Shell, you can scroll up and down by pressing <code>Ctrl + b</code> and then <code>[</code> to enter scroll mode. You can then use the arrow keys to scroll up and down. Press <code>q</code> to exit scroll mode.</p> <p>Tutorial Goals</p> <ul> <li>Run a small workflow using NextFlow</li> <li>Understand best practices for reproducing a workflow</li> <li>Apply FOSS procedures in order to enable easiness of reproducibility</li> </ul>"},{"location":"06_reproducibility_i/#prerequisites","title":"Prerequisites","text":"<p>What you'll be using:</p> <ul> <li>GitHub (already installed)</li> <li>Conda</li> <li>Mamba (optional, recommended)</li> </ul> <p>Installable through Conda/Mamba:</p> <ul> <li>Nextflow</li> <li>Salmon</li> <li>FastQC</li> </ul> <p>Installabe through Pip:</p> <ul> <li>MultiQC</li> </ul> What's a Conda and how do I install it? <p>Conda is a popular tool for installing software. Typically software you want to use requires other software (dependancies) to be installed. Conda can manage all of this for you. Each available Conda package is part of a \u201crecipe\u201d that includes everything you need to run your software. There are different versions of Conda, including some specific for bioinformatics like Bioconda.</p> <p>The CyVerse CLI already comes with Conda installed; Please follow these steps in order to install MiniConda (the lightweight version of Conda) on your system.</p> <p>For the appropriate installation package, visit https://docs.conda.io/en/latest/miniconda.html.  Note: If you are using the WSL, install the Linux version!!</p> <pre><code># Download conda and add right permissions\nwget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh     # Modify this to match the OS you're using.\nchmod +x Miniconda3-py39_4.12.0-Linux-x86_64.sh\n\n# install conda silenty (-b) and update (-u) and initial conda run\n./Miniconda3-py39_4.12.0-Linux-x86_64.sh -b -u\n~/miniconda3/bin/conda init\n\n# Restart bash so that conda is activated\nsource ~/.bashrc\n</code></pre> <p>You'll be able to tell when conda is active when next <code>(base)</code> is present next to the to the shell prompt such as</p> <pre><code>(base) user@machine\n</code></pre> <p>Conda should now be installed and can be used to install other necessary packages! </p> <p>When you start a Cyverse Cloud shell, it will start you in the directory:  <pre><code>/home/user/work \n</code></pre> Let's change to the Data-Store directory, where we will be working for the rest of the tutorial. This is the Cyverse cloud-storage directory where you can put all your data and files. </p> <p><pre><code>cd home\ncd &lt;username&gt;\n</code></pre> Create our own environment (select <code>y</code> when prompted).</p> <pre><code>conda create --name myenv\n</code></pre> <p>Activate your new environment with </p> <pre><code>conda activate myenv\n</code></pre> <p>You can see the list of environments you can activate by doing</p> <pre><code>conda env list\n</code></pre>"},{"location":"06_reproducibility_i/#package-management-with-conda","title":"Package management with Conda","text":"<p>We are going to to use conda to install Mamba, NextFlow, Salmon and FastQC.</p> <pre><code># Activate Conda using \nconda activate\n\n# Install Mamba\nconda install -c conda-forge mamba\n</code></pre> <p>You can either use the anaconda website to search for packager, or use the conda search feature (but also, Google is your best friend.)</p> <p>Makes things faster with Mamba</p> <p>Mamba is a reimplemetation of Conda using the C++ language, allowing for much quicker Conda experience. The tutorial is going to use Mamba instead of Conda, but you can always replace <code>mamba</code> with <code>conda</code>!</p> <p>Conda channels</p> <p>Conda operates through channels, specififc repositories where packages are stored. Specific packages sometimes may appear in multiple channels, however it is always helpful to specify a channel with the <code>-c</code> flag.</p> <p>Install Nextflow and verify its installation with the following commands: </p> <pre><code># Install NextFlow\nmamba install -c bioconda nextflow=22.10.6     # Press y when prompted with [Y/n]!\n\n# verify the installation\nnextflow -version\n</code></pre> <p>Now that you know how to install packages with Conda/Mamba, install Salmon and FastQC.</p> Installing Packages <p>As an exercise, install Salmon and FastQC using Conda/Mamba.</p> Need a hand? <pre><code>mamba install -c bioconda salmon\nmamba install -c bioconda fastqc\n</code></pre> <p>Or you can do it with a single line (doable if packages are from the same channel)! <pre><code>mamba install -c bioconda salmon fastqc\n</code></pre></p> <p>You can view the installed conda packages by doing </p> <pre><code>conda list\n</code></pre> <p>In order to make your environment reproducible, conda allows you to export your environment.</p> <pre><code>conda env export &gt; my_conda_env.yml\n</code></pre>"},{"location":"06_reproducibility_i/#package-management-with-pip","title":"Package management with Pip","text":"<p>Pip works similarly to Conda, as Pip is the package management supported by the Python Software foundation. If you use Python for your work it is likely you have installed packages using Pip.</p> <p>We only have to install a single package required for this tutorial, MultiQC. To install MultiQC using Pip, do:</p> <pre><code>pip install multiqc\n</code></pre> <p>Similar to Conda, you can export your pip environment by doing</p> <pre><code>pip3 freeze &gt; my_pip_env.txt\n</code></pre> <p>Why <code>pip3</code>?</p> <p><code>pip3 freeze &gt; my_pip_env.txt</code> is used to export the pip environment such that it is readable for Python 3. If you want to export an environment for Python 2, you can use <code>pip freeze &gt; my_pip_env.txt</code>.</p> <p>Conda exports your Pip environment as well</p> <p>Exporting your environment using Conda (<code>conda env export &gt; my_conda_env.yml</code>) will ALSO export your pip environment!</p>"},{"location":"06_reproducibility_i/#workflow-tutorial-using-nextflow","title":"Workflow Tutorial using Nextflow","text":"<p>...what are we doing?</p> <p>In this tutorial (now that we have set up the environment, repository and pushed our first commit) we are going to:</p> <ul> <li>Index a transcriptome file and quantification of DNA reads (using Salmon).</li> <li>Perform quality controls (with FastQC).</li> <li>Create a MultiQC report.</li> </ul> <p>I Don't know DNA stuff, can I still do this?</p> <p>Absolutely yes! This tutorial is supposed to introduce you to the process of reproducibility using GitHub repositories, package managers and workflow managers! You do not need to understand what each file is as this is meant to show how to make your science reproducible. Focus on understanding the process and theory behind the tutorial rather than the files themselves .</p> <p>Nextflow is a workflow manager, similar to Snakemake. For this tutorial, we decided to use Nextflow as it is easier to learn, more intuitive and user friendly than Snakemake.</p> <p>Download the required files using <code>wget</code> and <code>tar</code> to decompress them</p> <pre><code>wget -O nf_foss_tut.tar.gz https://github.com/CyVerse-learning-materials/foss/blob/mkdocs/docs/assets/tutorials/nf_foss_tut.tar.gz?raw=true\ntar -xvf nf_foss_tut.tar.gz\n</code></pre> <p>We can now look at the decompressed directory structure by using <code>tree nf_foss_tut</code> (if you don not have <code>tree</code> installed, you can install it with <code>sudo apt-get tree</code> or <code>mamba install -c conda-forge tree</code>).</p> <pre><code>.\n\u251c\u2500\u2500 nf_foss_tut\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2502   \u2514\u2500\u2500 ggal\n\u2502   \u2502       \u251c\u2500\u2500 gut_1.fq\n\u2502   \u2502       \u251c\u2500\u2500 gut_2.fq\n\u2502   \u2502       \u251c\u2500\u2500 liver_1.fq\n\u2502   \u2502       \u251c\u2500\u2500 liver_2.fq\n\u2502   \u2502       \u251c\u2500\u2500 lung_1.fq\n\u2502   \u2502       \u251c\u2500\u2500 lung_2.fq\n\u2502   \u2502       \u2514\u2500\u2500 transcriptome.fa\n\u2502   \u251c\u2500\u2500 example_script.nf\n\u2502   \u251c\u2500\u2500 script1.nf\n\u2502   \u251c\u2500\u2500 script2.nf\n\u2502   \u2514\u2500\u2500 script3.nf\n\n2 directories, 11 files\n</code></pre> <p>Files information</p> <ul> <li>Scripts 1 through 3 (<code>script&lt;number&gt;.nf</code>) and <code>example_script.nf</code> are the NextFlow files</li> <li><code>&lt;file&gt;.fq</code> are fastq files, containing DNA sequences and quality scores</li> <li><code>transcriptome.fa</code> is all of the RNA data from the organism (G.gallus)</li> </ul> <p>Let's look at one of the NextFlow files (<code>.nf</code>)</p> <p>Understanding the Nextflow synthax</p> <p>Nextflow is powerful workflow manager as it can be deployed on HPCs and Clouds. However, it does require a little effort in order to understand its synthax. </p> <p>The synthax is broken down into:</p> <ul> <li>Defining parameters early</li> <li>Defining Processes to be executed</li> <li>Defining Channels (blocks that work asynchronously that encapsulate other processes)</li> </ul> <p>More complex scripts include operators (channel manipulation) and executors (to run things on the cloud and HPC); Nextflow can also be used to run and orchestrate containers.</p> <p>As a good example, let's look at <code>example_script.nf</code>: <pre><code>/*                                                                                      \n    * pipeline input parameters                                                            \n    */                                                                                      \n    params.reads = \"$baseDir/data/ggal/gut_{1,2}.fq\"                                        #\n    params.transcriptome = \"$baseDir/data/ggal/transcriptome.fa\"                            # The parameters are set early in the script\n    params.multiqc = \"$baseDir/multiqc\"                                                     #\n    params.outdir = \"results\"                                                               #\n\n    println \"\"\"\\                                                                            #\n            R N A S E Q - N F   P I P E L I N E                                             #\n            ===================================                                             #\n            transcriptome: ${params.transcriptome}                                          # Print statement that will show once the script\n            reads        : ${params.reads}                                                  # is executed\n            outdir       : ${params.outdir}                                                 #\n            \"\"\"                                                                             #\n            .stripIndent()                                                                  #\n\n    /* \n     * create a transcriptome file object given then transcriptome string parameter\n     */\n    transcriptome_file = file(params.transcriptome)                                         # Convert input file to string\n\n    /* \n     * define the `index` process that create a binary index \n     * given the transcriptome file\n     */\n    process index {                                                                         # First process, named \"index\"\n    conda \"bioconda::salmon\"                                                                # Defines what package is necessary\n                                                                                            #\n    input:                                                                                  ## \n    file transcriptome from transcriptome_file                                              ##\n                                                                                            ## Defines the input and output of the process\n    output:                                                                                 ##\n    file 'index' into index_ch                                                              ##\n                                                                                            #\n    script:                                                                                 #\n    \"\"\"                                                                                     #\n    salmon index --threads $task.cpus -t $transcriptome -i index                            # Command to execute\n    \"\"\"                                                                                     #\n    }                                                                                       #\n\n\n    Channel                                                                                 # Channels allows for scripts to work asynchronously, without waiting for received process.\n    .fromFilePairs( params.reads )                                                          # .fromFilePairs method creates a channel emitting the file pairs matching a \"glob\" pattern provided by the user. \n    .ifEmpty { error \"Cannot find any reads matching: ${params.reads}\"  }                   # .ifEmpty emits a value specified if no input is found.\n    .set { read_pairs_ch }                                                                  # .set operator assigns the channel to a variable whose name is specified as a closure parameter.\n                                                                                            #                \n    process quantification {                                                                # Second process, named \"quantification\"\n    conda \"bioconda::salmon\"                                                                # Defines what package is necessary\n    input:                                                                                  #\n    file index from index_ch                                                                ##\n    set pair_id, file(reads) from read_pairs_ch                                             ##\n                                                                                            ## Defines the input and output of the process\n    output:                                                                                 ##\n    file(pair_id) into quant_ch                                                             ##\n                                                                                            #\n    script:                                                                                 #\n    \"\"\"                                                                                                     #\n    salmon quant --threads $task.cpus --libType=U -i index -1 ${reads[0]} -2 ${reads[1]} -o $pair_id        # Command to execute\n    \"\"\"                                                                                                     #\n    }        \n</code></pre></p> <p>Nextflow has in-depth documentation that can be found here.</p> <p>The 3 scripts' tasks are:</p> <ul> <li>Script 1 creates the transcriptome index file, necessary for downstream processes.</li> <li>Script 2 collects read files by pairs (fastq files come in pairs) and performs quantification.</li> <li>Script 3 performs quality control and summarizes all findings in a single report.</li> </ul>"},{"location":"06_reproducibility_i/#script-1-indexing-transcriptome","title":"Script 1: Indexing transcriptome","text":"<p>Execute script 1 <pre><code>nextflow run script1.nf\n</code></pre></p> <p>The output will be something similar to <pre><code>N E X T F L O W  ~  version 22.10.6\nLaunching `script1.nf` [admiring_banach] DSL1 - revision: 66baaf0091\nR N A S E Q - N F   P I P E L I N E    \n===================================\ntranscriptome: /home/user/work/folder/nf_foss_tut/data/ggal/transcriptome.fa\nreads        : /home/user/work/folder/nf_foss_tut/data/ggal/*_{1,2}.fq\noutdir       : results\n\nexecutor &gt;  local (1)\n[f0/0a72bc] process &gt; index [100%] 1 of 1 \u2714\n</code></pre> This is Nextflow's way of telling you that the process has been executed and completed. You should now have a new folder called <code>work</code>. Execute <code>tree work</code> to see what is inside the folder. <pre><code>work\n\u2514\u2500\u2500 f0\n    \u2514\u2500\u2500 0a72bc4d10dba1df2899b0449519e9\n        \u251c\u2500\u2500 index\n        \u2502   \u251c\u2500\u2500 duplicate_clusters.tsv\n        \u2502   \u251c\u2500\u2500 hash.bin\n        \u2502   \u251c\u2500\u2500 header.json\n        \u2502   \u251c\u2500\u2500 indexing.log\n        \u2502   \u251c\u2500\u2500 quasi_index.log\n        \u2502   \u251c\u2500\u2500 refInfo.json\n        \u2502   \u251c\u2500\u2500 rsd.bin\n        \u2502   \u251c\u2500\u2500 sa.bin\n        \u2502   \u251c\u2500\u2500 txpInfo.bin\n        \u2502   \u2514\u2500\u2500 versionInfo.json\n        \u2514\u2500\u2500 transcriptome.fa -&gt; /home/user/work/folder/nf_foss_tut/data/ggal/transcriptome.fa\n\n3 directories, 11 files\n</code></pre> These are new index files from the transcriptome provided.</p>"},{"location":"06_reproducibility_i/#script-2-collecting-pairs-and-performing-quantification","title":"Script 2: collecting pairs and performing quantification","text":"<p>Execute with <pre><code>nextflow run script2.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre></p> <p>The output should look like <pre><code>N E X T F L O W  ~  version 22.10.6\nLaunching `script2.nf` [stupefied_swirles] DSL2 - revision: d3b0d0121c\nR N A S E Q - N F   P I P E L I N E    \n===================================\ntranscriptome: /home/user/work/folder/nf_foss_tut/data/ggal/transcriptome.fa\nreads        : data/ggal/*_{1,2}.fq\noutdir       : results\n\nexecutor &gt;  local(3)                                                                                \n[c1/6ece54] process &gt; index [100%] 1 of 1, cached: 1 \u2714                          \n[1b/10b8d5] process &gt; quantification (1) [100%] 3 of 3 \u2714\n</code></pre></p>"},{"location":"06_reproducibility_i/#script-3-qc-and-report","title":"Script 3: QC and report","text":"<p>Execute with <pre><code>nextflow run script3.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre></p> <p>The output should look like  <pre><code>N E X T F L O W  ~  version 22.10.6\nLaunching `script3.nf` [voluminous_goodall] DSL1 - revision: d118356290\nR N A S E Q - N F   P I P E L I N E    \n===================================\ntranscriptome: /home/user/work/folder/nf_foss_tut/data/ggal/transcriptome.fa\nreads        : data/ggal/*_{1,2}.fq\noutdir       : results\n\nexecutor &gt;  local (4)\n[c1/6ece54] process &gt; index                   [100%] 1 of 1, cached: 1 \u2714\n[7a/4e9ce4] process &gt; quantification (lung)   [100%] 3 of 3, cached: 3 \u2714\n[34/d60dbb] process &gt; fastqc (FASTQC on lung) [100%] 3 of 3 \u2714\n[e9/e7c392] process &gt; multiqc                 [100%] 1 of 1 \u2714\n\nDone! Open the following report in your browser --&gt; results/multiqc_report.html\n</code></pre></p> <p>As you can notice, the report is an <code>html</code> file that can be opened with a browser. Navigate to this file in the Cyverse Data Store and open it. </p>"},{"location":"06_reproducibility_i/#essential-exercise-documenting-your-work","title":"Essential Exercise: Documenting your Work","text":"<p>Document your work. You should still be in your GitHub folder. Summarize your steps and work on your README file, and push your changes! This will ensure that your work and files are saved and have a valid version that you can come back to in the future if you ever require to.</p> <p>Prerequisites</p> <p>You will require the following in case you want to add code locally.</p> Create a GitHub account <p>Navigate to the GitHub website and click Sign Up, and follow the on screen instructions.</p> <p>Installing Git</p> <p>You can follow the official guidelines here: https://github.com/git-guides/install-git. Here we recommend how to install Git on your local machine.</p> Windows <p>These instructions are for Windows users NOT using WSL2. If you do have WSL2, follow the Unix instructions.</p> <ol> <li>Navigate to the latest Git for Windows installer and download the latest version.</li> <li>Once the installer has started, follow the instructions as provided in the Git Setup wizard screen until the installation is complete.</li> <li>Search and open Git Bash. From here, you should be able to run Git commands.</li> </ol> MacOS <ol> <li>Install Homebrew (a package manager for MacOS): `/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"``</li> <li>Install Git: <code>brew install git</code></li> </ol> Unix <p>The following command will install git and all related packages on your Unix machine. <pre><code>$ sudo apt-get install -y git-all\n</code></pre></p> <p>Additionally, you can choose between Generating a Personal Access Token or using SSH keys. This is useful if you want to work locally and push your changes to GitHub. We are going to cover this further in next week's lesson on Version Control.</p> Choice A: Generate a Personal Access Token <p>You can follow the official documentation on how to generate Tokens here. We  discussed how to generate tokens in Week 0. Here's are quick steps you can follow in order to setup your account on your machine using tokens:</p> <ol> <li>On your coumputer:<ol> <li>Clone your repository (<code>git clone &lt;repository&gt;</code>)</li> <li>Make changes where necessary, and add (<code>git add &lt;changed files&gt;</code>), commit (<code>git commit -m \"&lt;message on changes&gt;\"</code>) and push your changes (<code>git push origin</code>).</li> <li>You should be prompted to logging in your GitHub account. Put your email but not your password. Instead, open your web browser and follow the steps below:</li> </ol> </li> <li>On GitHub:<ol> <li>Navigate to your GitHub Settings (You can access your account Settings from the drop down menu where your account icon is, on the top right of the screen)</li> <li>Scroll to the bottom of the left hand side menu to find Developer settings and open it.</li> <li>Click Personal access tokens &gt; Tokens (classic)</li> <li>Click Generate new token &gt; Generate new token (classic). You might need to input your Authentification code if you have enabled 2FA.</li> <li>Give it a name, and all the scopes you require (tip: select all scopes and No Expiration), then click Generate Token. Copy the new generated Token</li> </ol> </li> <li>Back on your computer:<ol> <li>If you have been following the steps above, you should still be in your shell with GitHub still asking for your password.</li> <li>Paste your Token here, and you should be logging in. Your changes should then be saved to GitHub.</li> </ol> </li> </ol> Choice B: Connecting via SSH <p>The process of connecting your computer to GitHub using an SSH key is more expedited (and probably less confusing). </p> <p>As a setup step, see if your computer is already connected to GitHub by doing <code>ssh -T git@github.com</code>. If the response message is <code>git@github.com: Permission denied (publickey).</code> it signifies that your computer is not yet linked with GitHub. To link your computer to github to the following:</p> <ol> <li>Generate an SSH key with a level of encryption that you prefer: <code>ssh-keygen -t ed25519 -C &lt;your github email&gt;</code>. This command generates an SSH key with ed25519 encryption (harder to crack!) and adds your email as \"comment\" (<code>-C</code>, will help recongizing the user adding the key). A number of additional questions are going to ask you where you'd like to save the key and whether you'd like to add a password for protection; unless you want to save it elsewhere, feel free to use the default options. Upon completion you should see something like this: <pre><code>Your identification has been saved in /c/Users/&lt;user&gt;/.ssh/id_ed25519\nYour public key has been saved in /c/Users/&lt;user&gt;/.ssh/id_ed25519.pub\nThe key fingerprint is:\nSHA256:SMSPIStNyA00KPxuYu94KpZgRAYjgt9g4BA4kFy3g1o &lt;your github email&gt;\nThe key's randomart image is:\n+--[ED25519 256]--+\n|^B== o.          |\n|%*=.*.+          |\n|+=.E =.+         |\n| .=.+.o..        |\n|....  . S        |\n|.+ o             |\n|+ =              |\n|.o.o             |\n|oo+.             |\n+----[SHA256]-----+\n</code></pre></li> <li>Upon generating the ssh key, copy it. You can reveal it by doing <code>cat ~/.ssh/id_ed25519.pub</code>.</li> <li>In GitHub, go to your settings: click your account icon on top right, and from the drop down menu, select Settings and then SSH and GPG keys. Here, click on New SSH Key, where you can then paste the newly geneated key. Add a name reflecting your machine and save changes. </li> </ol> <p>Optional: if you want to check if you successfully linked your computer to GitHub, do <code>ssh -t git@github.com</code>. You should receive the following message: `Hi ! You've successfully authenticated, but GitHub does not provide shell access. What if my files are too big? <p>You can always use a <code>.gitignore</code>, a file that within itself has defined what should be saved in GitHub when pushing a commit, and what shouldn't be saved. An alternative is to move your files outside of the respository that you're pushing (\"stashing\").</p>"},{"location":"06_reproducibility_i/#github-repository-setup-and-documentation","title":"GitHub repository setup and documentation","text":"<p>Create a repository on GitHub to document your work:</p> <ul> <li>On GitHub, navigate to your account page and create a new repository (add a README to create structure!)</li> <li>Clone your repository locally with <code>git clone &lt;repository_url&gt;.git</code> (find the url under the green Code button)</li> <li>Navigate to your cloned repository with <code>cd &lt;repository_name&gt;</code>. You should now be inside your repository.</li> <li>Move your environemnt files into your repository with <code>mv ../my_conda_env.yml ../my_pip_env.txt .</code>.</li> <li>Modify your README to reflect the work so far, with meaningful comments (remember that the README is formatted with markdown, a guide to markdown here). A well documented document may look similar to:</li> </ul> <pre><code># reproducibility-tutorial\n\nThis repository contains information about the reproduciblility tutorial from [FOSS 2023 Spring](https://foss.cyverse.org/06_reproducibility_i/#reproducibility-tutorial).\n\n## Environment Setup\n\n- Download conda and add right permissions\n```\nwget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh\nchmod +x Miniconda3-py39_4.12.0-Linux-x86_64.sh\n```\n- Install conda silenty (-b), update (-u) and initial start\n```\n./Miniconda3-py39_4.12.0-Linux-x86_64.sh -b -u\n~/miniconda3/bin/conda init\n```\n-  Restart bash so that conda is activated\n```\nsource ~/.bashrc\n```\n- Install Mamba\n```\nconda install -c conda-forge mamba\n```\n- Use environment files in this repo to recreate tutorial env\n```\nmamba install -f &lt;my_conda_env.yml&gt;     # Will also install pip packages\n```\n\n## Obtaining tutorial files\n\nTutorial files avaiable [here](https://github.com/CyVerse-learning-materials/foss/blob/mkdocs/docs/assets/tutorials/nf_foss_tut.tar.gz?raw=true). Use `wget` to download appropriate files and decompress files with `tar -xvf`.\n```\nwget -O nf_foss_tut.tar.gz https://github.com/CyVerse-learning-materials/foss/blob/mkdocs/docs/assets/tutorials/nf_foss_tut.tar.gz?raw=true\ntar -xvf nf_foss_tut.tar.gz\n```\n\n## Workflow tutorial using Nextflow\nSteps of the nextflow tutorial will be added in future commits.\n</code></pre> <ul> <li>Add, commit and push your changes</li> </ul> <pre><code>git add .\ngit commit -m \"documenting the tutorial\"\ngit push\n</code></pre>"},{"location":"07_reproducibility_ii/","title":"Reproducibility II: Containers","text":"<p>Learning Objectives</p> <p>After this lesson, you should be able to:</p> <ul> <li>Explain what containers are used for in reproducible research contexts</li> <li>Search for and run a Docker container locally or on a remote system</li> <li>Understand how version control and data can be used inside a container</li> </ul>"},{"location":"07_reproducibility_ii/#reproducible-computer-code","title":"Reproducible Computer Code","text":"<p>Sharing your scientific analysis code with your colleagues is an act of collaboration that will help push your field forward. There are, however, technical challenges that may prevent your colleagues from effectively running the code on their own computer. These include:</p> <ul> <li>hardware: CPUs, GPUs, RAM</li> <li>Operating System: Linux, MacOS, Windows</li> <li>Software version: R, Python, etc</li> <li>Library versions and dependencies</li> </ul> <p>How do we make it easier to share analyis code and avoid the challenges of computer and environment setups?</p>"},{"location":"07_reproducibility_ii/#what-are-containers","title":"What are containers?","text":"<p>A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. Container images are a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Each of these elements are specifically versioned and do not change. The user does not need to install the software in the traditional sense.</p> <p>A useful analogy is to think of software containers as shipping containers. It allows us move cargo (software) around the world in standard way. The shipping container can be offloading and executed anywhere, as long the destination has a shipping port (i.e., Docker) </p> <p>  Software Shipping Containers </p> <p></p> <p>Containers are similar to virtual machines (VMs), but are smaller and easier to share. A big distinction between Containers and VMs is what is within each environment: VMs require the OS to be present within the image, whilst containers rely on the host OS and the container engine (e.g., Docker Engine). </p> <p>  Difference between Virtual Machines and Containers. Containers are a lot more portable as these do not require an OS to be bundled with the software. Figure source: Microsoft Cloudblogs. </p>"},{"location":"07_reproducibility_ii/#containers-for-reproducible-science","title":"Containers for Reproducible Science","text":"<p>Software containers, such as those managed by Docker or Singularity, are incredibly useful for reproducible science for several reasons:</p>"},{"location":"07_reproducibility_ii/#environment-consistency","title":"Environment Consistency:","text":"<p>Containers encapsulate the software environment, ensuring that the same versions of software, libraries, and dependencies are used every time, reducing the \"it works on my machine\" problem.</p>"},{"location":"07_reproducibility_ii/#ease-of-sharing","title":"Ease of Sharing:","text":"<p>Containers can be easily shared with other researchers, allowing them to replicate the exact software environment used in a study.</p>"},{"location":"07_reproducibility_ii/#platform-independence","title":"Platform Independence:","text":"<p>Containers can run on different operating systems and cloud platforms, allowing for consistency across different hardware and infrastructure.</p>"},{"location":"07_reproducibility_ii/#version-control","title":"Version Control:","text":"<p>Containers can be versioned, making it easy to keep track of changes in the software environment over time.</p>"},{"location":"07_reproducibility_ii/#scalability","title":"Scalability:","text":"<p>Containers can be easily scaled and deployed on cloud infrastructure, allowing for reproducible science at scale.</p>"},{"location":"07_reproducibility_ii/#isolation","title":"Isolation:","text":"<p>Containers isolate the software environment from the host system, reducing the risk of conflicts with other software and ensuring a clean and controlled environment.</p> <p> </p> <ul> <li> <p>The most common container software is  Docker, which is a platform for developers and sysadmins to develop, deploy, and run applications with containers. Apptainer (formerly, Singularity), is another popular container engine, which allows you to deploy containers on HPC clusters.</p> </li> <li> <p>DockerHub is the world's largest respository of container images. Think of it as the 'Github' of container images. It facilitates collaboration amongst developers and allows you to share your container images with the world. Dockerhub allows users to maintain different versions of container images.</p> </li> </ul> <p>Warning</p> <p>While Docker allows you to quickly run software from other people, it may not work across every platform. There are different CPU architectures (<code>arm</code>, <code>amd64</code>, <code>x64,</code>x86`) deployed across cloud, computer workstations, laptops, and cellular phones. Docker containers and their software can be cross-compiled across architectures, but this must be done by the creators.</p>"},{"location":"07_reproducibility_ii/#introduction-to-docker","title":"Introduction to  Docker","text":""},{"location":"07_reproducibility_ii/#prerequisites","title":"Prerequisites","text":"<p>In order to complete these exercises we STRONGLY recommend that you set up a personal  GitHub and  DockerHub account (account creation for both services is free). </p> <p>There are no specific skills needed for this tutorial beyond elementary command line ability and using a text editor. </p> <p>We are going to be using  GitHub CodeSpaces for the hands on portion of the workshop, which features  VS Code as a fully enabled development environment with Docker already installed. </p> <p>CodeSpaces is a featured product from GitHub and requires a paid subscription or Academic account for access. Your account will temporarily be integrated with the course GitHub Organization for the next steps in the workshop.</p> <p>Our instructions on starting a new CodeSpace are here. </p> Installing Docker on your personal computer <p>We are going to be using virtual machines on the cloud for this course, and we will explain why this is a good thing, but there may be a time when you want to run Docker on your own computer.</p> <p>Installing Docker takes a little time but it is reasonably straight forward and it is a one-time setup.</p> <p>Installation instructions from Docker Official Docs for common OS and chip architectures:</p> <ul> <li> Mac OS X</li> <li> Windows</li> <li> Ubuntu Linux</li> </ul> Never used a terminal before? <p>Before venturing much further, you should review the Software Carpentry lessons on \"The Unix Shell\" and \"Version Control with Git\" -- these are great introductory lessons related to the skills we're teaching here.</p> <p>You've given up on ever using a terminal? No problem, Docker can be used from graphic interfaces, like Docker Desktop, or platforms like Portainer. We suggest you read through their documentation on how to use Docker.</p>"},{"location":"07_reproducibility_ii/#fundamental-docker-commands","title":"Fundamental Docker Commands","text":"<p>Docker commands in the terminal use the prefix <code>docker</code>.</p> <p>For every command listed, the correct execution of the commands through the command line is by using <code>docker</code> in front of the command: for example <code>docker help</code> or <code>docker search</code>. Thus, every  = <code>docker</code>.</p>"},{"location":"07_reproducibility_ii/#help","title":"help","text":"<p>Like many other command line applications the most helpful flag is the <code>help</code> command which can be used with the Management Commands:</p> <pre><code>$ docker \n$ docker --help\n</code></pre>"},{"location":"07_reproducibility_ii/#search","title":"search","text":"<p>We talk about the concept of Docker Registries in the next section, but you can search the public list of registeries by using the <code>docker search</code> command to find public containers on the Official Docker Hub Registry:</p> <pre><code>$ docker search  \n</code></pre>"},{"location":"07_reproducibility_ii/#pull","title":"pull","text":"<p>Go to the Docker Hub and type <code>hello-world</code> in the search bar at the top of the page. </p> <p>Click on the 'tag' tab to see all the available 'hello-world' images. </p> <p>Click the 'copy' icon at the right to copy the <code>docker pull</code> command, or type it into your terminal:</p> <pre><code>$ docker pull hello-world\n</code></pre> <p>Note</p> <pre><code>If you leave off the `:` and the tag name, it will by default pull the `latest` image\n</code></pre> <pre><code>$ docker pull hello-world\nUsing default tag: latest\nlatest: Pulling from library/hello-world\n2db29710123e: Pull complete \nDigest: sha256:bfea6278a0a267fad2634554f4f0c6f31981eea41c553fdf5a83e95a41d40c38\nStatus: Downloaded newer image for hello-world:latest\ndocker.io/library/hello-world:latest\n</code></pre> <p>Now try to list the files in your current working directory:</p> <pre><code>$ ls -l\n</code></pre> Where is the image you just pulled? <p>Docker saves container images to the Docker directory (where Docker is installed). </p> <p>You won't ever see them in your working directory.</p> <p>Use 'docker images' to see all the images on your computer:</p> <pre><code>$ docker images\n</code></pre>"},{"location":"07_reproducibility_ii/#run","title":"run","text":"<p>The single most common command that you'll use with Docker is <code>docker run</code> (see official help manual for more details).</p> <pre><code>$ docker run hello-world:latest\n</code></pre> <p>In the demo above, you used the <code>docker pull</code> command to download the <code>hello-world:latest</code> image.</p> <p>What about if you run a container that you haven't downloaded?</p> <pre><code>$ docker run alpine:latest\n</code></pre> <p>When you executed the command <code>docker run alpine:latest</code>, Docker first looked for the cached image locally, but did not find it, it then ran a <code>docker pull</code> behind the scenes to download the <code>alpine:latest</code> image and then execute your command.</p>"},{"location":"07_reproducibility_ii/#images","title":"images","text":"<p>You can now use the <code>docker images</code> command to see a list of all the cached images on your system:</p> <pre><code>$ docker images \nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nalpine                  latest              c51f86c28340        4 weeks ago         1.109 MB\nhello-world             latest              690ed74de00f        5 months ago        960 B\n</code></pre> Inspecting your containers <p>To find out more about a Docker images, run <code>docker inspect hello-world:latest</code></p>"},{"location":"07_reproducibility_ii/#ps","title":"ps","text":"<p>Now it's time to see the <code>docker ps</code> command which shows you all containers that are currently running on your machine. </p> <pre><code>docker ps\n</code></pre> <p>Since no containers are running, you see a blank line.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</code></pre> <p>Let's try a more useful variant: <code>docker ps --all</code></p> <pre><code>$ docker ps --all\nCONTAINER ID   IMAGE                                            COMMAND                  CREATED          STATUS                      PORTS     NAMES\na5eab9243a15   hello-world                                      \"/hello\"                 5 seconds ago    Exited (0) 3 seconds ago              loving_mcnulty\n3bb4e26d2e0c   alpine:latest                                    \"/bin/sh\"                17 seconds ago   Exited (0) 16 seconds ago             objective_meninsky\n192ffdf0cbae   opensearchproject/opensearch-dashboards:latest   \"./opensearch-dashbo\u2026\"   3 days ago       Exited (0) 3 days ago                 opensearch-dashboards\na10d47d3b6de   opensearchproject/opensearch:latest              \"./opensearch-docker\u2026\"   3 days ago       Exited (0) 3 days ago                 opensearch-node1\n</code></pre> <p>What you see above is a list of all containers that you have run. </p> <p>Notice that the <code>STATUS</code> column shows the current condition of the container: running, or as shown in the example, when the container was exited.</p>"},{"location":"07_reproducibility_ii/#stop","title":"stop","text":"<p>The <code>stop</code> command is used for containers that are actively running, either as a foreground process or as a detached background one.</p> <p>You can find a running container using the <code>docker ps</code> command.</p>"},{"location":"07_reproducibility_ii/#rm","title":"rm","text":"<p>You can remove individual stopped containers by using the <code>rm</code> command. Use the <code>ps</code> command to see all your stopped contiainers:</p> <pre><code>@user \u279c /workspaces $ docker ps -a\nCONTAINER ID   IMAGE                        COMMAND                  CREATED              STATUS                          PORTS     NAMES\n03542eaac9dc   hello-world                  \"/hello\"                 About a minute ago   Exited (0) About a minute ago             unruffled_nobel\n</code></pre> <p>Use the first few unique alphanumerics in the CONTAINER ID to remove the stopped container:</p> <pre><code>@user \u279c /workspaces (mkdocs \u2717) $ docker rm 0354\n0354\n</code></pre> <p>Check to see that the container is gone using <code>ps -a</code> a second time (<code>-a</code> is shorthand for <code>--all</code>; the full command is <code>docker ps -a</code> or <code>docker ps --all</code>).</p>"},{"location":"07_reproducibility_ii/#rmi","title":"rmi","text":"<p>The <code>rmi</code> command is similar to <code>rm</code> but it will remove the cached images. Used in combination with <code>docker images</code> or <code>docker system df</code> you can clean up a full cache</p> <pre><code>docker rmi\n</code></pre> <pre><code>@user \u279c /workspaces/ (mkdocs \u2717) $ docker images\nREPOSITORY                   TAG       IMAGE ID       CREATED        SIZE\nopendronemap/webodm_webapp   latest    e075d13aaf35   21 hours ago   1.62GB\nredis                        latest    a10f849e1540   5 days ago     117MB\nopendronemap/nodeodm         latest    b4c50165f838   6 days ago     1.77GB\nhello-world                  latest    feb5d9fea6a5   7 months ago   13.3kB\nopendronemap/webodm_db       latest    e40c0f274bba   8 months ago   695MB\n@user \u279c /workspaces (mkdocs \u2717) $ docker rmi hello-world\nUntagged: hello-world:latest\nUntagged: hello-world@sha256:10d7d58d5ebd2a652f4d93fdd86da8f265f5318c6a73cc5b6a9798ff6d2b2e67\nDeleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412\nDeleted: sha256:e07ee1baac5fae6a26f30cabfe54a36d3402f96afda318fe0a96cec4ca393359\n@user \u279c /workspaces (mkdocs \u2717) $ \n</code></pre>"},{"location":"07_reproducibility_ii/#system","title":"system","text":"<p>The <code>system</code> command can be used to view information about containers on your cache, you can view your total disk usage, view events or info.</p> <p>You can also use it to <code>prune</code> unused data and image layers.</p> <p>To remove all cached layers, images, and data you can use the <code>-af</code> flag for <code>all</code> and <code>force</code></p> <pre><code>docker system prune -af\n</code></pre>"},{"location":"07_reproducibility_ii/#tag","title":"tag","text":"<p>By default an image will recieve the tag <code>latest</code> when it is not specified during the <code>docker build</code> </p> <p>Image names and tags can be created or changed using the <code>docker tag</code> command. </p> <pre><code>docker tag imagename:oldtag imagename:newtag\n</code></pre> <p>You can also change the registry name used in the tag:</p> <pre><code>docker tag docker.io/username/imagename:oldtag harbor.cyverse.org/project/imagename:newtag\n</code></pre> <p>The cached image laters will not change their <code>sha256</code> and both image tags will still be present after the new tag name is generated. </p>"},{"location":"07_reproducibility_ii/#example-of-running-a-container","title":"Example of Running a Container","text":"<p>This is a tutorial to demonstrate how to run PDAL using Docker. PDAL is a stand-alone software package that can analyze and manipulate point cloud data files such as .las and .laz. In this tutorial, we will convert a LiDAR .laz file into a Cloud-optimized Point Cloud format (.copc.laz). </p>"},{"location":"07_reproducibility_ii/#1-clone-this-repository-to-your-local-machine","title":"1. Clone this repository to your local machine","text":"<p><code>git clone https://github.com/jeffgillan/pdal_copc.git</code></p>"},{"location":"07_reproducibility_ii/#2-change-directories-into-the-newly-cloned-repository","title":"2. Change directories into the newly cloned repository","text":"<p><code>cd pdal_copc</code></p>"},{"location":"07_reproducibility_ii/#3-run-the-container","title":"3. Run the Container","text":"<p><code>docker run -v $(pwd):/data jeffgillan/pdal_copc:1.0</code></p> <p>You are mounting a local volume (-v) directory to the container (<code>/data</code>). This local directory should have all of the point clouds files you want to convert. <code>$(pwd)</code> is telling it that the point clouds are in the current working directory. </p> <p><code>jeffgillan/pdal_copc:1.0</code> is the name of the container image you want to run. </p> <p><code>jeffgillan</code> = the Dockerhub account name</p> <p><code>pdal_copc</code> = the name of the image</p> <p><code>1.0</code> = the tag name    </p> <p>Your if everything worked correctly, you should have a new file <code>tree.copc.laz</code> in your present working directory.</p>"},{"location":"07_reproducibility_ii/#working-with-interactive-containers","title":"Working with Interactive Containers","text":"<p>Let's go ahead and run some Integrated Development Environment images from \"trusted\" organizations on the Docker Hub Registry.</p>"},{"location":"07_reproducibility_ii/#jupyter-lab","title":"Jupyter Lab","text":"<p>In this section, let's find a Docker image which can run a Jupyter Notebook</p> <p>Search for official images on Docker Hub which contain the string 'jupyter'</p> <pre><code>$ docker search jupyter\n</code></pre> <p>It should return something like:</p> <pre><code>NAME                                   DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\njupyter/datascience-notebook           Jupyter Notebook Data Science Stack from htt\u2026   912                  \njupyter/all-spark-notebook             Jupyter Notebook Python, Scala, R, Spark, Me\u2026   374                  \njupyter/scipy-notebook                 Jupyter Notebook Scientific Python Stack fro\u2026   337                  \njupyterhub/jupyterhub                  JupyterHub: multi-user Jupyter notebook serv\u2026   307                  [OK]\njupyter/tensorflow-notebook            Jupyter Notebook Scientific Python Stack w/ \u2026   298                  \njupyter/pyspark-notebook               Jupyter Notebook Python, Spark, Mesos Stack \u2026   224                  \njupyter/base-notebook                  Small base image for Jupyter Notebook stacks\u2026   168                  \njupyter/minimal-notebook               Minimal Jupyter Notebook Stack from https://\u2026   150                  \njupyter/r-notebook                     Jupyter Notebook R Stack from https://github\u2026   44                   \njupyterhub/singleuser                  single-user docker images for use with Jupyt\u2026   43                   [OK]\njupyter/nbviewer                       Jupyter Notebook Viewer                         27                   [OK]\n</code></pre> Untrusted community images <p>An important thing to note: None of these Jupyter or RStudio images are 'official' Docker images, meaning they could be trojans for spyware, malware, or other nasty warez.</p>"},{"location":"07_reproducibility_ii/#understanding-ports","title":"Understanding PORTS","text":"<p>When we want to run a container that runs on the open internet, we need to add a TCP or UDP port number from which we can access the application in a browser using the machine's IP (Internet Protocol) address or DNS (Domain Name Service) location.</p> <p>To do this, we need to access the container over a separate port address on the machine we're working on.</p> <p>Docker uses the flag <code>--port</code> or <code>-p</code> for short followed by two sets of port numbers. </p> Exposing Ports <p>Docker can in fact expose all ports to a container using the capital <code>-P</code> flag</p> <p>For security purposes, it is generally NEVER a good idea to expose all ports.</p> <p>Typically these numbers can be the same, but in some cases your machine may already be running another program (or container) on that open port.</p> <p>The port has two sides <code>left:right</code> separated by a colon. The left side port number is the INTERNAL port that the container software thinks its using. The right side number is the EXTERNAL port that you can access on your computer (or virtual machine).</p> <p>Here are some examples to run basic RStudio and Jupyter Lab:</p> <pre><code>$ docker run --rm -p 8787:8787 -e PASSWORD=cc2022 rocker/rstudio\n</code></pre> <p>note: on CodeSpaces, the reverse proxy for the DNS requires you to turn off authentication</p> <pre><code>$ docker run --rm -p 8787:8787 -e DISABLE_AUTH=true rocker/rstudio\n</code></pre> <p><pre><code>$ docker run --rm -p 8888:8888 jupyter/base-notebook\n</code></pre> <pre><code>docker run --rm -p 8888:8888 jupyter/base-notebook start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''\n</code></pre></p> Preempting stale containers from your cache <p>We've added the <code>--rm</code> flag, which means the container will automatically removed from the cache when the container is exited.</p> <p>When you start an IDE in a terminal, the terminal connection must stay active to keep the container alive.</p>"},{"location":"07_reproducibility_ii/#detaching-your-container-while-it-is-running","title":"Detaching your container while it is running","text":"<p>If we want to keep our window in the foreground  we can use the <code>-d</code> - the detached flag will run the container as a background process, rather than in the foreground. </p> <p>When you run a container with this flag, it will start, run, telling you the container ID:</p> <p><pre><code>$ docker run --rm -d -p 8888:8888 jupyter/base-notebook\n</code></pre> Note, that your terminal is still active and you can use it to launch more containers. </p> <p>To view the running container, use the <code>docker ps</code> command.</p>"},{"location":"07_reproducibility_ii/#interactive-commands-with-containers","title":"Interactive Commands with Containers","text":"<p>Lets try another command, this time to access the container as a shell:</p> <pre><code>$ docker run alpine:latest sh\n</code></pre> <p>Wait, nothing happened, right? </p> <p>Is that a bug? </p> <p>Well, no.</p> <p>The container will exit after running any scripted commands such as <code>sh</code>, unless they are run in an \"interactive\" terminal (TTY) - so for this example to not exit, you need to add the <code>-i</code> for interactive and <code>-t</code> for TTY. You can run them both in a single flag as <code>-it</code>, which is the more common way of adding the flag:</p> <pre><code>$ docker run -it alpine:latest sh\n</code></pre> <p>The prompt should change to something more like <code>/ #</code>.</p> <p>You are now running a shell inside the container! </p> <p>Try out a few commands like <code>ls -l</code>, <code>uname -a</code> and others.</p> <p>Exit out of the container by giving the <code>exit</code> command.</p> <pre><code>/ # exit\n</code></pre> Making sure you've exited the container <p>If you type <code>exit</code> your container will exit and is no longer active. To check that, try the following:</p> <pre><code>$ docker ps --latest\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                          PORTS                    NAMES\nde4bbc3eeaec        alpine                \"/bin/sh\"                3 minutes ago       Exited (0) About a minute ago                            pensive_leavitt\n</code></pre> <p>If you want to keep the container active, then you can use keys <code>ctrl +p</code> <code>ctrl +q</code>. To make sure that it is not exited run the same <code>docker ps --latest</code> command again:</p> <pre><code>$ docker ps --latest\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                         PORTS                    NAMES\n0db38ea51a48        alpine                \"sh\"                     3 minutes ago       Up 3 minutes                                            elastic_lewin\n</code></pre> <p>Now if you want to get back into that container, then you can type <code>docker attach &lt;container id&gt;</code>. This way you can save your container:</p> <pre><code>$ docker attach 0db38ea51a48\n</code></pre>"},{"location":"07_reproducibility_ii/#house-keeping-and-cleaning-up-exited-containers","title":"House Keeping and  Cleaning Up Exited Containers","text":""},{"location":"07_reproducibility_ii/#managing-docker-images","title":"Managing Docker Images","text":"<p>In the previous example, you pulled the <code>alpine</code> image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the <code>docker images</code> command.</p> <pre><code>$ docker images\nREPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE\nubuntu                     bionic              47b19964fb50        4 weeks ago         88.1MB\nalpine                     latest              caf27325b298        4 weeks ago         5.53MB\nhello-world                latest              fce289e99eb9        2 months ago        1.84kB\n</code></pre> <p>Above is a list of images that I've pulled from the registry. You will have a different list of images on your machine. The TAG refers to a particular snapshot of the image and the ID is the corresponding unique identifier for that image.</p> <p>For simplicity, you can think of an image akin to a Git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to latest.</p>"},{"location":"07_reproducibility_ii/#clutter-and-cache","title":"Clutter and Cache","text":"<p>Docker images are cached on your machine in the location where Docker was installed. These image files are not visible in the same directory where you might have used <code>docker pull &lt;imagename&gt;</code>.</p> <p>Some Docker images can be large. Especially data science images with many scientific programming libraries and packages pre-installed.</p> <p>Pulling many images from the Docker Registries may fill up your hard disk! To inspect your system and disk use:</p> <pre><code>$ docker system info\n$ docker system df\n</code></pre> <p>To find out how many images are on your machine, type:</p> <pre><code>$ docker images\n</code></pre> <p>To remove images that you no longer need, type:</p> <pre><code>$ docker system prune\n</code></pre> <p>This is where it becomes important to differentiate between images, containers, and volumes (which we'll get to more in a bit).  You can take care of all of the dangling images and containers on your system. </p> <p>Note, that <code>prune</code> will not remove your cached images </p> <pre><code>$ docker system prune\n    WARNING! This will remove:\n    - all stopped containers\n    - all networks not used by at least one container\n    - all dangling images\n    - all dangling build cache\n\nAre you sure you want to continue? [y/N]\n</code></pre> <p>If you added the <code>-af</code> flag it will remove \"all\" <code>-a</code> dangling images, empty containers, AND ALL CACHED IMAGES with \"force\" <code>-f</code>.</p>"},{"location":"07_reproducibility_ii/#managing-data-in-docker","title":"Managing Data in Docker","text":"<p>It is possible to store data within the writable layer of a container, but there are some limitations:</p> <ul> <li>The data doesn\u2019t persist when that container is no longer running, and it can be difficult to get the data out of the container if another process needs it.</li> <li>A container\u2019s writable layer is tightly coupled to the host machine where the container is running. You can\u2019t easily move the data somewhere else.</li> <li>Its better to put your data into the container AFTER it is built - this keeps the container size smaller and easier to move across networks.</li> </ul> <p>Docker offers three different ways to mount data into a container from the Docker host: Volumes, tmpfs mounts and bind mounts. Here, we will only be exploring Volumes.</p> <p>  The various methods for accessing data using containers. tmpfs mounts store data directly in memory, bind mounts and volumes use the host's file system. Volumes are flexible and only attach a specific directory to the container, whilst bind mounts require the user to share the full path to a file in order to allow the container to access it. Taken from the official Docker documentation on data management with docker. </p> <p>Why Volumes?</p> <p>Volumes are often a better choice than persisting data in a container\u2019s writable layer, because using a volume does not increase the size of containers using it, and the volume\u2019s contents exist outside the lifecycle of a given container. Some of the advantages of volumes include:</p> <ul> <li>Volumes are easier to back up or migrate.</li> <li>You can manage volumes using Docker CLI commands or the Docker API.</li> <li>Volumes work on both UNIX and Windows containers.</li> <li>Volumes can be more safely shared among multiple containers.</li> <li>A new volume\u2019s contents can be pre-populated by a container.</li> </ul> <p>The <code>-v</code> flag is used for mounting volumes:</p> <p><code>-v</code> or <code>--volume</code>: Consists of three fields, separated by colon characters (:). </p> <p>The fields must be in the correct order, and the meaning of each field is not immediately obvious.</p> <p>Required:</p> <ul> <li>The first field is the path on your local machine that where the data are.</li> <li>The second field is the path where the file or directory are mounted in the container.</li> </ul> <p>Optional:</p> <ul> <li>The third field is optional, and is a comma-separated list of options, such as <code>ro</code> (read only).</li> </ul> <p>The synthax looks like the following: <pre><code>-v /home/username/your_data_folder:/container_folder\n</code></pre></p> <p>This is what a full docker command with a mounted volume looks like: <pre><code>$ docker run -v /home/$USER/read_cleanup:/work alpine:latest ls -l /work\n</code></pre></p> <p>So what if we wanted to work interactively inside the container?</p> <pre><code>$ docker run -it -v /home/$USER/read_cleanup:/work alpine:latest sh\n</code></pre> <p>Once you're in the container, you will see that the <code>/work</code> directory is mounted in the working directory.</p> <p>Any data that you add to that folder outside the container will appear INSIDE the container. And any work you do inside the container saved in that folder will be saved OUTSIDE the container as well.</p>"},{"location":"07_reproducibility_ii/#docker-commands","title":"Docker Commands","text":"<p>Here is a compiled list of fundamental Docker Commands:</p> Command Usage Example <code>pull</code> Downloads an image from Docker Hub <code>docker pull hello-world:latest</code> <code>run</code> runs a container with entrypoint <code>docker run -it user/image:tag</code> <code>build</code> Builds a docker image from a Dockerfile in current working directory <code>docker build -t user/image:tag .</code> <code>images</code> List all images on the local machine <code>docker images list</code> <code>tag</code> Adds a different tag name to an image <code>docker tag hello-world:latest hello-world:new-tag-name</code> <code>login</code> Authenticate to the Docker Hub (requires username and password) <code>docker login</code> <code>push</code> Upload your new image to the Docker Hub <code>docker push user/image:tag</code> <code>inspect</code> Provide detailed information on constructs controlled by Docker <code>docker inspect containerID</code> <code>ps</code> List all containers on your system <code>docker ps -a</code> <code>rm</code> Delete a stopped or running container <code>docker rm -f &lt;container ID&gt;</code> <code>rmi</code> Delete an image from your cache <code>docker rmi hello-world:latest</code> <code>stop</code> Stop a running container <code>docker stop alpine:latest</code> <code>system</code> View system details, remove old images and containers with <code>prune</code> <code>docker system prune</code> <code>push</code> Uploads an image to the Docker Hub (or other private registry) <code>docker push username/image:tag</code>"},{"location":"07_reproducibility_ii/#self-assessment","title":"Self Assessment","text":"A Docker container with the tagname <code>latest</code> ensures old code and data will work on a new computer setup? <p>Answer</p> <p>Never use the <code>latest</code> tag for a publication or archival. </p> <p>The <code>latest</code> version is always being updated and should be considered \"cutting edge\".</p> <p><code>latest</code> is the default tag name of all Docker images</p> <p><code>latest</code> versions MAY have backward compatibility with older code and data, but this is not always a given</p> When are containers the right solution? <p>Success</p> <p>Containers are valuable when you need to run an analyses on a remote platform, and you need it to work every time.</p> <p>Failure</p> <p>You need to do some simple text file editing </p> <p>You need to run a web service</p> True or False: Docker containers allow for reproducibility across all computing platforms <p>False</p> <p>While Docker allows you to quickly run software from other people, it may not work across every platform.</p> <p>There are different CPU architectures (<code>arm</code>, <code>amd64</code>, <code>x64,</code>x86`) deployed across cloud, computer workstations, laptops, and cellular phones. </p> <p>Docker containers and their software can be cross-compiled across architectures, but this must be done by the creators.</p> When is it advisable to not trust a Docker image? <p>When you cannot view its Dockerfile</p> <p>Featured and Verified Docker images can be trusted, in general.</p> <p>User generated images should not be trusted unless you can view their Dockerfile, or build logs to determine what is actually in the container you are attempting to run.</p>"},{"location":"07_reproducibility_ii/#additional-resources","title":"Additional Resources","text":"<p>Deeper Exploration of Containers and how to create them can be found here</p> <p>The Carpentries have an incubator workshop on Docker Containers</p>"},{"location":"08_reproducibility_III/","title":"Reproducibility III: Building Docker Containers","text":"<p>Learning Objectives</p> <p>After this lesson, you should be able to:</p> <ul> <li>Understand the Dockerfile structure and fields </li> <li>Build, execute and push your own Docker image</li> </ul> <p>In Reproducibility II we saw how we can access and execute Docker containers. In this lesson, we continue to explore containerization, covering how we can create our own container and the various commands necessary in order to create them.</p>"},{"location":"08_reproducibility_III/#images-vs-containers","title":"Images vs Containers","text":"<p>As mentioned before, there are differences between what an Image is and what Containers are. Here's a table that addresses some of these differences:</p> Image Container What A snapshot of an application  containing code, libraries, dependencides  and files needed for the application to run A runtime instance of the Docker image When Created through a Dockerfile Runtime executed after Image is created Why Reproducibility and consistency! Reproducibility and consistency! Where Stored in an online registry (e.g., Docker Hub) Executed on your machine <p>Once an Image is created, one can run as many Containers of it as required. Here's a Dockerfile-to-container diagram:</p> <pre><code>graph LR\n    A[Dockerfile] --&gt; |Local build| B{Image}:::colorclass;\n    B --&gt; |storage| C[Online Registry];\n    C ---&gt; |Image pull to local &lt;br&gt; and Execution| D[Container];\n    B ----&gt; |Local Execution| E[Container];\n    B ----&gt; |Local Execution| F[Container];\n    B ----&gt; |Local Execution| G[Container]\n    classDef colorclass fill:#f96</code></pre>"},{"location":"08_reproducibility_III/#dockerfiles-and-instructions","title":"Dockerfiles and Instructions","text":"<p><code>docker run</code> starts a container and executes the default \"entrypoint\", or any other \"command\" that follows <code>run</code> and any optional flags. These commands are specified within a Dockerfile. </p> <p>What is a Dockerfile?</p> <p>A Dockerfile is a text file that contains a list of commands, known as instructions used by Docker to  build an image. </p> <p>These commands can include specifying the base image to use, copying files into the image, setting environment variables, running commands, and defining entry points and default commands to run when a container is started from the image. The Dockerfile is processed by the docker build command, which creates a Docker image that can be used to run containers.</p> <pre><code>FROM pdal/pdal:latest                   # Tells the Dockerfile which image to pull from\n                                        # \nWORKDIR /app                            # Sets the initial working directory\n                                        #\nCOPY pdal_copc.sh /app/pdal_copc.sh     # Copies a certain file from your directory to the container\n                                        #\nCOPY copc.json /app/copc.json           # Copies a certain file from your directory to the container\n                                        #\nRUN chmod +x pdal_copc.sh               # Runs a specific command (in this case, adds specific permissions)\n                                        #\nENTRYPOINT [\"/app/pdal_copc.sh\"]        # Sets the first command activating the container\n</code></pre> What is an entrypoint? <p>An entrypoint is the initial command(s) executed upon starting the Docker container. It is listed in the <code>Dockerfile</code> as <code>ENTRYPOINT</code> and can take 2 forms: as commands followed by parameters (<code>ENTRYPOINT command param1 param2</code>)  or as an executable (<code>ENTRYPOINT [\u201cexecutable\u201d, \u201cparam1\u201d, \u201cparam2\u201d]</code>)</p>"},{"location":"08_reproducibility_III/#building-docker-images","title":"Building Docker Images","text":"<p>Now that we are relatively comfortable with running Docker, we can look at some advanced Docker topics, such as:</p> <ul> <li>Building our own Docker images from the <code>Dockerfile</code></li> <li>Modify an existing Dockerfile and create a new image</li> <li>Push an image to a Registry</li> </ul>"},{"location":"08_reproducibility_III/#requirements","title":"Requirements","text":"<p>Clone our example repository with pre-written Dockerfiles From your CodeSpace, we are going to copy a second GitHub repository onto our VM. If you are working locally, make sure that you change directories away from any other Git repository that you may have been working in.</p> <pre><code>$ cd /workspaces\n\n$ git clone https://github.com/cyverse-education/intro2docker\n\n$ cd intro2docker/\n</code></pre>"},{"location":"08_reproducibility_III/#writing-a-dockerfile","title":"Writing a Dockerfile","text":"<p>Important</p> <p><code>Dockerfile</code> must be capitalized. It does not have a file extension.</p> <p>Create a file called <code>Dockerfile</code>, and add content to it as described below, e.g.</p> <pre><code>$ touch Dockerfile\n</code></pre> <p>Formatting in the <code>Dockerfile</code></p> <p>We use a code line escape character <code>\\</code> to allow single line scripts to be written on multiple lines in the Dockerfile.</p> <p>We also use the double characters <code>&amp;&amp;</code> which essentially mean \u201cif true, then do this\u201d while executing the code. The <code>&amp;&amp;</code> can come at the beginning of a line or the end when used with <code>\\</code>.</p> <p>The <code>Dockerfile</code> contains Instructions: a series of commands that Docker executes during the creation and execution of a container.</p>"},{"location":"08_reproducibility_III/#arg","title":"ARG","text":"<p>The only command that can come before a <code>FROM</code> statement is <code>ARG</code></p> <p><code>ARG</code> can be used to set arguments for later in the build, e.g.,</p> <pre><code>ARG VERSION=latest\n\nFROM ubuntu:$VERSION\n</code></pre>"},{"location":"08_reproducibility_III/#from","title":"FROM","text":"<p>A valid <code>Dockerfile</code> must start with a <code>FROM</code> statement which initializes a new build stage and sets the base image for subsequent layers.</p> <p>We\u2019ll start by specifying our base image, using the FROM statement</p> <pre><code>FROM ubuntu:latest\n</code></pre> <p>If you are building on an <code>arm64</code> or Windows system you can also give the optional <code>--platform</code> flag, e.g.,</p> <pre><code>FROM --platform=linux/amd64 ubuntu:latest\n</code></pre> When to use a multi-stage build pattern? <p>Docker has the ability to build container images from one image, and run that \"builder\" image from a second \"base\" image, in what is called a \"builder pattern\".</p> <p>Build patterns are useful if you're compiling code from (proprietary) source code and only want to feature the binary code as an executed function in the container at run time. </p> <p>Build patterns can greatly reduce the size of your container.</p> <p>You can use multiple <code>FROM</code> commands as build stages. The <code>AS</code> statement follows the <code>image:tag</code> as a psuedo argument. </p> <pre><code># build stage\nFROM golang:latest AS build-env\nWORKDIR /go/src/app\nADD . /go/src/app\nRUN go mod init\nRUN cd /go/src/app &amp;&amp; go build -o hello\n\n# final stage\nFROM alpine:latest\nWORKDIR /app\nCOPY --from=build-env /go/src/app /app/\nENTRYPOINT ./hello\n</code></pre>"},{"location":"08_reproducibility_III/#label","title":"LABEL","text":"<p>You can create labels which are then tagged as  JSON metadata to the image</p> <pre><code>LABEL author=\"your-name\" \nLABEL email=\"your@email-address\"\nLABEL version=\"v1.0\"\nLABEL description=\"This is your first Dockerfile\"\nLABEL date_created=\"2022-05-13\"\n</code></pre> <p>You can also add labels to a container when it is run:</p> <pre><code>$ docker run --label description=\"this label came later\" ubuntu:latest\n\n$ docker ps -a\n\n$ docker inspect ###\n</code></pre>"},{"location":"08_reproducibility_III/#run","title":"RUN","text":"<p>Different than the <code>docker run</code> command is the <code>RUN</code> build function. <code>RUN</code> is used to create new layers atop the \"base image\"</p> <p>Here, we are going to install some games and programs into our base image:</p> <pre><code>RUN apt-get update &amp;&amp; apt-get install -y fortune cowsay lolcat\n</code></pre> <p>Here we've installed <code>fortune</code> <code>cowsay</code> and <code>lolcat</code> as new programs into our base image.</p> <p>Best practices for building new layers</p> <p>Ever time you use the <code>RUN</code> command it is a good idea to use the <code>apt-get update</code> or <code>apt update</code> command to make sure your layer is up-to-date. This can become a problem though if you have a very large container with a large number of <code>RUN</code> layers. </p>"},{"location":"08_reproducibility_III/#env","title":"ENV","text":"<p>In our new container, we need to change and update some of the environment flags. We can do this using the <code>ENV</code> command</p> <pre><code>ENV PATH=/usr/games:${PATH}\n\nENV LC_ALL=C\n</code></pre> <p>Here we are adding the <code>/usr/games</code> directory to the <code>PATH</code> so that when we run the new container it will find our newly installed game commands</p> <p>We are also updating the \"locales\" to set the language of the container.</p>"},{"location":"08_reproducibility_III/#copy","title":"COPY","text":"<p>The <code>COPY</code> command will copy files from the directory where <code>Dockerfile</code> is kept into the new image. You must specify where to copy the files or directories</p> <pre><code>COPY . /app\n</code></pre> When to use <code>COPY</code> vs <code>ADD</code> <p><code>COPY</code> is more basic and is good for files</p> <p><code>ADD</code> has some extra features like <code>.tar</code> extraction and URL support</p>"},{"location":"08_reproducibility_III/#cmd","title":"CMD","text":"<p>The <code>CMD</code> command is used to run software in your image. In general use the [\"command\"] syntax:</p> <pre><code>CMD [\"executable\", \"parameter1\", \"parameter2\"]\n</code></pre>"},{"location":"08_reproducibility_III/#entrypoint","title":"ENTRYPOINT","text":"<p>ENTRYPOINT works similarly to <code>CMD</code> but is designed to allow you to run your container as an executable.</p> <pre><code>ENTRYPOINT fortune | cowsay | lolcat\n</code></pre> <p>The default <code>ENTRYPOINT</code> of most images is <code>/bin/sh -c</code> which executes a <code>shell</code> command.</p> <p><code>ENTRYPOINT</code> supports both the <code>ENTRYPOINT [\"command\"]</code> syntax and the <code>ENTRYPOINT command</code> syntax</p> What is the difference in the <code>ENTRYPOINT</code> and <code>CMD</code> <p>The CMD instruction is used to define what is execute when the container is run.</p> <p>The ENTRYPOINT instruction cannot be overridden, instead it is appended to when a new command is given to the <code>docker run container:tag new-cmd</code> statement </p> <p>the executable is defined with ENTRYPOINT, while CMD specifies the default parameter</p>"},{"location":"08_reproducibility_III/#user","title":"USER","text":"<p>Most containers are run as <code>root</code> meaning that they have super-user privileges within themselves</p> <p>Typically, a new user is necessary in a container that is used interactively or may be run on a remote system.</p> <p>During the build of the container, you can create a new user with the <code>adduser</code> command and set up a <code>/home/</code> directory for them. This new user would have something like 1000:1000 <code>uid:gid</code> permissions without <code>sudo</code> privileges.</p> <p>As a last step, the container is run as the new <code>USER</code>, e.g., </p> <pre><code>ARG VERSION=18.04\n\nFROM ubuntu:$VERSION\n\nRUN useradd ubuntu &amp;&amp; \\\n    chown -R ubuntu:ubuntu /home/ubuntu\n\nUSER ubuntu\n</code></pre>"},{"location":"08_reproducibility_III/#expose","title":"EXPOSE","text":"<p>You can open ports using the <code>EXPOSE</code> command.</p> <pre><code>EXPOSE 8888\n</code></pre> <p>The above command will expose port 8888.</p> <p>Note</p> <p>Running multiple containers using the same port is not trivial and would require the usage of a web server such as NGINX. However, you can have multiple containers interact with each other using Docker Compose.</p>"},{"location":"08_reproducibility_III/#pushing-to-a-registry-with-docker-push","title":"Pushing to a Registry with  docker push","text":"<p>By default <code>docker push</code> will upload your local container image to the Docker Hub.</p> <p>Also, make sure that your container has the appropriate tag.</p> <p>First, make sure to log into the Docker Hub, this will allow you to download private limages, to upload private/public images:</p> <pre><code>docker login\n</code></pre> <p>Alternately, you can link GitHub / GitLab accounts to the Docker Hub.</p> <p>To push the image to the Docker Hub:</p> <pre><code>docker push username/imagename:tag \n</code></pre> <p>or, to a private registry, here we push to CyVerse private <code>harbor.cyverse.org</code> registry which uses \"project\" sub folders:</p> <pre><code>docker push harbor.cyverse.org/project/imagename:newtag \n</code></pre>"},{"location":"08_reproducibility_III/#summary-of-instructions","title":"Summary of Instructions","text":"Instruction Command Description <code>ARG</code> Sets environmental variables during image building <code>FROM</code> Instructs to use a specific Docker image <code>LABEL</code> Adds metadata to the image <code>RUN</code> Executes a specific command <code>ENV</code> Sets environmental variables <code>COPY</code> Copies a file from a specified location to the image <code>CMD</code> Sets a command to be executed when running a container <code>ENTRYPOINT</code> Configures and run a container as an executable <code>USER</code> Used to set User specific information <code>EXPOSE</code> exposes a specific port"},{"location":"08_reproducibility_III/#managing-data-in-docker","title":"Managing Data in Docker","text":"<p>It is possible to store data within the writable layer of a container, but there are some limitations:</p> <ul> <li>The data doesn\u2019t persist when that container is no longer running, and it can be difficult to get the data out of the container if another process needs it.</li> <li>A container\u2019s writable layer is tightly coupled to the host machine where the container is running. You can\u2019t easily move the data somewhere else.</li> <li>Its better to put your data into the container AFTER it is built - this keeps the container size smaller and easier to move across networks.</li> </ul> <p>Docker offers three different ways to mount data into a container from the Docker host:</p> <ul> <li>Volumes</li> <li>tmpfs mounts</li> <li>Bind mounts</li> </ul> <p></p> <p>When in doubt, volumes are almost always the right choice.</p>"},{"location":"08_reproducibility_III/#volumes","title":"Volumes","text":"<p>Volumes are often a better choice than persisting data in a container\u2019s writable layer, because using a volume does not increase the size of containers using it, and the volume\u2019s contents exist outside the lifecycle of a given container. While bind mounts (which we will see in the Advanced portion of the Camp) are dependent on the directory structure of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts:</p> <ul> <li>Volumes are easier to back up or migrate than bind mounts.</li> <li>You can manage volumes using Docker CLI commands or the Docker API.</li> <li>Volumes work on both UNIX and Windows containers.</li> <li>Volumes can be more safely shared among multiple containers.</li> <li>A new volume\u2019s contents can be pre-populated by a container.</li> </ul> When Should I Use the Temporary File System mount? <p>If your container generates non-persistent state data, consider using a <code>tmpfs</code> mount to avoid storing the data anywhere permanently, and to increase the container\u2019s performance by avoiding writing into the container\u2019s writable layer. The data is written to the host's memory instead of a volume; When the container stops, the <code>tmpfs</code> mount is removed, and files written there will not be kept.</p> <p>Choose the <code>-v</code> flag for mounting volumes</p> <p><code>-v</code> or <code>--volume</code>: Consists of three fields, separated by colon characters (:). </p> <p>The fields must be in the correct order, and the meaning of each field is not immediately obvious.</p> <ul> <li>The first field is the path on your local machine that where the data are.</li> <li>The second field is the path where the file or directory are mounted in the container.</li> <li>The third field is optional, and is a comma-separated list of options, such as <code>ro</code> (read only).</li> </ul> <pre><code>-v /home/username/your_data_folder:/container_folder\n</code></pre> <pre><code>$ docker run -v /home/$USER/read_cleanup:/work alpine:latest ls -l /work\n</code></pre> <p>So what if we wanted to work interactively inside the container?</p> <pre><code>$ docker run -it -v /home/$USER/read_cleanup:/work alpine:latest sh\n</code></pre> <pre><code>$ ls -l \n$ ls -l work\n</code></pre> <p>Once you're in the container, you will see that the <code>/work</code> directory is mounted in the working directory.</p> <p>Any data that you add to that folder outside the container will appear INSIDE the container. And any work you do inside the container saved in that folder will be saved OUTSIDE the container as well.</p>"},{"location":"09_reproducibility_IV/","title":"Containers and the HPC","text":"<p>  Singularity &gt; Apptainer and working with the SLURM workload manager (HPC scheduler). </p> <p>Expected Outcomes</p> <ul> <li>Being able to log onto the HPC and start an interactive node</li> <li>Know basic HPC related commands</li> <li>Execute containerized code through Singularity/Apptainer</li> <li>Being able to search and load specific software</li> <li>Execute NextFlow scripts on the HPC</li> </ul> <p>Through FOSS and FOSS+ we have learned about container technology and how it can affect reproducibility by wrapping all the necessary components that allow a specific software to be executed. </p> <p>Similar to Docker, Singularity/Apptainer is a powerful tool that enables researchers to package entire environments, including software dependencies and libraries, into a single executable file.</p> <p>Unlike other containerization platforms, Singularity/Apptainer is designed with HPC in mind, allowing seamless integration with cluster computing environments.</p> <p>The biggest difference between Docker and Singularity/Apptainer, is that with the latter you do not require sudo priviledges. Exciting!</p> <p>In this workshop, we are going to learn how we can use Singularity/Apptainer on the UA HPC, covering orientation of the HPC, and executing Singularity/Apptainer control commands.</p>"},{"location":"09_reproducibility_IV/#a-10000ft-view-of-the-hpc","title":"A 10,000ft View of the HPC","text":"<p>Next week (Apr 04th) Chris Reidy from UITS is going to talk in more details regarding the hardware specifications of the UA HPC systems. </p> <p>Here, we are going to concentrate on HOW to operate the HPC system as a general user.</p> <p>Who is this lesson for?</p> <p>This workshop is  primarely aimed to UA students, grad students, faculty and staff as being part of the University of Arizona grants you access to the UA HPC. </p> <p>If you are not part of UA... you're still very welcome to take part of this lesson! You may not be able to execute the commands but you can still walk out of this with a good understanding of your institution's own HPC and Singularity/Apptainer. Everyone's welcome!</p>"},{"location":"09_reproducibility_IV/#logging-onto-the-hpc","title":"Logging onto the HPC","text":"<p>If you have a UA account, to connect to the HPC you need to use <code>ssh</code> (Secure Shell). Open a terminal, and type:</p> <pre><code>ssh &lt;UA username&gt;@hpc.arizona.edu\n</code></pre> <p>Type your UA password and if successful you'll be greeted with a two-factor login. Select which choice, and complete the authentification. Once you are past the authentification steps, you will enter the Bastion server. This step has 2 purposes: </p> <ol> <li>Protect from attacks.</li> <li>Select what HPC system you want to use.</li> </ol> <p>Note: the Bastion server is NOT YET the HPC! Here you cannot submit jobs or run analyes. Type <code>shell</code> in order to select what system you want to use.</p> <p>The whole process (from logging to selecting the system) looks like the following:</p> <pre><code>ssh cosi@hpc.arizona.edu\n(cosi@hpc.arizona.edu) Password: \n(cosi@hpc.arizona.edu) Duo two-factor login for cosi\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-8418\n 2. SMS passcodes to XXX-XXX-8418\n\nPasscode or option (1-2): 1\nSuccess. Logging you in...\nLast login: Tue Mar 26 14:52:39 2024 from dhcp-10-132-212-1.uawifi.arizona.edu\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n-----------------------------------------\n\n[cosi@gatekeeper ~]$ shell\nLast login: Wed Mar 20 10:30:25 2024 from gatekeeper.hpc.arizona.edu\n***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nPuma:\n$ puma\n(puma) $\nOcelote:\n$ ocelote\n(ocelote) $\nElGato:\n$ elgato\n(elgato) $\n-----------------------------------------\n\n[cosi@wentletrap ~]$ ocelote\n(ocelote) [cosi@wentletrap ~]$\n</code></pre> <p>At this point you are in the Login Node, where you can submit jobs or ask for an interactive node.</p> <p>  A representation of what the HPC structure and its various nodes. Source. </p>"},{"location":"09_reproducibility_IV/#choosing-a-system","title":"Choosing a System","text":"<p>In the example above, we chose the Ocelote system. Notice how there are 2 other choices: Puma and El Gato. Without going in  too much detail, here are some of the statistics regarding the 3 HPC systems at the UA HPC.</p> System Year of Aquisition Processors RAM GPU Puma 2020 2x AMD Zen2 48 CPU (94 cores total) 512GB 6x Nvidia V100S Ocelote 2016 2x Xeon E5-2695v3 14-core (28 cores total) 192GB 46x Nvidia P100 El Gato 2013 2x Xeon E5-2650v2 8-core (16 core total) 64GB removed as obsolete <p>Find the full systems specs at the official UA HPC documentatio resources page.</p> <p>El Gato is the oldest system, and potentially not useful for heavy research. Puma is the newest and most requested, whislt Ocelote is the \"middle child\": not as popular but still able to pack a punch. </p> <p>Depending on what your work is, your best bet would be Puma for heavy computation (if you are ok with waiting long queues); However, if your jobs aren't as taxing, then Ocelote could easily be a safe choice.</p> <p>For this workshop, we are going to be using Ocelote.</p>"},{"location":"09_reproducibility_IV/#checking-available-resources","title":"Checking Available Resources","text":""},{"location":"09_reproducibility_IV/#allocations","title":"Allocations","text":"<p>Once past the Bastion server and logged into the Ocelote login node, you are able to submit jobs or request an interactive node. Before you do so, it is wise to check your available resources. These resources are the ones made available to you by your PI or working group.</p> <p>In order for you to check your resources, type <code>va</code>.</p> <pre><code>(ocelote) [cosi@wentletrap ~]$ va\nWindfall: Unlimited\n\nPI: parent_1743 Total time: 100000:00:00\n    Total used*: 1:00:00\n    Total encumbered: 0:00:00\n    Total remaining: 99999:00:00\n    Group: datalab Time used: 1:00:00 Time encumbered: 0:00:00\n\n\n*Usage includes all subgroups, some of which may not be displayed here\n</code></pre> <p><code>va</code> allows you to view all of the resources available from all the groups you are part of.</p>"},{"location":"09_reproducibility_IV/#storage","title":"Storage","text":"<p>There are a number of ways one can approach storage on the HPC:</p> <ul> <li>Your own folder (in <code>/home/</code>): 50GB limit</li> <li>Your group (in <code>/groups/</code>): 500GB limit</li> <li>Your PI research (in <code>/xdisk/</code>): 20TB</li> </ul> <p>Four the purpose of this workshop, we can access the datalab group storage in <code>/groups/cosi</code> (we will try and rename it in the future).</p>"},{"location":"09_reproducibility_IV/#queues-and-submissions","title":"Queues and Submissions","text":"<p>One can check queue times and sumbissions by executing the SLURM command <code>squeue</code>. This will display the job id, submission type (standard, windfall, high priority), name of submission, user, status (queued, running), time elapsed, number of used nodes, and nodelist.</p> <pre><code>(ocelote) [cosi@wentletrap ~]$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n2875825_[432-1000]  standard    qchem snanayak PD       0:00      1 (AssocGrpCPUMinutesLimit)\n           2890485  standard R92L_Met krishnag PD       0:00      1 (Dependency)\n           2890484  standard R92L_Met krishnag PD       0:00      1 (Dependency)\n           2890483  standard R92L_Met krishnag PD       0:00      1 (Dependency)\n           2881573  standard R92W_Met krishnag PD       0:00      1 (Dependency)\n           2881572  standard R92W_Met krishnag PD       0:00      1 (Dependency)\n           2802511  standard    eigen krishnag PD       0:00      1 (Dependency)\n           2949224  standard brutefor theronir PD       0:00      1 (None)\n           2898419  standard start_so mmartino PD       0:00      1 (Dependency)\n           2898418  standard make_sor mmartino PD       0:00      1 (Dependency)\n           2898416  standard start_so mmartino PD       0:00      1 (Dependency)\n           2898415  standard make_sor mmartino PD       0:00      1 (Dependency)\n           2898410  standard start_so mmartino PD       0:00      1 (Dependency)\n           2898409  standard make_sor mmartino PD       0:00      1 (Dependency)\n              .         .        .        .     .       .         .   .\n              .         .        .        .     .       .         .   .\n              .         .        .        .     .       .         .   .\n           2884142  windfall li_d_t_6    bubin  R 2-06:58:50      1 i5n14\n           2884107  windfall li_d_t_4    bubin  R 2-07:00:26      1 i6n8\n           2884098  windfall li_d_t_7    bubin  R 2-07:00:50      1 i6n5\n           2880486  windfall   be_10b   teodar  R 4-22:35:44      1 i16n5\n           2880487  windfall    be_9b   teodar  R 4-22:35:44      1 i16n6\n           2880488  windfall    be_7b   teodar  R 4-22:35:44      1 i16n12\n           2880489  windfall    be_2b   teodar  R 4-22:35:44      1 i16n16\n</code></pre> <p>Likely, this will output 100s of lines, therefore if you want to check on your own job, you could use the CLI and <code>grep</code> to select the running submission (e.g., <code>squeue | grep &lt;username&gt;</code> or <code>squeue --user $NETID</code>).</p>"},{"location":"09_reproducibility_IV/#hpc-slurm-and-jobs-submissions","title":"HPC, SLURM and Jobs Submissions","text":"SLURM not Slurm.  <p>All of the UA HPC systems run on a workload manager and job scheduler named SLURM (Simple Linux Utility for Resource Management). It's designed to manage and schedule computing resources such as CPUs, GPUs, memory, and storage across a cluster of interconnected nodes.</p> <p>You can learn more on SLURM and HPC system commands here. </p>"},{"location":"09_reproducibility_IV/#job-submissions","title":"Job Submissions","text":"<p>There are 2 ways one can submit jobs onto the HPC system. The first is to run a batch job, which is the more popular submission type, whilst the other is by requesting an interactive node.</p>"},{"location":"09_reproducibility_IV/#batch-jobs","title":"Batch jobs","text":"<p>As we are not going to be using batch submissions, we are not going to be going into too much detail. However, here is what you need to know. For more details on running batch jobs, visit the official documentation page on batch jobs.</p> <p>Writing a Batch Script</p> <p>Batch scripts require a number of job directives. These are similar to the Dockerfile instructions, but instead of telling Docker how to build the image, these instead tell the SLURM system what to do with the job. The essential directives are the following:</p> Directive Purpose <code>#SBATCH --account=group_name</code> Specify the account where hours are charged. <code>#SBATCH --partition=partition_name</code> Set the job partition. This determines your job's priority and the hours charged. <code>#SBATCH --time=DD-HH:MM:SS</code> Set the job's runtime limit in days, hours, minutes, and seconds. A single job cannot exceed 10 days or 240 hours. <code>#SBATCH --nodes=N</code> Allocate N nodes to your job. <code>#SBATCH --cpus-per-task=M</code>  and  <code>#SBATCH --ntasks=N</code> ntasks specifies the number of tasks (or processes) the job will run. By default, you will be allocated one CPU/task. This can be increased by including the additional directive --cpus-per-task. <code>#SBATCH --mem=Ngb</code> Select N gb of memory per node. If \"gb\" is not included, this value defaults to MB. <p>After setting your directives, you can instruct the HPC to do what you require similar to a bash script.</p> <p>Here's an example of a batch job:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=blast_job          # Job name\n#SBATCH --partition=standard          # Sets the job priority to standard\n#SBATCH --nodes=1                     # Number of nodes\n#SBATCH --ntasks=1                    # Number of tasks (processes) per node\n#SBATCH --cpus-per-task=4             # Number of CPU cores per task\n#SBATCH --mem=8G                      # Memory per node (in this case, 8GB)\n#SBATCH --time=02:00:00               # Time limit (HH:MM:SS)\n\n# Load necessary modules\nmodule load blast/2.12.0              # Load BLAST module (adjust version as needed)\n\n# Change to the directory where the job will run\ncd $SLURM_SUBMIT_DIR\n\n# Define input and output files\nquery_file=\"query.fasta\"              # Input query file (FASTA format)\ndatabase_file=\"database.fasta\"        # BLAST database file (FASTA format)\noutput_file=\"blast_results.out\"       # Output file for BLAST results\n\n# Run BLAST command\nblastp -query $query_file -db $database_file -out $output_file -evalue 0.001 -num_threads $SLURM_CPUS_PER_TASK\n</code></pre> <p>Submitting a Batch Script</p> <ul> <li>To submit jobs you need to use <code>sbatch</code>, such as <code>sbatch script.slurm</code></li> <li>To cancel your job you do <code>scancel</code>, such as <code>scancel $JOBID</code> or <code>scancel -u $NETID</code></li> </ul> <p>This will submit your job to the queue. Execution will depend on your submission type (partition).</p>"},{"location":"09_reproducibility_IV/#launching-an-interactive-node","title":"Launching an Interactive Node","text":"<p>An interactive node, unlike batch jobs which are run asynchronously, allows immediate access to compute. Similar to batch jobs, interactive nodes are submitted to the queue, but once available, you will receive a prompt for a node with the selected resources. Read more on how to launch interactive jobs in the official documentation.</p> <p>The Quick and Dirty</p> <p>Don't need a lot of resources and just want access to the compute?</p> <p>Just type <code>interactive</code>. </p> <p>Disclaimer: you may require to wait longer as your job is going to fall in the <code>windfall</code> queue.</p> <p>Following are a list of useful flags (options) for setting up the interactive node.</p> Flag Default value Description Example <code>-n</code> 1 Number of CPUs requested per node interactive -n 8 <code>-m</code> 4GB Memory per CPU interactive -m 5GB <code>-a</code> none Account (group) to charge interactive -a datalab <code>--partition=</code> windfall Partition to determine CPU time charges and is set to windfall when no account is specified, and is set to standard when an account is provided. interactive --partition=windfall <code>-t</code> 01:00:00 Time allocated to session. interactive -t 08:00:00 <code>-N</code> 1 Number of nodes. There is no reason to exceed 1 node unless the number of CPUs requested is greater than the number of CPUs per node on a given cluster. <p>An example for an interactive node is:</p> <pre><code>interactive -n 8 -m 16GB -a datalab -t 02:00:00 \n</code></pre> <p>The above example will request an interactive node with 8 cores, 16GB RAM, \"charging\" the datalab, running for 2 hours. Try it!  </p> <p>Modules</p> <p>There are 100s of tools installed on the HPC, few of which are available on the login screen. These tools are available only during a batch job submission or within interactive jobs.</p> <p>To see what tools are already running, or which are available, you will need to use the <code>module</code> command.</p> <p>Helpful <code>module</code> commands</p> Command Purpose <code>module list</code> Lists loaded modules <code>module avail</code> Lists available modules <code>module spider</code> Lists ALL modules <code>module load</code> Loads a module <code>module help</code> Help command!"},{"location":"09_reproducibility_IV/#singularityapptainer","title":"Singularity/Apptainer","text":"<p>In 2021, the Sylabs, the developers behind the original Singularity, made a fork of the original project and renamed it SingularityCE (Community Edition). This would allow for the SingularityCE to be compliat with FOSS and allowing for the community to contribute to the builds. The Singularity team then joined the Linux Foundation and decided to rename their effor to Apptainer. </p> <p>The technology behind Singularity/Apptainer is similar to the one of Docker, but, as mentioned before, it was created with the HPC in mind, and therefore bypasses the requirement of sudo.</p> <p>Note</p> <p>Until now we have used Singularity/Apptainer to refer to the same software. Onwards, you can decide whether to use Singularity OR Apptainer; in order to keep up with the latest release, we are going to be executing apptainer commands. </p> Docker vs SingularityCE &amp; Apptainer in the blink of an eye <p> Apptainer and SingularityCE are 100% compatible with Docker but they do have some distinct differences</p> <p> Docker</p> <p> Docker containers run as <code>root</code></p> <ul> <li>This privilege is almost never supported by administrators of High Performance Computing (HPC) centers. Meaning Docker is not, and will likely never be, installed natively on your HPC cluster.</li> </ul> <p> uses compressed layers to create one image</p> <p> SingularityCE &amp; Apptainer:</p> <p>  Same user and group identity inside as outside the container</p> <p>  User only has <code>root</code> privileges if elevated with <code>sudo</code> when the container is run</p> <p>  Can run and modify any existing Docker image</p> <ul> <li>These key differences allow Singularity to be installed on most HPC centers. Because you can run virtually all Docker containers in Singularity, you can effectively run Docker on an HPC. </li> </ul>"},{"location":"09_reproducibility_IV/#general-executable-commands","title":"General Executable Commands","text":"<p>Resources:</p> <ul> <li>https://cc.cyverse.org/singularity/intro/</li> <li>https://cc.cyverse.org/singularity/hpc/</li> <li>https://cc.cyverse.org/singularity/advanced/</li> </ul> <p>Apptainer\u2019s command line interface allows you to build and interact with containers transparently. You can run programs inside a container as if they were running on your host system. You can easily redirect IO, use pipes, pass arguments, and access files, sockets, and ports on the host system from within a container.</p>"},{"location":"09_reproducibility_IV/#help","title":"help","text":"<p>The <code>help</code> command gives an overview of Apptainer options and subcommands as follows:</p> <pre><code> $ apptainer help pull\nPull an image from a URI\n\nUsage:\n  apptainer pull [pull options...] [output file] &lt;URI&gt;\n\nDescription:\n  The 'pull' command allows you to download or build a container from a given\n  URI. Supported URIs include:\n\n  library: Pull an image from the currently configured library\n      library://user/collection/container[:tag]\n\n  docker: Pull a Docker/OCI image from Docker Hub, or another OCI registry.\n      docker://user/image:tag\n\n  shub: Pull an image from Singularity Hub\n      shub://user/image:tag\n\n  oras: Pull a SIF image from an OCI registry that supports ORAS.\n      oras://registry/namespace/image:tag\n\n  http, https: Pull an image using the http(s?) protocol\n      https://example.com/alpine.sif\n\nOptions:\n      --arch string           architecture to pull from library (default\n                              \"amd64\")\n      --arch-variant string   architecture variant to pull from library\n      --dir string            download images to the specific directory\n      --disable-cache         do not use or create cached images/blobs\n      --docker-host string    specify a custom Docker daemon host\n      --docker-login          login to a Docker Repository interactively\n  -F, --force                 overwrite an image file if it exists\n  -h, --help                  help for pull\n      --library string        download images from the provided library\n      --no-cleanup            do NOT clean up bundle after failed build,\n                              can be helpful for debugging\n      --no-https              use http instead of https for docker://\n                              oras:// and library://&lt;hostname&gt;/... URIs\n\n\nExamples:\n  From a library\n  $ apptainer pull alpine.sif library://alpine:latest\n\n  From Docker\n  $ apptainer pull tensorflow.sif docker://tensorflow/tensorflow:latest\n  $ apptainer pull --arch arm --arch-variant 6 alpine.sif docker://alpine:latest\n\n  From Shub\n  $ apptainer pull apptainer-images.sif shub://vsoch/apptainer-images\n\n  From supporting OCI registry (e.g. Azure Container Registry)\n  $ apptainer pull image.sif oras://&lt;username&gt;.azurecr.io/namespace/image:tag\n\n\nFor additional help or support, please visit https://apptainer.org/help/\n</code></pre>"},{"location":"09_reproducibility_IV/#search","title":"search","text":"<p>Just like with Docker, you can <code>search</code> the Apptainer container registries for images.</p> <pre><code>$ apptainer search tensorflow\n</code></pre>"},{"location":"09_reproducibility_IV/#pull","title":"pull","text":"<p>The easiest way to use a Apptainer is to <code>pull</code> an existing container from one of the Registries.</p> <pre><code>$ apptainer pull library://lolcow\n</code></pre> <p>Not only you can pull fromt the Apptainer registries/libraries, but you can pull from Docker.</p> <pre><code>$ apptainer pull docker://alpine\n</code></pre> <p>In my humble opinion...</p> <p>This is whre Apptainer shines: you can pull from Docker and run Docker built images on the HPC! These are automatically converted to Apptainer images (<code>.sif</code>) and executable on the HPC!</p> <p>... so where are the Apptainer <code>.sif</code> images stored?</p> <p>Right where in the directory you are pulling them to. Check with <code>cd</code>!</p>"},{"location":"09_reproducibility_IV/#obtaining-images","title":"Obtaining Images","text":"<p>As metioned earlier, you can use the <code>pull</code> command to download pre-built images from a number of Container Registries, here we'll be focusing on the DockerHub.</p> <p>Container Registries:</p> <ul> <li><code>library://</code> - images hosted on Sylabs Cloud</li> <li><code>docker://</code> - images hosted on Docker Hub</li> <li><code>localimage://</code> - images saved on your machine</li> <li><code>yum://</code> - yum based systems such as CentOS and Scientific Linux</li> <li><code>debootstrap://</code> - apt based systems such as Debian and Ubuntu</li> <li><code>arch://</code> - Arch Linux</li> <li><code>busybox://</code> - BusyBox</li> <li><code>zypper://</code> - zypper based systems such as Suse and OpenSuse</li> <li><code>shub://</code> - (archived) images hosted on Singularity Hub, no longer maintained</li> </ul>"},{"location":"09_reproducibility_IV/#pulling-an-image-from-singularity-hub","title":"Pulling an image from Singularity Hub","text":"<p>Similar to previous example, in this example I am pulling a base Ubuntu container from Singularity-Hub:</p> <pre><code>$ apptainer pull shub://singularityhub/ubuntu\nINFO:    Downloading shub image\n88.6MiB / 88.6MiB [=============================================================================] 100 % 39.1 MiB/s 0s\n</code></pre> <p>Re/naming</p> <p>You can give the  the container using the <code>--name</code> flag: such as <code>apptainer pull --name my-own-ubuntu-pulled-image.sif shub://singularityhub/ubuntu</code></p>"},{"location":"09_reproducibility_IV/#pulling-an-image-from-docker-hub","title":"Pulling an image from Docker Hub","text":"<p>This example pulls an <code>ubuntu:22.04</code> image from DockerHub and saves it to the working directory.</p> <pre><code>$ apptainer pull docker://ubuntu:22.04\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob bccd10f490ab done\nCopying config ca2b0f2696 done\nWriting manifest to image destination\nStoring signatures\n2024/03/27 20:14:50  info unpack layer: sha256:bccd10f490ab0f3fba61b193d1b80af91b17ca9bdca9768a16ed05ce16552fcb\nINFO:    Creating SIF file...\n</code></pre>"},{"location":"09_reproducibility_IV/#interacting-with-images","title":"Interacting with Images","text":"<p>You can interact with images in several ways such as <code>run</code>, <code>shell</code> and <code>exec</code>.</p> <p>For these examples we will use a <code>cowsay_latest.sif</code> image that can be pulled from the Docker Hub.</p> <pre><code>$ apptainer pull docker://tswetnam/cowsay\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 05e030abce7b done\nCopying blob b4624b3efe06 done\nCopying blob 6cf436f81810 done\nCopying blob 987088a85b96 done\nCopying blob d42beb8ded59 done\nCopying config ee9e20351a done\nWriting manifest to image destination\nStoring signatures\n2024/03/27 20:16:29  info unpack layer: sha256:6cf436f81810f067c6d4ffca6793eae7cb6d38456715b0707d8a5a2d1acccf12\n2024/03/27 20:16:29  warn rootless{dev/full} creating empty file in place of device 1:7\n2024/03/27 20:16:29  warn rootless{dev/null} creating empty file in place of device 1:3\n2024/03/27 20:16:29  warn rootless{dev/ptmx} creating empty file in place of device 5:2\n2024/03/27 20:16:29  warn rootless{dev/random} creating empty file in place of device 1:8\n2024/03/27 20:16:29  warn rootless{dev/tty} creating empty file in place of device 5:0\n2024/03/27 20:16:29  warn rootless{dev/urandom} creating empty file in place of device 1:9\n2024/03/27 20:16:29  warn rootless{dev/zero} creating empty file in place of device 1:5\n2024/03/27 20:16:30  info unpack layer: sha256:987088a85b9606eb474a365eb210db765ff0d011ee099a6e3de5087435c6f966\n2024/03/27 20:16:30  info unpack layer: sha256:b4624b3efe0617e59ed3998407eafdbe1cb6451346a6cabd066b6e253f50efb1\n2024/03/27 20:16:30  info unpack layer: sha256:d42beb8ded595df5627ad4ef31bf528a6fdbfbd11d82f9023152738d6b05a7fa\n2024/03/27 20:16:30  info unpack layer: sha256:05e030abce7b562606031bcc54646a868984685f4c89c7c354f34f1f6e502917\nINFO:    Creating SIF file..\n\n$ ls\nalpine_latest.sif  lolcow_latest.sif  ubuntu_22.04.sif  cowsay_latest.sif\n</code></pre>"},{"location":"09_reproducibility_IV/#run","title":"run","text":"<p>Apptainer containers contain runscripts. These are user defined scripts that define the actions a container should perform when someone runs it. The runscript can be triggered with the <code>run</code> command, or simply by calling the container as though it were an executable.</p> <pre><code>$ apptainer run cowsay_latest.sif\nINFO:    underlay of /etc/localtime required more than 50 (76) bind mounts\n ____________________________________\n/ Q: Do you know what the death rate \\\n\\ around here is? A: One per person. /\n ------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"09_reproducibility_IV/#shell","title":"shell","text":"<p>The <code>shell</code> command allows you to spawn a new shell within your container and interact with it as though it were a small virtual machine.</p> <pre><code>$ apptainer shell cowsay_latest.sif\nINFO:    underlay of /etc/localtime required more than 50 (76) bind mounts\n(ocelote) Apptainer&gt;\n</code></pre> <p>The change in prompt indicates that you have entered the container (though you should not rely on that to determine whether you are in container or not).</p> <p>Once inside of a Apptainer container, you are the same user as you are on the host system.</p> <pre><code>(ocelote) Apptainer&gt; whoami\ncosi\n</code></pre> <p>Type <code>exit</code> to exit the container.</p> <p>The more you know </p> <p><code>shell</code> also works with the <code>library://</code>, <code>docker://</code>, and <code>shub://</code> URIs. This creates an ephemeral container* that disappears when the shell is exited.</p> <p>Ephemeral container*: a short-lived container instance that is created dynamically to perform a specific task or process and then terminated once the task is complete. These containers are typically used for one-off jobs, temporary operations, or short-duration tasks within a larger computing environment.</p>"},{"location":"09_reproducibility_IV/#exec","title":"exec","text":"<p>The exec command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the <code>cowsay</code> program within the <code>cowsay_latest.sif</code> container:</p> <pre><code>$ apptainer exec cowsay_latest.sif cowsay whoaaaa the grass is soooo green inside the HPC!\nINFO:    underlay of /etc/localtime required more than 50 (76) bind mounts\n _________________________________________\n/ whoaaaa the grass is soooo green inside \\\n\\ the HPC!                                /\n -----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>This  also creates an ephemeral container that executes a command and disappears.</p>"},{"location":"09_reproducibility_IV/#inspect","title":"inspect","text":"<p>The <code>inspect</code> command will provide information about labels, metadata, and environmental variables.</p> <pre><code>$ apptainer inspect cowsay_latest.sif\norg.label-schema.build-arch: amd64\norg.label-schema.build-date: Wednesday_27_March_2024_20:16:32_MST\norg.label-schema.schema-version: 1.0\norg.label-schema.usage.apptainer.version: 1.2.5-1.el7\norg.label-schema.usage.singularity.deffile.bootstrap: docker\norg.label-schema.usage.singularity.deffile.from: tswetnam/cowsay\n</code></pre>"},{"location":"10_hpc/","title":"The HPC","text":"<pre><code>Material is coming!\n\n       _.--,_......----..__\n       \\  .'          '    ```--...__      \\\n        \\;           '            .  '.   ||\n        :           '            '     \\ .''.\n      .';          :            '       .|  |.--..___\n     /   \\         |           :        :|  /.------.\\\n    /    .'._      :           |        ||  ||      |\\\\\n   /.-. /|-| `-.               :        ;|  ||______|_\\`.______\n  //  // |-|    \\   '           '      / |  ||='      | |      `.\n //  //\\\\|-|     `-._'           '   .'  |  ||        | |        \\\n/.-.//  \\\\-|_________```------------` ___'. ||        | '_.--.   &lt;)\n'._.'  /  .-----.   .-----.   .''''''''.    |'--..____| /  _  \\   |\n       |_/.'   '.\\_/.'   '.\\_[ [ [  ] ] ]___|_________.'.'   '.\\  ]\n         :  .-.  : :  .-.  :  '........'    (_________):  .-.  :`-'\n         :  '-'  : :  '-'  :                           :  '-'  :\n          '._ _.'   '._ _.'                             '._ _.'\n</code></pre>"},{"location":"11_sql_duckdb/","title":"Exploring SQL Databases with DuckDB","text":"<pre><code>Material is coming!\n\n       _.--,_......----..__\n       \\  .'          '    ```--...__      \\\n        \\;           '            .  '.   ||\n        :           '            '     \\ .''.\n      .';          :            '       .|  |.--..___\n     /   \\         |           :        :|  /.------.\\\n    /    .'._      :           |        ||  ||      |\\\\\n   /.-. /|-| `-.               :        ;|  ||______|_\\`.______\n  //  // |-|    \\   '           '      / |  ||='      | |      `.\n //  //\\\\|-|     `-._'           '   .'  |  ||        | |        \\\n/.-.//  \\\\-|_________```------------` ___'. ||        | '_.--.   &lt;)\n'._.'  /  .-----.   .-----.   .''''''''.    |'--..____| /  _  \\   |\n       |_/.'   '.\\_/.'   '.\\_[ [ [  ] ] ]___|_________.'.'   '.\\  ]\n         :  .-.  : :  .-.  :  '........'    (_________):  .-.  :`-'\n         :  '-'  : :  '-'  :                           :  '-'  :\n          '._ _.'   '._ _.'                             '._ _.'\n</code></pre>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":"<p>In conjunction with for using CyVerse cyberinfrastructure, this Code of Conduct applies to all Event participants and their activities while using CyVerse resources and/or attending the Event.</p> <p>CyVerse is dedicated to providing professional computational research and educational experiences for all of our users, regardless of domain focus, academic status, educational level, gender/gender identity/expression, age, sexual orientation, mental or physical ability, physical appearance, body size, race, ethnicity, religion (or lack thereof), technology choices, dietary preferences, or any other personal characteristic.</p> <p>When using CyVerse or participating at an Event, we expect you to:</p> <ul> <li>Interact with others and use CyVerse professionally and ethically by     complying with our Policies.</li> <li>Constructively critize ideas and processes, not people.</li> <li>Follow the Golden Rule (treat others as you want to be treated) when     interacting online or in-person with collaborators, trainers, and     support staff.</li> <li>Comply with this Code in spirit as much as the letter, as it is     neither exhaustive nor complete in identifying any and all possible     unacceptable conduct.</li> </ul> <p>We do not tolerate harassment of other users or staff in any form (including, but not limited to, violent threats or language, derogatory language or jokes, doxing, insults, advocating for or encouraging any of these behaviors). Sexual language and imagery are not appropriate at any time (excludes Protected Health Information in compliance with HIPAA). Any user violating this Code may be expelled from the platform and the workshop at CyVerse's sole discretion without warning.</p> <p>To report a violation of this Code, directly message a trainer via Slack or email info@cyverse.org with the following information:</p> <ul> <li>Your contact information</li> <li>Names (real, username, pseudonyms) of any individuals involved, and     or witness(es) if any.</li> <li>Your account of what occurred and if the incident is ongoing. If     there is a publicly available record (a tweet, public chat log,     etc.), please include a link or attachment.</li> <li>Any additional information that may be helpful in resolving the     issue.</li> </ul>"},{"location":"glossary/","title":"Glossary &amp; Acronyms","text":"<p>A</p> <ul> <li>action: automate a workflow in the context of CI/CD, see GitHub Actions</li> <li>agile: development methodology     for organizing a team to complete tasks organized over short periods     called 'sprints'</li> <li>allocation: portion of a resource assigned to a particular     recipient, typical unit is a core or node hour</li> <li>Anaconda: open source data science platform.     Anaconda.com</li> <li>application: also called an 'app', a software designed to help     the user to perform specific task</li> <li>awesome: a curated set of lists that provide insight into     awesome software projects on GitHub</li> <li>AVU: Attribute-Value-Unit a components for iRODS     metadata.</li> </ul> <p>B</p> <ul> <li>beta: a software version which is not yet ready for     publication but is being tested</li> <li>bash: Bash is the GNU Project's shell, the Bourne-Again     Shell</li> <li>biocontainer: a community-driven project that provides the     infrastructure and basic guidelines to create, manage and distribute     bioinformatics packages (e.g conda) and containers (e.g docker,     singularity)</li> <li>bioconda: a channel for the conda package manager specializing     in bioinformatics software</li> </ul> <p>C</p> <ul> <li>CLI: the UNIX shell command line interface,     most typically BASH</li> <li>command: a set of instructions sent to the computer, typically     in a typed interface</li> <li>conda: an installation type of the Anaconda data science     platform. Command line application for managing packages and     environments</li> <li>container: virtualization of an operating system run within an     isolated user space</li> <li>Continuous Integration: (CI) is testing automation to check that     the application is not broken whenever new commits are integrated     into the main branch</li> <li>Continuous Delivery: (CD) is an extension of 'continuous     integration' to make sure that you can release new changes in a     sustainable way</li> <li>Continuous Deployment: a step further than 'continuous     delivery', every change that passes all stages of your production     pipeline is released</li> <li>Continuous Development: a process for iterative software     development and is an umbrella over several other processes     including 'continuous integration', 'continuous testing',     'continuous delivery' and 'continuous deployment'</li> <li>Continuous Testing: a process of testing and automating software     development.</li> <li>CRAN: The Comprehensive R Archive     Network</li> <li>CyVerse tool: Software program that is integrated into the back     end of the DE for use in DE apps</li> <li>CyVerse app: graphic interface of a tool made available for use     in the DE</li> </ul> <p>D</p> <ul> <li>Debian: a free OS, base of other     Linux distributions such as Ubuntu</li> <li>Development: the environment on your computer where you write     code</li> <li>DevOps Software *Dev*elopment and information techology     *Op*erations techniques for shortening the time to change software     in relation to CI/CD</li> <li>Discovery Environment (DE): a data science workbench for running     executable, interactive, and high throughput applications in     CyVerse DE</li> <li>distribution: abbreviated as 'distro', an operating system     made from a software collection based upon the Linux kernel</li> <li>Docker: Docker is an open source     software platform to create, deploy and manage virtualized     application containers on a common operating system (OS), with an     ecosystem of allied tools. A program that runs and handles     life-cycle of containers and images</li> <li>DockerHub: an official registry of docker containers, operated     by Docker. DockerHub</li> <li>DOI: a digital object identifier. A persistant identifier     number, managed by the doi.org</li> <li>Dockerfile: a text document that contains all the commands you     would normally execute manually in order to build a Docker image.     Docker can build images automatically by reading the instructions     from a Dockerfile</li> </ul> <p>E</p> <ul> <li>environment: software that includes operating system, database     system, specific tools for analysis</li> <li>entrypoint: In a Dockerfile, an ENTRYPOINT is an optional     definition for the first part of the command to be run</li> </ul> <p>F</p> <ul> <li>FOSS: (1) Free and Open Source Software, (2)     Foundational Open Science Skills - this class!</li> <li>function: a named section of a program that performs a specific     task</li> </ul> <p>G</p> <ul> <li>git: a version control system software</li> <li>gitter: a Github based messaging service that uses markdown     gitter.im</li> <li>GitHub: a website for hosting <code>git</code> repositories - owned by     Microsoft GitHub</li> <li>GitLab: a website for hosting <code>git</code> repositories     GitLab</li> <li>GitOps: using <code>git</code> framework as a means of deploying     infrastructure on cloud using Kubernetes</li> <li>GPU: graphic processing unit</li> <li>GUI: graphical user interface</li> </ul> <p>H</p> <ul> <li>hack: a quick job that produces what is needed, but not well</li> <li>HPC: High Performance Computer, for large syncronous computation</li> <li>HTC: High Throughput Computer, for many parallel tasks</li> </ul> <p>I</p> <ul> <li>IaaS: Infrastructure as a Service.     online services that provide APIs</li> <li>iCommands: command line application for     accessing iRODS Data Store</li> <li>IDE: integrated development environment, typically a graphical     interface for working with code language or packages</li> <li>instance: a single virtul machine</li> <li>image: self-contained, read-only 'snapshot' of your applications     and packages, with all their dependencies</li> <li>iRODS: an open source integrated Rule-Oriented Data Management     System, iRODS.org</li> </ul> <p>J</p> <ul> <li>Java: programming language, class-based, object-oriented</li> <li>JavaScript: programming language</li> <li>JSON: Java Script Object Notation, data interchange format that     uses human-readable text</li> <li>Jupyter(Hub,Lab,Notebooks): an IDE, originally the     iPythonNotebook, operates in the browser Project     Jupyter</li> </ul> <p>K</p> <ul> <li>kernel: central component of most operating systems (OS)</li> <li>Kubernetes: an open source container orchestration platform     created by Google Kubernetes is often     referred to as <code>K8s</code></li> </ul> <p>L</p> <ul> <li>lib: a UNIX library</li> <li>linux: open source Unix-like operating system</li> </ul> <p>M</p> <ul> <li>makefile: a file containing a set of directives used by a make     build automation tool</li> <li>markdown: a lightweight markup language with plain text     formatting syntax</li> <li>metadata:: data about data, useful for searching and querying</li> <li>multi-thread: a process which runs on more than one CPU or GPU     core at the same time</li> <li>master node: responsible for deciding what runs on all of the     cluster's nodes. Can include scheduling workloads, like     containerized applications, and managing the workloads' lifecycle,     scaling, and upgrades. The master also manages network and storage     resources for those workloads</li> <li>Mac OS X: Apple's popular desktop OS</li> </ul> <p>N</p> <ul> <li>node: a computer, typically 1 or 2 core (with many threads)     server in a cloud or HPC center</li> </ul> <p>O</p> <ul> <li>ontology: formal naming and structural hierarchy used to     describe data, also called a knowledge     graph</li> <li>organization: a group, in the context of GitHub a place where     developers contribute code to repositories</li> <li>Operating System (OS): software that manages computer hardware,     software resources, and provides common services for computer     programs</li> <li>Open Science Grid (OSG): national, distributed computing     partnership for data-intensive research     opensciencegrid.org</li> <li>ORCID: Open Researcher and Contributor ID     (ORCiD), a persistent digital identifier that     distinguishes you from every other researcher</li> </ul> <p>P</p> <ul> <li>PaaS: Platform as Service run     and manage applications in cloud without complexity of developing it     yourself</li> <li>package: an app designed for a particular langauge</li> <li>package manager: a collection of software tools that automates     the process of installing, upgrading, configuring, and removing     computer programs for a computer's operating system in a consistent     manner</li> <li>Production: environment where users access the final code after     all of the updates and testing</li> <li>Python: interpreted, high-level, general-purpose programming     language Python.org</li> </ul> <p>Q</p> <ul> <li>QUAY.io: private Docker registry QUAY.io</li> </ul> <p>R</p> <ul> <li>R: data science programming language R Project</li> <li>recipe file: a file with installation scripts used for building     software such as containers, e.g. Dockerfile</li> <li>registry: a storage and content delivery system, such as that     used by Docker</li> <li>remote desktop: a VM with a graphic user interface accessed via     a browser</li> <li>repo(sitory): a directory structure for hosting code and data</li> <li>RST: ReStructuredText, a markdown type file</li> <li>ReadTheDocs: a web service for rendering documentation (that     this website uses) readthedocs.org and     readthedocs.com</li> <li>root: the administrative user on a linux kernel - use your     powers wisely</li> </ul> <p>S</p> <ul> <li>SaaS: Software as a Service web     based platform for using software</li> <li>schema: a metadata standard for labeling, tagging or coding for     recording &amp; cataloging information or structuring descriptive     records. see schema.org</li> <li>scrum: daily set of tasks and evalautions as part of a sprint.</li> <li>shell: is a command line interface program that runs other     programs (may be complex, technical programs or very simple programs     such as making a directory). These simple, stand-alone programs are     called commands</li> <li>Singularity: a container software, used widely on HPC, created     by SyLabs</li> <li>SLACK: Searchable Log of All Conversation and Knowledge, a team     communication tool slack.com</li> <li>sprint: set period of time during which specific work has to be     completed and made ready for review</li> <li>Singularity def file: (definition file) recipe for building a     Singualrity container</li> <li>Stage: environment that is as similar to the production     environment as can be for final testing</li> </ul> <p>T</p> <ul> <li>tar: software utility for collecting many files into one archive     file, often referred to as a tarball</li> <li>tensor: algebraic object that describes a linear mapping from     one set of algebraic objects to another</li> <li>terminal: a windowed emulator for directly enterinc commands to     a computer</li> <li>thread: a CPU process or a series of linked messages in a     discussion board</li> <li>tool: In the context of CyVerse Discovery Environment, a Docker     Container</li> <li>TPU: tensor processing unit</li> <li>Travis: Travis-CI, a continuous     integration software</li> </ul> <p>U</p> <ul> <li>Ubuntu: most popular Linux OS     distribution, based on Debian</li> <li>UNIX: operating system</li> <li>user: the profile under which applications are started and run,     <code>root</code> is the most powerful system administrator</li> </ul> <p>V</p> <ul> <li>VICE: Visual Interactive Computing     Environment -     Cyverse Data Science Workbench</li> <li>virtual machine: is a software computer that, like a physical     computer, runs an operating system and applications</li> </ul> <p>W</p> <ul> <li>waterfall: software development broken into linear sequential     phases, similar to a Gantt chart</li> <li>webGL: JavaScript API for rendering interactive 2D and 3D     graphics within any compatible web browser without the use of     plug-ins</li> <li>Windows: Microsoft's most popular desktop OS</li> <li>workspace: (vs. repo)</li> <li>worker node: A cluster typically has one or more nodes, which     are the worker machines that run your containerized applications and     other workloads. Each node is managed from the master, which     receives updates on each node's self-reported status.</li> </ul> <p>X</p> <ul> <li>XML: Extensible Markup Language, data interchange format that     uses human-readable text</li> </ul> <p>Y</p> <ul> <li>YAML: YAML Ain't Markup Language, data interchange format that     uses human-readable text</li> </ul> <p>Z</p> <ul> <li>ZenHub: team collaboration solution built directly into GitHub     that uses kanban style boards</li> <li>Zenodo: general-purpose open-access repository developed under     the European OpenAIRE program and operated by CERN</li> <li>zip: a compressed file format</li> <li>zsh: Z-Shell, now the default shell on     new Mac OS X</li> </ul>"},{"location":"installation/","title":"Pre-FOSS Setup","text":"<p>Welcome to FOSS Online, we're happy you're here! To get you ready to hit the ground running, please set up the prerequisite accounts and software listed below before the course starts.</p>"},{"location":"installation/#account-creation","title":"Account Creation","text":"<p>We will be using several services that require you to create a user account.</p> Account Notes  GitHub GitHub will be used to store lecture materials and your own work. We will use GitHub Education and its free features for hands-on.  Docker Link your GitHub account to the DockerHub.  Slack We use the <code>UA Data Science</code>  Slack organization's foss channel, you should have received an invitation via email. You can use Slack in the browser, but the desktop app is usually less buggy.  HackMD We will use HackMD in order to facilitate daily discussions, questions and general notes. Link your HackMD using your GitHub account Cyverse We will introduce you to Cyverse which is a powerful cloud computer with large data storage. <p>Link to  HackMD</p> <p>https://hackmd.io/y9XyyinFToOJS3XPJ2TtGg?view </p> Dual Monitors vs Side-by-Side <p>We strongly recommend you have dual monitors set-up while attending virtual FOSS Zoom lessons.</p> <p>We will be doing a lot of screen-sharing, and this will make your own interactive sessions less visible, or you will have to make them less than full screen.</p> <p>If you only have one monitor, make sure to exit full screen mode on Zoom and your browser, so you can view everything side-by-side</p>"},{"location":"installation/#required-software","title":"Required Software","text":"<p>You will need to have the following software installed on your personal computer:</p> Software Notes Web Browser  Chrome or  Firefox. Text Editor  VS Code or SublimeText <p>Attention  Windows users</p> <p>Much of what we are going to be teaching is based on open-source software which operates on cloud and is incompatible with Windows OS.</p> <p>Unix-based systems such as Linux  Ubuntu and  MacOS X, as many scientific tools require a Unix Operating System (OS). </p> <p>There are a number of software that allow Windows users to execute Unix commands, however we recommend the use of  Windows Subsystem for Linux (WSL) 2.0.</p> <p> VS Code is a Microsoft product and integrates seamlessly with Unix systems, we therefore strongly encourage you to install Code on your Windows OS.</p>"},{"location":"plus_index/","title":"FOSS+ Home &amp; Schedule","text":"<p>Thank you for being with us throughout FOSS! Expanding on the concepts and activities covered throughout FOSS, we wanted to capitalize on certain aspects of reproducibility with FOSS+.</p> <p>In the next 4 weeks, we are going to resume our discussion on Docker and containerization, execution of containers on the HPC and Cloud, and we are going to have 2 guests lectures from Chris Reidy (UITS, covering the HPC in detail) and Shashank Yadav (DataLab, discussing SQL and databases).</p> <p>The workshop structure and schedule will be the same as FOSS: Thursdays 11am - 1pm Arizona Time using the same Zoom link as before.</p>"},{"location":"plus_index/#schedule","title":"Schedule","text":"Date Content Instructor(s) 03/21 Reproducibility III: Building Containers Michele Cosi, Jeff Gillan 03/28 Executing Containers on the HPC Michele Cosi 04/04 The HPC Chris Reidy (UITS) 03/11 SQL and Databases Shashank Yadav (DataLab)"},{"location":"plus_index/#expected-outcomes","title":"Expected Outcomes","text":"<ul> <li>Be able to build your own Container from a public containerized image.</li> <li>Be able to  deploy personalized images onto the HPC.</li> <li>Understand the HPC structure.</li> <li>Understand database structures and execute SQL commands.</li> </ul>"},{"location":"schedule/","title":"Weekly Schedule","text":"<p>Thursdays 11 am - 1 pm Arizona Time.</p> <p>This schedule is tentative and subject to change</p>"},{"location":"schedule/#calendar","title":"Calendar","text":"Week Date Content Instructor(s) Week 0 Recorded pre-FOSS workshop:  - Unix shell basics  - Git/GitHub basics  - ChatGPT &amp; LLMs Michele Cosi &amp; Jeff Gillan Week 1 Jan. 25 Workshop introduction:  - Intro to Open Science Tyson Swetnam, Michele Cosi, Jeff Gillan Week 2 Feb. 1 Data management:  - FAIR data  - Data Management Plans  - Processing activity Jeff Gillan, Michele Cosi  Guest Speaker: Wade Bishop, UTK Week 3 Feb. 8 - Project management Michele Cosi  Guest Speaker: Rudy Salcido Week 4 Feb. 15 Documentation / Communication:  - Internal + External Documentation  - Internal + External Communication  - GitHub Pages websites Michele Cosi, Jeff Gillan Week 5 Feb. 22 Version Control  - Version control as a philosophy  - GitHub functionality  Version control everything Michele Cosi,  Guest Speaker: Jason Williams, CSH Week 6 Feb. 29 Reproducibility I:  - Software installation  - Software management Jeff Gillan, Michele Cosi Week 7 March 7 Reproducibility II:  - Brief intro to containers Michele Cosi, Jeffrey Gillan Week 8 March 14 Capstone Presentations"},{"location":"documentation/githubpages/","title":"GitHub Pages - quick start","text":"<p>This is a quick introduction to GitHub Pages, a simple way to use GitHub to set up a small website written in Markdown. This page won't do everything, but you can throw up a basic website, use themes, and extend it.</p> <ol> <li>Go to https://github.com/. Login, or if you don't have an account get one and login.</li> <li>Go to the \"+\" icon on the upper right and select New repository.</li> <li>Enter a name for your repository (e.g. \"profile\"). Enter a description, and leave the repository as public. Select \"Initialize this repository with a README\". If desired select a license. Finally click Create repository.</li> <li>Look for the Settings menu (upper right, next to a \"gear\" icon). Scroll down to GitHub Pages and choose master branch and save your selection. Then Choose a theme and select your theme. You will be asked to Commit changes.</li> <li>Your website will be visible at <code>https://GITHUBUSERNAME.github.io/REPONAME/</code>. (be sure to change GITHUBUSERNAME to your username, and REPONAME to the name you selected for your repo.)</li> <li>You can edit your website by editing the readme file as desired.</li> </ol> <p>Tip</p> <pre><code>You can preview how your [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) looks using and editor like [Markdown Plus](https://mdp.tylingsoft.com/).\n</code></pre>"},{"location":"final_project/overview/","title":"Capstone Project Overview","text":"<p>The capstone project for FOSS is designed to give you the opportunity to apply some of the skills and ideas from the rest of the course to one of your own projects, get support and feedback from your peers, and share your experiences with the rest of the attendees.</p>"},{"location":"final_project/overview/#objectives","title":"Objectives","text":"<ul> <li>\"Level up\" the openness of an existing or planned project</li> <li>Apply a skill or concept from FOSS</li> <li>Discuss your applied skills/concepts with groupmates</li> <li>Determine possible next steps to \"level up\" your project</li> <li>Share your experiences with the rest of the attendees</li> </ul>"},{"location":"final_project/overview/#description","title":"Description","text":""},{"location":"final_project/overview/#option-1","title":"Option 1","text":"<p>For the Fall 2023 capstone project, we will introduce hands-on exercises at the end of each FOSS session that will help crystalize the material learned that day. Over the 8 weeks of the course, you will be building and populating a Github repository or 'research object' for you own research projects. For example, for the session where we cover Data Management, we will have an exercise where you write a mini data management plan and post it in your Github Repo. </p> <p>Option 1 will be using the following website as reference: FOSS Reference Hub.</p>"},{"location":"final_project/overview/#option-2","title":"Option 2","text":"<p>For option 2, you can forego the week-to-week exercises. Instead, you will choose one (or more) skill(s) or concept(s) from FOSS and apply it to one of your own projects. </p> <p>For instance, you could write a Data Management Plan for an upcoming project proposal, or you could reorganize an existing project and put all the code onto GitHub. You could build a resume git.io web page, or you could try to containerize software. You should try to identify what level your project is currently at for a given topic, and attempt to move up a level. This means you can try out a skill or idea you've never used before, or go to a more advanced level for something you already do.</p> <p>You should have your skill/topic chosen by Week 6, and you will then enter your name / topic into a spreadsheet (shared during the week 6 session). We'll use this to help students find others working on similar projects.</p> <ul> <li>You can do a project completely solo</li> <li>You can work solo, but join a 'support group' to:<ol> <li>discuss your experiences applying your skill or concept</li> <li>give each other help with sticking point you may encounter</li> </ol> </li> <li>You can work directly with a FOSS colleague on a joint project</li> </ul>"},{"location":"final_project/overview/#final-presentations","title":"Final Presentations","text":"<p>Each student or group will deliver a short (5-10 minute) presentation to the rest of the class on week 8</p> <p>Your small group presentation should focus on the challenges and tips you may have for other FOSS attendees who want to utilize your skill or concept in the future. Here are some prompts that you should address during your presentation:</p> <ul> <li>What was the general topic or skill that members of your group worked on?</li> <li>What are some challenges you encountered while working on your projects?<ul> <li>Were you able to overcome these challenges? If so, how?</li> <li>Where did you look for help?</li> <li>Do any roadblocks remain? How might you try to overcome them?</li> </ul> </li> <li>Are there any new things you learned while working on your project?<ul> <li>Did you end up using any new or different tools?</li> </ul> </li> <li>What are some tips you might have for other FOSS attendees who want to work on the same topic/skill in the future?<ul> <li>Are there any pitfalls to avoid?</li> </ul> </li> <li>What things do you want to do next?<ul> <li>How might you \"level up\" in the current topic/skill compared to your project's current state?</li> <li>Are there any other FOSS skills that you want to tackle next? If so, how might they integrate with the topic/skill you focused on for your project?</li> </ul> </li> </ul>"},{"location":"final_project/overview/#examples","title":"Examples","text":"<p>Links to previous' year capstone projects will be posted in the FOSS HackMD. Stay tuned!</p>"}]}